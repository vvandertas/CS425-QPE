2019-10-17 12:20:42 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-10-17 12:20:43 INFO  SparkContext:54 - Running Spark version 2.3.1
2019-10-17 12:20:43 INFO  SparkContext:54 - Submitted application: lenet5
2019-10-17 12:20:43 INFO  SecurityManager:54 - Changing view acls to: ubuntu
2019-10-17 12:20:43 INFO  SecurityManager:54 - Changing modify acls to: ubuntu
2019-10-17 12:20:43 INFO  SecurityManager:54 - Changing view acls groups to: 
2019-10-17 12:20:43 INFO  SecurityManager:54 - Changing modify acls groups to: 
2019-10-17 12:20:43 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()
2019-10-17 12:20:44 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 38091.
2019-10-17 12:20:44 INFO  SparkEnv:54 - Registering MapOutputTracker
2019-10-17 12:20:44 INFO  SparkEnv:54 - Registering BlockManagerMaster
2019-10-17 12:20:44 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-10-17 12:20:44 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up
2019-10-17 12:20:44 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-4d981927-8c45-4528-b0e9-5fa0eb488427
2019-10-17 12:20:44 INFO  MemoryStore:54 - MemoryStore started with capacity 413.9 MB
2019-10-17 12:20:44 INFO  SparkEnv:54 - Registering OutputCommitCoordinator
2019-10-17 12:20:44 INFO  log:192 - Logging initialized @3335ms
2019-10-17 12:20:44 INFO  Server:346 - jetty-9.3.z-SNAPSHOT
2019-10-17 12:20:44 INFO  Server:414 - Started @3510ms
2019-10-17 12:20:44 INFO  AbstractConnector:278 - Started ServerConnector@391b67e5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-10-17 12:20:44 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@347610b2{/jobs,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@64eade6{/jobs/json,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7cb1a65c{/jobs/job,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3e40253b{/jobs/job/json,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4d929e88{/stages,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3ce04220{/stages/json,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@26e175c2{/stages/stage,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@7e47f5cb{/stages/stage/json,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@53a5c0a4{/stages/pool,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@759ae63d{/stages/pool/json,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@77650ba8{/storage,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@364d5122{/storage/json,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4b9017af{/storage/rdd,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6999bbee{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@367ed53e{/environment,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6ae65692{/environment/json,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2100190e{/executors,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@562800b{/executors/json,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@457562bc{/executors/threadDump,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1d7ffe46{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3c165c8e{/static,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4dfa24a4{/,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@b8e7f6c{/api,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@36f93cae{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4b78e82{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-10-17 12:20:44 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://bigdl-instance-master1-hdd.europe-west4-a.c.seventh-oven-254809.internal:4040
2019-10-17 12:20:44 INFO  SparkContext:54 - Added JAR file:///home/f1r3flyp1l0t/bd/spark/lib/bigdl-SPARK_2.3-0.8.0-jar-with-dependencies.jar at spark://bigdl-instance-master1-hdd.europe-west4-a.c.seventh-oven-254809.internal:38091/jars/bigdl-SPARK_2.3-0.8.0-jar-with-dependencies.jar with timestamp 1571314844868
2019-10-17 12:20:44 INFO  SparkContext:54 - Added file file:/home/f1r3flyp1l0t/bd/codes/lenet5.py at spark://bigdl-instance-master1-hdd.europe-west4-a.c.seventh-oven-254809.internal:38091/files/lenet5.py with timestamp 1571314844921
2019-10-17 12:20:44 INFO  Utils:54 - Copying /home/f1r3flyp1l0t/bd/codes/lenet5.py to /tmp/spark-ba1df533-5792-45ad-a7d3-6a60066ed986/userFiles-3f41ace7-f808-475f-a60c-e2e224b06597/lenet5.py
2019-10-17 12:20:44 INFO  SparkContext:54 - Added file file:///home/f1r3flyp1l0t/bd/spark/lib/bigdl-0.8.0-python-api.zip at spark://bigdl-instance-master1-hdd.europe-west4-a.c.seventh-oven-254809.internal:38091/files/bigdl-0.8.0-python-api.zip with timestamp 1571314844939
2019-10-17 12:20:44 INFO  Utils:54 - Copying /home/f1r3flyp1l0t/bd/spark/lib/bigdl-0.8.0-python-api.zip to /tmp/spark-ba1df533-5792-45ad-a7d3-6a60066ed986/userFiles-3f41ace7-f808-475f-a60c-e2e224b06597/bigdl-0.8.0-python-api.zip
2019-10-17 12:20:45 INFO  StandaloneAppClient$ClientEndpoint:54 - Connecting to master spark://10.164.0.12:7077...
2019-10-17 12:20:45 INFO  TransportClientFactory:267 - Successfully created connection to /10.164.0.12:7077 after 42 ms (0 ms spent in bootstraps)
2019-10-17 12:20:45 INFO  StandaloneSchedulerBackend:54 - Connected to Spark cluster with app ID app-20191017122045-0004
2019-10-17 12:20:45 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor added: app-20191017122045-0004/0 on worker-20191017120306-10.164.0.9-44135 (10.164.0.9:44135) with 2 core(s)
2019-10-17 12:20:45 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37347.
2019-10-17 12:20:45 INFO  NettyBlockTransferService:54 - Server created on bigdl-instance-master1-hdd.europe-west4-a.c.seventh-oven-254809.internal:37347
2019-10-17 12:20:45 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-10-17 12:20:45 INFO  StandaloneSchedulerBackend:54 - Granted executor ID app-20191017122045-0004/0 on hostPort 10.164.0.9:44135 with 2 core(s), 12.0 GB RAM
2019-10-17 12:20:45 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor added: app-20191017122045-0004/1 on worker-20191017120336-10.164.0.10-44749 (10.164.0.10:44749) with 2 core(s)
2019-10-17 12:20:45 INFO  StandaloneSchedulerBackend:54 - Granted executor ID app-20191017122045-0004/1 on hostPort 10.164.0.10:44749 with 2 core(s), 12.0 GB RAM
2019-10-17 12:20:45 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20191017122045-0004/0 is now RUNNING
2019-10-17 12:20:45 INFO  StandaloneAppClient$ClientEndpoint:54 - Executor updated: app-20191017122045-0004/1 is now RUNNING
2019-10-17 12:20:45 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, bigdl-instance-master1-hdd.europe-west4-a.c.seventh-oven-254809.internal, 37347, None)
2019-10-17 12:20:45 INFO  BlockManagerMasterEndpoint:54 - Registering block manager bigdl-instance-master1-hdd.europe-west4-a.c.seventh-oven-254809.internal:37347 with 413.9 MB RAM, BlockManagerId(driver, bigdl-instance-master1-hdd.europe-west4-a.c.seventh-oven-254809.internal, 37347, None)
2019-10-17 12:20:45 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, bigdl-instance-master1-hdd.europe-west4-a.c.seventh-oven-254809.internal, 37347, None)
2019-10-17 12:20:45 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, bigdl-instance-master1-hdd.europe-west4-a.c.seventh-oven-254809.internal, 37347, None)
2019-10-17 12:20:45 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@729820ea{/metrics/json,null,AVAILABLE,@Spark}
2019-10-17 12:20:47 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.164.0.9:44550) with ID 0
2019-10-17 12:20:47 INFO  CoarseGrainedSchedulerBackend$DriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.164.0.10:35110) with ID 1
2019-10-17 12:20:47 INFO  StandaloneSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 1.0
2019-10-17 12:20:47 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.164.0.9:37059 with 6.2 GB RAM, BlockManagerId(0, 10.164.0.9, 37059, None)
2019-10-17 12:20:47 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.164.0.10:39109 with 6.2 GB RAM, BlockManagerId(1, 10.164.0.10, 39109, None)
2019-10-17 12:20:47 INFO  Engine$:112 - Auto detect executor number and executor cores number
2019-10-17 12:20:47 INFO  Engine$:114 - Executor number is 2 and executor cores number is 2
2019-10-17 12:20:48 INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 17
2019-10-17 12:20:48 INFO  Engine$:402 - Find existing spark context. Checking the spark conf...
cls.getname: com.intel.analytics.bigdl.python.api.Sample
BigDLBasePickler registering: bigdl.util.common  Sample
cls.getname: com.intel.analytics.bigdl.python.api.EvaluatedResult
BigDLBasePickler registering: bigdl.util.common  EvaluatedResult
cls.getname: com.intel.analytics.bigdl.python.api.JTensor
BigDLBasePickler registering: bigdl.util.common  JTensor
cls.getname: com.intel.analytics.bigdl.python.api.JActivity
BigDLBasePickler registering: bigdl.util.common  JActivity
0.01
('Extracting', '/tmp/mnist/train-images-idx3-ubyte.gz')
('Extracting', '/tmp/mnist/train-labels-idx1-ubyte.gz')
('Extracting', '/tmp/mnist/t10k-images-idx3-ubyte.gz')
('Extracting', '/tmp/mnist/t10k-labels-idx1-ubyte.gz')
creating: createSequential
creating: createReshape
creating: createSpatialConvolution
creating: createTanh
creating: createSpatialMaxPooling
creating: createSpatialConvolution
creating: createTanh
creating: createSpatialMaxPooling
creating: createReshape
creating: createLinear
creating: createTanh
creating: createLinear
creating: createLogSoftMax
creating: createClassNLLCriterion
creating: createDefault
creating: createSGD
creating: createMaxEpoch
creating: createDistriOptimizer
disableCheckSingleton is deprecated. Please use bigdl.check.singleton instead
creating: createEveryEpoch
creating: createTop1Accuracy
creating: createEveryEpoch
2019-10-17 12:20:50 INFO  DistriOptimizer$:784 - caching training rdd ...
2019-10-17 12:20:58 INFO  DistriOptimizer$:624 - Cache thread models...
2019-10-17 12:20:59 INFO  DistriOptimizer$:626 - Cache thread models... done
2019-10-17 12:20:59 INFO  DistriOptimizer$:154 - Count dataset
2019-10-17 12:20:59 INFO  DistriOptimizer$:158 - Count dataset complete. Time elapsed: 0.213106366s
2019-10-17 12:20:59 INFO  DistriOptimizer$:166 - config  {
	computeThresholdbatchSize: 100
	maxDropPercentage: 0.0
	warmupIterationNum: 200
	isLayerwiseScaled: false
	dropPercentage: 0.0
 }
2019-10-17 12:20:59 INFO  DistriOptimizer$:170 - Shuffle data
2019-10-17 12:20:59 INFO  DistriOptimizer$:173 - Shuffle data complete. Takes 0.042167062s
2019-10-17 12:20:59 INFO  DistriOptimizer$:408 - [Epoch 1 256/60000][Iteration 1][Wall Clock 0.553408463s] Trained 256 records in 0.553408463 seconds. Throughput is 462.5878 records/second. Loss is 2.3288455. Sequentialfabab260's hyper parameters: Current learning rate is 0.01. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:00 INFO  DistriOptimizer$:408 - [Epoch 1 512/60000][Iteration 2][Wall Clock 0.796805604s] Trained 256 records in 0.243397141 seconds. Throughput is 1051.7789 records/second. Loss is 2.321579. Sequentialfabab260's hyper parameters: Current learning rate is 0.009998000399920017. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:00 INFO  DistriOptimizer$:408 - [Epoch 1 768/60000][Iteration 3][Wall Clock 1.006629192s] Trained 256 records in 0.209823588 seconds. Throughput is 1220.0725 records/second. Loss is 2.3153982. Sequentialfabab260's hyper parameters: Current learning rate is 0.009996001599360257. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:00 INFO  DistriOptimizer$:408 - [Epoch 1 1024/60000][Iteration 4][Wall Clock 1.249449499s] Trained 256 records in 0.242820307 seconds. Throughput is 1054.2776 records/second. Loss is 2.321216. Sequentialfabab260's hyper parameters: Current learning rate is 0.009994003597841297. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:00 INFO  DistriOptimizer$:408 - [Epoch 1 1280/60000][Iteration 5][Wall Clock 1.436338318s] Trained 256 records in 0.186888819 seconds. Throughput is 1369.7985 records/second. Loss is 2.3174233. Sequentialfabab260's hyper parameters: Current learning rate is 0.009992006394884094. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:01 INFO  DistriOptimizer$:408 - [Epoch 1 1536/60000][Iteration 6][Wall Clock 1.612174133s] Trained 256 records in 0.175835815 seconds. Throughput is 1455.9036 records/second. Loss is 2.330241. Sequentialfabab260's hyper parameters: Current learning rate is 0.009990009990009992. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:01 INFO  DistriOptimizer$:408 - [Epoch 1 1792/60000][Iteration 7][Wall Clock 1.781094956s] Trained 256 records in 0.168920823 seconds. Throughput is 1515.5028 records/second. Loss is 2.3142047. Sequentialfabab260's hyper parameters: Current learning rate is 0.00998801438274071. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:01 INFO  DistriOptimizer$:408 - [Epoch 1 2048/60000][Iteration 8][Wall Clock 1.950472875s] Trained 256 records in 0.169377919 seconds. Throughput is 1511.4131 records/second. Loss is 2.3063807. Sequentialfabab260's hyper parameters: Current learning rate is 0.009986019572598362. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:01 INFO  DistriOptimizer$:408 - [Epoch 1 2304/60000][Iteration 9][Wall Clock 2.160253164s] Trained 256 records in 0.209780289 seconds. Throughput is 1220.3243 records/second. Loss is 2.305614. Sequentialfabab260's hyper parameters: Current learning rate is 0.009984025559105431. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:01 INFO  DistriOptimizer$:408 - [Epoch 1 2560/60000][Iteration 10][Wall Clock 2.302455945s] Trained 256 records in 0.142202781 seconds. Throughput is 1800.2461 records/second. Loss is 2.3075588. Sequentialfabab260's hyper parameters: Current learning rate is 0.009982032341784788. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:01 INFO  DistriOptimizer$:408 - [Epoch 1 2816/60000][Iteration 11][Wall Clock 2.450311834s] Trained 256 records in 0.147855889 seconds. Throughput is 1731.4156 records/second. Loss is 2.3018284. Sequentialfabab260's hyper parameters: Current learning rate is 0.00998003992015968. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:02 INFO  DistriOptimizer$:408 - [Epoch 1 3072/60000][Iteration 12][Wall Clock 2.59433372s] Trained 256 records in 0.144021886 seconds. Throughput is 1777.5077 records/second. Loss is 2.306379. Sequentialfabab260's hyper parameters: Current learning rate is 0.009978048293753742. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:02 INFO  DistriOptimizer$:408 - [Epoch 1 3328/60000][Iteration 13][Wall Clock 2.741461669s] Trained 256 records in 0.147127949 seconds. Throughput is 1739.982 records/second. Loss is 2.3098483. Sequentialfabab260's hyper parameters: Current learning rate is 0.009976057462090982. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:02 INFO  DistriOptimizer$:408 - [Epoch 1 3584/60000][Iteration 14][Wall Clock 2.934729405s] Trained 256 records in 0.193267736 seconds. Throughput is 1324.5874 records/second. Loss is 2.3095746. Sequentialfabab260's hyper parameters: Current learning rate is 0.009974067424695792. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:02 INFO  DistriOptimizer$:408 - [Epoch 1 3840/60000][Iteration 15][Wall Clock 3.083654616s] Trained 256 records in 0.148925211 seconds. Throughput is 1718.9836 records/second. Loss is 2.2995787. Sequentialfabab260's hyper parameters: Current learning rate is 0.009972078181092942. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:02 INFO  DistriOptimizer$:408 - [Epoch 1 4096/60000][Iteration 16][Wall Clock 3.221688294s] Trained 256 records in 0.138033678 seconds. Throughput is 1854.6199 records/second. Loss is 2.309026. Sequentialfabab260's hyper parameters: Current learning rate is 0.009970089730807579. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:02 INFO  DistriOptimizer$:408 - [Epoch 1 4352/60000][Iteration 17][Wall Clock 3.365506472s] Trained 256 records in 0.143818178 seconds. Throughput is 1780.0253 records/second. Loss is 2.2997375. Sequentialfabab260's hyper parameters: Current learning rate is 0.00996810207336523. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:02 INFO  DistriOptimizer$:408 - [Epoch 1 4608/60000][Iteration 18][Wall Clock 3.496619441s] Trained 256 records in 0.131112969 seconds. Throughput is 1952.5148 records/second. Loss is 2.3000786. Sequentialfabab260's hyper parameters: Current learning rate is 0.009966115208291807. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:03 INFO  DistriOptimizer$:408 - [Epoch 1 4864/60000][Iteration 19][Wall Clock 3.671706465s] Trained 256 records in 0.175087024 seconds. Throughput is 1462.1301 records/second. Loss is 2.2964573. Sequentialfabab260's hyper parameters: Current learning rate is 0.00996412913511359. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:03 INFO  DistriOptimizer$:408 - [Epoch 1 5120/60000][Iteration 20][Wall Clock 3.802537058s] Trained 256 records in 0.130830593 seconds. Throughput is 1956.729 records/second. Loss is 2.3021255. Sequentialfabab260's hyper parameters: Current learning rate is 0.009962143853357242. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:03 INFO  DistriOptimizer$:408 - [Epoch 1 5376/60000][Iteration 21][Wall Clock 3.930320245s] Trained 256 records in 0.127783187 seconds. Throughput is 2003.3933 records/second. Loss is 2.3051996. Sequentialfabab260's hyper parameters: Current learning rate is 0.0099601593625498. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:03 INFO  DistriOptimizer$:408 - [Epoch 1 5632/60000][Iteration 22][Wall Clock 4.071813504s] Trained 256 records in 0.141493259 seconds. Throughput is 1809.2734 records/second. Loss is 2.2994733. Sequentialfabab260's hyper parameters: Current learning rate is 0.009958175662218682. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:03 INFO  DistriOptimizer$:408 - [Epoch 1 5888/60000][Iteration 23][Wall Clock 4.205821521s] Trained 256 records in 0.134008017 seconds. Throughput is 1910.3334 records/second. Loss is 2.3033543. Sequentialfabab260's hyper parameters: Current learning rate is 0.009956192751891677. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:03 INFO  DistriOptimizer$:408 - [Epoch 1 6144/60000][Iteration 24][Wall Clock 4.37905866s] Trained 256 records in 0.173237139 seconds. Throughput is 1477.7432 records/second. Loss is 2.2921023. Sequentialfabab260's hyper parameters: Current learning rate is 0.009954210631096954. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:03 INFO  DistriOptimizer$:408 - [Epoch 1 6400/60000][Iteration 25][Wall Clock 4.497048807s] Trained 256 records in 0.117990147 seconds. Throughput is 2169.6729 records/second. Loss is 2.288338. Sequentialfabab260's hyper parameters: Current learning rate is 0.009952229299363059. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:04 INFO  DistriOptimizer$:408 - [Epoch 1 6656/60000][Iteration 26][Wall Clock 4.628951702s] Trained 256 records in 0.131902895 seconds. Throughput is 1940.8218 records/second. Loss is 2.3013403. Sequentialfabab260's hyper parameters: Current learning rate is 0.009950248756218907. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:04 INFO  DistriOptimizer$:408 - [Epoch 1 6912/60000][Iteration 27][Wall Clock 4.767025298s] Trained 256 records in 0.138073596 seconds. Throughput is 1854.0837 records/second. Loss is 2.2914119. Sequentialfabab260's hyper parameters: Current learning rate is 0.00994826900119379. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:04 INFO  DistriOptimizer$:408 - [Epoch 1 7168/60000][Iteration 28][Wall Clock 4.884192794s] Trained 256 records in 0.117167496 seconds. Throughput is 2184.9062 records/second. Loss is 2.2850776. Sequentialfabab260's hyper parameters: Current learning rate is 0.009946290033817386. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:04 INFO  DistriOptimizer$:408 - [Epoch 1 7424/60000][Iteration 29][Wall Clock 5.043218781s] Trained 256 records in 0.159025987 seconds. Throughput is 1609.7998 records/second. Loss is 2.288875. Sequentialfabab260's hyper parameters: Current learning rate is 0.00994431185361973. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:04 INFO  DistriOptimizer$:408 - [Epoch 1 7680/60000][Iteration 30][Wall Clock 5.155400167s] Trained 256 records in 0.112181386 seconds. Throughput is 2282.0186 records/second. Loss is 2.2927973. Sequentialfabab260's hyper parameters: Current learning rate is 0.00994233446013124. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:04 INFO  DistriOptimizer$:408 - [Epoch 1 7936/60000][Iteration 31][Wall Clock 5.272801396s] Trained 256 records in 0.117401229 seconds. Throughput is 2180.5564 records/second. Loss is 2.2823544. Sequentialfabab260's hyper parameters: Current learning rate is 0.009940357852882704. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:04 INFO  DistriOptimizer$:408 - [Epoch 1 8192/60000][Iteration 32][Wall Clock 5.381936199s] Trained 256 records in 0.109134803 seconds. Throughput is 2345.723 records/second. Loss is 2.293315. Sequentialfabab260's hyper parameters: Current learning rate is 0.009938382031405287. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:04 INFO  DistriOptimizer$:408 - [Epoch 1 8448/60000][Iteration 33][Wall Clock 5.50030756s] Trained 256 records in 0.118371361 seconds. Throughput is 2162.6853 records/second. Loss is 2.281393. Sequentialfabab260's hyper parameters: Current learning rate is 0.009936406995230525. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:05 INFO  DistriOptimizer$:408 - [Epoch 1 8704/60000][Iteration 34][Wall Clock 5.659571315s] Trained 256 records in 0.159263755 seconds. Throughput is 1607.3965 records/second. Loss is 2.278154. Sequentialfabab260's hyper parameters: Current learning rate is 0.009934432743890324. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:05 INFO  DistriOptimizer$:408 - [Epoch 1 8960/60000][Iteration 35][Wall Clock 5.777598281s] Trained 256 records in 0.118026966 seconds. Throughput is 2168.9958 records/second. Loss is 2.2745671. Sequentialfabab260's hyper parameters: Current learning rate is 0.009932459276916966. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:05 INFO  DistriOptimizer$:408 - [Epoch 1 9216/60000][Iteration 36][Wall Clock 5.89329508s] Trained 256 records in 0.115696799 seconds. Throughput is 2212.6802 records/second. Loss is 2.2808857. Sequentialfabab260's hyper parameters: Current learning rate is 0.0099304865938431. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:05 INFO  DistriOptimizer$:408 - [Epoch 1 9472/60000][Iteration 37][Wall Clock 6.016122051s] Trained 256 records in 0.122826971 seconds. Throughput is 2084.2327 records/second. Loss is 2.276919. Sequentialfabab260's hyper parameters: Current learning rate is 0.009928514694201746. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:05 INFO  DistriOptimizer$:408 - [Epoch 1 9728/60000][Iteration 38][Wall Clock 6.137809068s] Trained 256 records in 0.121687017 seconds. Throughput is 2103.7578 records/second. Loss is 2.2713468. Sequentialfabab260's hyper parameters: Current learning rate is 0.009926543577526304. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:05 INFO  DistriOptimizer$:408 - [Epoch 1 9984/60000][Iteration 39][Wall Clock 6.280744276s] Trained 256 records in 0.142935208 seconds. Throughput is 1791.0214 records/second. Loss is 2.2776747. Sequentialfabab260's hyper parameters: Current learning rate is 0.009924573243350535. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:05 INFO  DistriOptimizer$:408 - [Epoch 1 10240/60000][Iteration 40][Wall Clock 6.400922646s] Trained 256 records in 0.12017837 seconds. Throughput is 2130.167 records/second. Loss is 2.2783825. Sequentialfabab260's hyper parameters: Current learning rate is 0.009922603691208573. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:05 INFO  DistriOptimizer$:408 - [Epoch 1 10496/60000][Iteration 41][Wall Clock 6.505165154s] Trained 256 records in 0.104242508 seconds. Throughput is 2455.812 records/second. Loss is 2.2742558. Sequentialfabab260's hyper parameters: Current learning rate is 0.00992063492063492. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:06 INFO  DistriOptimizer$:408 - [Epoch 1 10752/60000][Iteration 42][Wall Clock 6.60559193s] Trained 256 records in 0.100426776 seconds. Throughput is 2549.1208 records/second. Loss is 2.2710176. Sequentialfabab260's hyper parameters: Current learning rate is 0.009918666931164452. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:06 INFO  DistriOptimizer$:408 - [Epoch 1 11008/60000][Iteration 43][Wall Clock 6.713004986s] Trained 256 records in 0.107413056 seconds. Throughput is 2383.323 records/second. Loss is 2.2726133. Sequentialfabab260's hyper parameters: Current learning rate is 0.009916699722332408. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:06 INFO  DistriOptimizer$:408 - [Epoch 1 11264/60000][Iteration 44][Wall Clock 6.865892659s] Trained 256 records in 0.152887673 seconds. Throughput is 1674.4319 records/second. Loss is 2.2744343. Sequentialfabab260's hyper parameters: Current learning rate is 0.009914733293674401. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:06 INFO  DistriOptimizer$:408 - [Epoch 1 11520/60000][Iteration 45][Wall Clock 6.979039569s] Trained 256 records in 0.11314691 seconds. Throughput is 2262.5452 records/second. Loss is 2.2699094. Sequentialfabab260's hyper parameters: Current learning rate is 0.009912767644726409. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:06 INFO  DistriOptimizer$:408 - [Epoch 1 11776/60000][Iteration 46][Wall Clock 7.085674723s] Trained 256 records in 0.106635154 seconds. Throughput is 2400.7092 records/second. Loss is 2.2706532. Sequentialfabab260's hyper parameters: Current learning rate is 0.009910802775024779. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:06 INFO  DistriOptimizer$:408 - [Epoch 1 12032/60000][Iteration 47][Wall Clock 7.185991095s] Trained 256 records in 0.100316372 seconds. Throughput is 2551.9263 records/second. Loss is 2.2732053. Sequentialfabab260's hyper parameters: Current learning rate is 0.009908838684106223. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:06 INFO  DistriOptimizer$:408 - [Epoch 1 12288/60000][Iteration 48][Wall Clock 7.290563159s] Trained 256 records in 0.104572064 seconds. Throughput is 2448.0725 records/second. Loss is 2.265967. Sequentialfabab260's hyper parameters: Current learning rate is 0.009906875371507825. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:06 INFO  DistriOptimizer$:408 - [Epoch 1 12544/60000][Iteration 49][Wall Clock 7.423938906s] Trained 256 records in 0.133375747 seconds. Throughput is 1919.3894 records/second. Loss is 2.269368. Sequentialfabab260's hyper parameters: Current learning rate is 0.009904912836767036. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:06 INFO  DistriOptimizer$:408 - [Epoch 1 12800/60000][Iteration 50][Wall Clock 7.529081338s] Trained 256 records in 0.105142432 seconds. Throughput is 2434.7925 records/second. Loss is 2.2622225. Sequentialfabab260's hyper parameters: Current learning rate is 0.009902951079421667. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:07 INFO  DistriOptimizer$:408 - [Epoch 1 13056/60000][Iteration 51][Wall Clock 7.63840392s] Trained 256 records in 0.109322582 seconds. Throughput is 2341.6936 records/second. Loss is 2.256267. Sequentialfabab260's hyper parameters: Current learning rate is 0.009900990099009901. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:07 INFO  DistriOptimizer$:408 - [Epoch 1 13312/60000][Iteration 52][Wall Clock 7.755824108s] Trained 256 records in 0.117420188 seconds. Throughput is 2180.2043 records/second. Loss is 2.254971. Sequentialfabab260's hyper parameters: Current learning rate is 0.009899029895070284. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:07 INFO  DistriOptimizer$:408 - [Epoch 1 13568/60000][Iteration 53][Wall Clock 7.866078975s] Trained 256 records in 0.110254867 seconds. Throughput is 2321.8928 records/second. Loss is 2.2570672. Sequentialfabab260's hyper parameters: Current learning rate is 0.009897070467141727. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:07 INFO  DistriOptimizer$:408 - [Epoch 1 13824/60000][Iteration 54][Wall Clock 8.006088905s] Trained 256 records in 0.14000993 seconds. Throughput is 1828.4418 records/second. Loss is 2.2611845. Sequentialfabab260's hyper parameters: Current learning rate is 0.009895111814763508. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:07 INFO  DistriOptimizer$:408 - [Epoch 1 14080/60000][Iteration 55][Wall Clock 8.11809326s] Trained 256 records in 0.112004355 seconds. Throughput is 2285.6255 records/second. Loss is 2.2572916. Sequentialfabab260's hyper parameters: Current learning rate is 0.009893153937475268. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:07 INFO  DistriOptimizer$:408 - [Epoch 1 14336/60000][Iteration 56][Wall Clock 8.224048499s] Trained 256 records in 0.105955239 seconds. Throughput is 2416.1147 records/second. Loss is 2.2566102. Sequentialfabab260's hyper parameters: Current learning rate is 0.009891196834817014. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:07 INFO  DistriOptimizer$:408 - [Epoch 1 14592/60000][Iteration 57][Wall Clock 8.337803301s] Trained 256 records in 0.113754802 seconds. Throughput is 2250.4543 records/second. Loss is 2.2524319. Sequentialfabab260's hyper parameters: Current learning rate is 0.009889240506329113. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:07 INFO  DistriOptimizer$:408 - [Epoch 1 14848/60000][Iteration 58][Wall Clock 8.449772062s] Trained 256 records in 0.111968761 seconds. Throughput is 2286.352 records/second. Loss is 2.2496746. Sequentialfabab260's hyper parameters: Current learning rate is 0.009887284951552304. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:08 INFO  DistriOptimizer$:408 - [Epoch 1 15104/60000][Iteration 59][Wall Clock 8.589614676s] Trained 256 records in 0.139842614 seconds. Throughput is 1830.6294 records/second. Loss is 2.2524219. Sequentialfabab260's hyper parameters: Current learning rate is 0.009885330170027679. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:08 INFO  DistriOptimizer$:408 - [Epoch 1 15360/60000][Iteration 60][Wall Clock 8.697628104s] Trained 256 records in 0.108013428 seconds. Throughput is 2370.0757 records/second. Loss is 2.251968. Sequentialfabab260's hyper parameters: Current learning rate is 0.0098833761612967. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:08 INFO  DistriOptimizer$:408 - [Epoch 1 15616/60000][Iteration 61][Wall Clock 8.80741165s] Trained 256 records in 0.109783546 seconds. Throughput is 2331.8613 records/second. Loss is 2.2502518. Sequentialfabab260's hyper parameters: Current learning rate is 0.009881422924901186. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:08 INFO  DistriOptimizer$:408 - [Epoch 1 15872/60000][Iteration 62][Wall Clock 8.911415211s] Trained 256 records in 0.104003561 seconds. Throughput is 2461.454 records/second. Loss is 2.2500956. Sequentialfabab260's hyper parameters: Current learning rate is 0.009879470460383323. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:08 INFO  DistriOptimizer$:408 - [Epoch 1 16128/60000][Iteration 63][Wall Clock 9.017944787s] Trained 256 records in 0.106529576 seconds. Throughput is 2403.0884 records/second. Loss is 2.2457952. Sequentialfabab260's hyper parameters: Current learning rate is 0.009877518767285659. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:08 INFO  DistriOptimizer$:408 - [Epoch 1 16384/60000][Iteration 64][Wall Clock 9.14855698s] Trained 256 records in 0.130612193 seconds. Throughput is 1960.0007 records/second. Loss is 2.2408323. Sequentialfabab260's hyper parameters: Current learning rate is 0.009875567845151097. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:08 INFO  DistriOptimizer$:408 - [Epoch 1 16640/60000][Iteration 65][Wall Clock 9.251398703s] Trained 256 records in 0.102841723 seconds. Throughput is 2489.2622 records/second. Loss is 2.245874. Sequentialfabab260's hyper parameters: Current learning rate is 0.009873617693522907. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:08 INFO  DistriOptimizer$:408 - [Epoch 1 16896/60000][Iteration 66][Wall Clock 9.357169825s] Trained 256 records in 0.105771122 seconds. Throughput is 2420.3203 records/second. Loss is 2.247693. Sequentialfabab260's hyper parameters: Current learning rate is 0.00987166831194472. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:08 INFO  DistriOptimizer$:408 - [Epoch 1 17152/60000][Iteration 67][Wall Clock 9.452749995s] Trained 256 records in 0.09558017 seconds. Throughput is 2678.38 records/second. Loss is 2.2336128. Sequentialfabab260's hyper parameters: Current learning rate is 0.00986971969996052. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:08 INFO  DistriOptimizer$:408 - [Epoch 1 17408/60000][Iteration 68][Wall Clock 9.542345019s] Trained 256 records in 0.089595024 seconds. Throughput is 2857.3015 records/second. Loss is 2.23462. Sequentialfabab260's hyper parameters: Current learning rate is 0.009867771857114663. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:09 INFO  DistriOptimizer$:408 - [Epoch 1 17664/60000][Iteration 69][Wall Clock 9.659973804s] Trained 256 records in 0.117628785 seconds. Throughput is 2176.3381 records/second. Loss is 2.239139. Sequentialfabab260's hyper parameters: Current learning rate is 0.009865824782951855. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:09 INFO  DistriOptimizer$:408 - [Epoch 1 17920/60000][Iteration 70][Wall Clock 9.75564274s] Trained 256 records in 0.095668936 seconds. Throughput is 2675.8948 records/second. Loss is 2.2305775. Sequentialfabab260's hyper parameters: Current learning rate is 0.009863878477017163. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:09 INFO  DistriOptimizer$:408 - [Epoch 1 18176/60000][Iteration 71][Wall Clock 9.856356656s] Trained 256 records in 0.100713916 seconds. Throughput is 2541.8533 records/second. Loss is 2.2317147. Sequentialfabab260's hyper parameters: Current learning rate is 0.009861932938856016. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:09 INFO  DistriOptimizer$:408 - [Epoch 1 18432/60000][Iteration 72][Wall Clock 9.956795358s] Trained 256 records in 0.100438702 seconds. Throughput is 2548.8184 records/second. Loss is 2.2340689. Sequentialfabab260's hyper parameters: Current learning rate is 0.009859988168014198. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:09 INFO  DistriOptimizer$:408 - [Epoch 1 18688/60000][Iteration 73][Wall Clock 10.058500205s] Trained 256 records in 0.101704847 seconds. Throughput is 2517.0876 records/second. Loss is 2.2305875. Sequentialfabab260's hyper parameters: Current learning rate is 0.009858044164037856. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:09 INFO  DistriOptimizer$:408 - [Epoch 1 18944/60000][Iteration 74][Wall Clock 10.175317432s] Trained 256 records in 0.116817227 seconds. Throughput is 2191.4575 records/second. Loss is 2.2262416. Sequentialfabab260's hyper parameters: Current learning rate is 0.009856100926473488. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:09 INFO  DistriOptimizer$:408 - [Epoch 1 19200/60000][Iteration 75][Wall Clock 10.267833339s] Trained 256 records in 0.092515907 seconds. Throughput is 2767.0918 records/second. Loss is 2.2286665. Sequentialfabab260's hyper parameters: Current learning rate is 0.009854158454867956. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:09 INFO  DistriOptimizer$:408 - [Epoch 1 19456/60000][Iteration 76][Wall Clock 10.36529214s] Trained 256 records in 0.097458801 seconds. Throughput is 2626.751 records/second. Loss is 2.2321136. Sequentialfabab260's hyper parameters: Current learning rate is 0.009852216748768475. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:09 INFO  DistriOptimizer$:408 - [Epoch 1 19712/60000][Iteration 77][Wall Clock 10.464276368s] Trained 256 records in 0.098984228 seconds. Throughput is 2586.2708 records/second. Loss is 2.2227147. Sequentialfabab260's hyper parameters: Current learning rate is 0.009850275807722615. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:09 INFO  DistriOptimizer$:408 - [Epoch 1 19968/60000][Iteration 78][Wall Clock 10.554758936s] Trained 256 records in 0.090482568 seconds. Throughput is 2829.2742 records/second. Loss is 2.2290914. Sequentialfabab260's hyper parameters: Current learning rate is 0.009848335631278314. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:10 INFO  DistriOptimizer$:408 - [Epoch 1 20224/60000][Iteration 79][Wall Clock 10.663505855s] Trained 256 records in 0.108746919 seconds. Throughput is 2354.0898 records/second. Loss is 2.2327921. Sequentialfabab260's hyper parameters: Current learning rate is 0.009846396218983852. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:10 INFO  DistriOptimizer$:408 - [Epoch 1 20480/60000][Iteration 80][Wall Clock 10.752427042s] Trained 256 records in 0.088921187 seconds. Throughput is 2878.9539 records/second. Loss is 2.2193267. Sequentialfabab260's hyper parameters: Current learning rate is 0.009844457570387872. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:10 INFO  DistriOptimizer$:408 - [Epoch 1 20736/60000][Iteration 81][Wall Clock 10.857252643s] Trained 256 records in 0.104825601 seconds. Throughput is 2442.1516 records/second. Loss is 2.2213862. Sequentialfabab260's hyper parameters: Current learning rate is 0.00984251968503937. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:10 INFO  DistriOptimizer$:408 - [Epoch 1 20992/60000][Iteration 82][Wall Clock 10.952957717s] Trained 256 records in 0.095705074 seconds. Throughput is 2674.8843 records/second. Loss is 2.2163484. Sequentialfabab260's hyper parameters: Current learning rate is 0.0098405825624877. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:10 INFO  DistriOptimizer$:408 - [Epoch 1 21248/60000][Iteration 83][Wall Clock 11.04320334s] Trained 256 records in 0.090245623 seconds. Throughput is 2836.703 records/second. Loss is 2.2159727. Sequentialfabab260's hyper parameters: Current learning rate is 0.009838646202282567. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:10 INFO  DistriOptimizer$:408 - [Epoch 1 21504/60000][Iteration 84][Wall Clock 11.162270223s] Trained 256 records in 0.119066883 seconds. Throughput is 2150.052 records/second. Loss is 2.218384. Sequentialfabab260's hyper parameters: Current learning rate is 0.009836710603974032. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:10 INFO  DistriOptimizer$:408 - [Epoch 1 21760/60000][Iteration 85][Wall Clock 11.259381483s] Trained 256 records in 0.09711126 seconds. Throughput is 2636.1516 records/second. Loss is 2.209782. Sequentialfabab260's hyper parameters: Current learning rate is 0.009834775767112511. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:10 INFO  DistriOptimizer$:408 - [Epoch 1 22016/60000][Iteration 86][Wall Clock 11.346402367s] Trained 256 records in 0.087020884 seconds. Throughput is 2941.8228 records/second. Loss is 2.2073927. Sequentialfabab260's hyper parameters: Current learning rate is 0.009832841691248772. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:10 INFO  DistriOptimizer$:408 - [Epoch 1 22272/60000][Iteration 87][Wall Clock 11.439322464s] Trained 256 records in 0.092920097 seconds. Throughput is 2755.0554 records/second. Loss is 2.2139938. Sequentialfabab260's hyper parameters: Current learning rate is 0.009830908375933936. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:10 INFO  DistriOptimizer$:408 - [Epoch 1 22528/60000][Iteration 88][Wall Clock 11.534786214s] Trained 256 records in 0.09546375 seconds. Throughput is 2681.646 records/second. Loss is 2.1978283. Sequentialfabab260's hyper parameters: Current learning rate is 0.00982897582071948. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:11 INFO  DistriOptimizer$:408 - [Epoch 1 22784/60000][Iteration 89][Wall Clock 11.655160105s] Trained 256 records in 0.120373891 seconds. Throughput is 2126.707 records/second. Loss is 2.2138174. Sequentialfabab260's hyper parameters: Current learning rate is 0.009827044025157232. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:11 INFO  DistriOptimizer$:408 - [Epoch 1 23040/60000][Iteration 90][Wall Clock 11.754302879s] Trained 256 records in 0.099142774 seconds. Throughput is 2582.1348 records/second. Loss is 2.2174957. Sequentialfabab260's hyper parameters: Current learning rate is 0.009825112988799371. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:11 INFO  DistriOptimizer$:408 - [Epoch 1 23296/60000][Iteration 91][Wall Clock 11.852272854s] Trained 256 records in 0.097969975 seconds. Throughput is 2613.0457 records/second. Loss is 2.1946802. Sequentialfabab260's hyper parameters: Current learning rate is 0.009823182711198428. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:11 INFO  DistriOptimizer$:408 - [Epoch 1 23552/60000][Iteration 92][Wall Clock 11.955547368s] Trained 256 records in 0.103274514 seconds. Throughput is 2478.8303 records/second. Loss is 2.2085855. Sequentialfabab260's hyper parameters: Current learning rate is 0.009821253191907287. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:11 INFO  DistriOptimizer$:408 - [Epoch 1 23808/60000][Iteration 93][Wall Clock 12.063839244s] Trained 256 records in 0.108291876 seconds. Throughput is 2363.9814 records/second. Loss is 2.2060368. Sequentialfabab260's hyper parameters: Current learning rate is 0.009819324430479184. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:11 INFO  DistriOptimizer$:408 - [Epoch 1 24064/60000][Iteration 94][Wall Clock 12.187364885s] Trained 256 records in 0.123525641 seconds. Throughput is 2072.444 records/second. Loss is 2.197804. Sequentialfabab260's hyper parameters: Current learning rate is 0.009817396426467702. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:11 INFO  DistriOptimizer$:408 - [Epoch 1 24320/60000][Iteration 95][Wall Clock 12.280509199s] Trained 256 records in 0.093144314 seconds. Throughput is 2748.4233 records/second. Loss is 2.1889822. Sequentialfabab260's hyper parameters: Current learning rate is 0.009815469179426778. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:11 INFO  DistriOptimizer$:408 - [Epoch 1 24576/60000][Iteration 96][Wall Clock 12.375317367s] Trained 256 records in 0.094808168 seconds. Throughput is 2700.1892 records/second. Loss is 2.2032127. Sequentialfabab260's hyper parameters: Current learning rate is 0.009813542688910697. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:11 INFO  DistriOptimizer$:408 - [Epoch 1 24832/60000][Iteration 97][Wall Clock 12.47589065s] Trained 256 records in 0.100573283 seconds. Throughput is 2545.4075 records/second. Loss is 2.185358. Sequentialfabab260's hyper parameters: Current learning rate is 0.009811616954474096. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:12 INFO  DistriOptimizer$:408 - [Epoch 1 25088/60000][Iteration 98][Wall Clock 12.567150257s] Trained 256 records in 0.091259607 seconds. Throughput is 2805.184 records/second. Loss is 2.1855893. Sequentialfabab260's hyper parameters: Current learning rate is 0.009809691975671964. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:12 INFO  DistriOptimizer$:408 - [Epoch 1 25344/60000][Iteration 99][Wall Clock 12.688054788s] Trained 256 records in 0.120904531 seconds. Throughput is 2117.373 records/second. Loss is 2.1875217. Sequentialfabab260's hyper parameters: Current learning rate is 0.00980776775205963. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:12 INFO  DistriOptimizer$:408 - [Epoch 1 25600/60000][Iteration 100][Wall Clock 12.780401452s] Trained 256 records in 0.092346664 seconds. Throughput is 2772.163 records/second. Loss is 2.1908157. Sequentialfabab260's hyper parameters: Current learning rate is 0.009805844283192783. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:12 INFO  DistriOptimizer$:408 - [Epoch 1 25856/60000][Iteration 101][Wall Clock 12.870053167s] Trained 256 records in 0.089651715 seconds. Throughput is 2855.4949 records/second. Loss is 2.1741233. Sequentialfabab260's hyper parameters: Current learning rate is 0.00980392156862745. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:12 INFO  DistriOptimizer$:408 - [Epoch 1 26112/60000][Iteration 102][Wall Clock 12.965022977s] Trained 256 records in 0.09496981 seconds. Throughput is 2695.5935 records/second. Loss is 2.176872. Sequentialfabab260's hyper parameters: Current learning rate is 0.009801999607920015. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:12 INFO  DistriOptimizer$:408 - [Epoch 1 26368/60000][Iteration 103][Wall Clock 13.064378273s] Trained 256 records in 0.099355296 seconds. Throughput is 2576.6116 records/second. Loss is 2.1787617. Sequentialfabab260's hyper parameters: Current learning rate is 0.009800078400627205. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:12 INFO  DistriOptimizer$:408 - [Epoch 1 26624/60000][Iteration 104][Wall Clock 13.167155822s] Trained 256 records in 0.102777549 seconds. Throughput is 2490.8164 records/second. Loss is 2.183481. Sequentialfabab260's hyper parameters: Current learning rate is 0.009798157946306094. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:12 INFO  DistriOptimizer$:408 - [Epoch 1 26880/60000][Iteration 105][Wall Clock 13.252907689s] Trained 256 records in 0.085751867 seconds. Throughput is 2985.3577 records/second. Loss is 2.1686833. Sequentialfabab260's hyper parameters: Current learning rate is 0.009796238244514107. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:12 INFO  DistriOptimizer$:408 - [Epoch 1 27136/60000][Iteration 106][Wall Clock 13.346776953s] Trained 256 records in 0.093869264 seconds. Throughput is 2727.1973 records/second. Loss is 2.1780329. Sequentialfabab260's hyper parameters: Current learning rate is 0.009794319294809012. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:12 INFO  DistriOptimizer$:408 - [Epoch 1 27392/60000][Iteration 107][Wall Clock 13.440317278s] Trained 256 records in 0.093540325 seconds. Throughput is 2736.7876 records/second. Loss is 2.1727011. Sequentialfabab260's hyper parameters: Current learning rate is 0.009792401096748922. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:12 INFO  DistriOptimizer$:408 - [Epoch 1 27648/60000][Iteration 108][Wall Clock 13.527186885s] Trained 256 records in 0.086869607 seconds. Throughput is 2946.9456 records/second. Loss is 2.158456. Sequentialfabab260's hyper parameters: Current learning rate is 0.009790483649892304. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:13 INFO  DistriOptimizer$:408 - [Epoch 1 27904/60000][Iteration 109][Wall Clock 13.634037697s] Trained 256 records in 0.106850812 seconds. Throughput is 2395.864 records/second. Loss is 2.160509. Sequentialfabab260's hyper parameters: Current learning rate is 0.009788566953797963. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:13 INFO  DistriOptimizer$:408 - [Epoch 1 28160/60000][Iteration 110][Wall Clock 13.724121406s] Trained 256 records in 0.090083709 seconds. Throughput is 2841.8013 records/second. Loss is 2.1650333. Sequentialfabab260's hyper parameters: Current learning rate is 0.009786651008025053. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:13 INFO  DistriOptimizer$:408 - [Epoch 1 28416/60000][Iteration 111][Wall Clock 13.817085255s] Trained 256 records in 0.092963849 seconds. Throughput is 2753.7585 records/second. Loss is 2.1749883. Sequentialfabab260's hyper parameters: Current learning rate is 0.009784735812133072. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:13 INFO  DistriOptimizer$:408 - [Epoch 1 28672/60000][Iteration 112][Wall Clock 13.916233706s] Trained 256 records in 0.099148451 seconds. Throughput is 2581.9868 records/second. Loss is 2.153932. Sequentialfabab260's hyper parameters: Current learning rate is 0.009782821365681862. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:13 INFO  DistriOptimizer$:408 - [Epoch 1 28928/60000][Iteration 113][Wall Clock 14.009763975s] Trained 256 records in 0.093530269 seconds. Throughput is 2737.0818 records/second. Loss is 2.167817. Sequentialfabab260's hyper parameters: Current learning rate is 0.009780907668231613. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:13 INFO  DistriOptimizer$:408 - [Epoch 1 29184/60000][Iteration 114][Wall Clock 14.118205436s] Trained 256 records in 0.108441461 seconds. Throughput is 2360.7207 records/second. Loss is 2.1639788. Sequentialfabab260's hyper parameters: Current learning rate is 0.009778994719342852. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:13 INFO  DistriOptimizer$:408 - [Epoch 1 29440/60000][Iteration 115][Wall Clock 14.206611457s] Trained 256 records in 0.088406021 seconds. Throughput is 2895.7305 records/second. Loss is 2.1540897. Sequentialfabab260's hyper parameters: Current learning rate is 0.009777082518576457. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:13 INFO  DistriOptimizer$:408 - [Epoch 1 29696/60000][Iteration 116][Wall Clock 14.295415012s] Trained 256 records in 0.088803555 seconds. Throughput is 2882.7676 records/second. Loss is 2.149382. Sequentialfabab260's hyper parameters: Current learning rate is 0.009775171065493648. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:13 INFO  DistriOptimizer$:408 - [Epoch 1 29952/60000][Iteration 117][Wall Clock 14.387176059s] Trained 256 records in 0.091761047 seconds. Throughput is 2789.855 records/second. Loss is 2.138924. Sequentialfabab260's hyper parameters: Current learning rate is 0.009773260359655981. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:13 INFO  DistriOptimizer$:408 - [Epoch 1 30208/60000][Iteration 118][Wall Clock 14.466823964s] Trained 256 records in 0.079647905 seconds. Throughput is 3214.146 records/second. Loss is 2.1446002. Sequentialfabab260's hyper parameters: Current learning rate is 0.009771350400625366. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:14 INFO  DistriOptimizer$:408 - [Epoch 1 30464/60000][Iteration 119][Wall Clock 14.573281163s] Trained 256 records in 0.106457199 seconds. Throughput is 2404.7224 records/second. Loss is 2.126767. Sequentialfabab260's hyper parameters: Current learning rate is 0.009769441187964047. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:14 INFO  DistriOptimizer$:408 - [Epoch 1 30720/60000][Iteration 120][Wall Clock 14.669092289s] Trained 256 records in 0.095811126 seconds. Throughput is 2671.9233 records/second. Loss is 2.1504495. Sequentialfabab260's hyper parameters: Current learning rate is 0.009767532721234616. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:14 INFO  DistriOptimizer$:408 - [Epoch 1 30976/60000][Iteration 121][Wall Clock 14.759082161s] Trained 256 records in 0.089989872 seconds. Throughput is 2844.7646 records/second. Loss is 2.151381. Sequentialfabab260's hyper parameters: Current learning rate is 0.009765625. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:14 INFO  DistriOptimizer$:408 - [Epoch 1 31232/60000][Iteration 122][Wall Clock 14.855921227s] Trained 256 records in 0.096839066 seconds. Throughput is 2643.5613 records/second. Loss is 2.1482887. Sequentialfabab260's hyper parameters: Current learning rate is 0.009763718023823472. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:14 INFO  DistriOptimizer$:408 - [Epoch 1 31488/60000][Iteration 123][Wall Clock 14.957392832s] Trained 256 records in 0.101471605 seconds. Throughput is 2522.8733 records/second. Loss is 2.1346836. Sequentialfabab260's hyper parameters: Current learning rate is 0.009761811792268645. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:14 INFO  DistriOptimizer$:408 - [Epoch 1 31744/60000][Iteration 124][Wall Clock 15.078890998s] Trained 256 records in 0.121498166 seconds. Throughput is 2107.0276 records/second. Loss is 2.1495984. Sequentialfabab260's hyper parameters: Current learning rate is 0.009759906304899474. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:14 INFO  DistriOptimizer$:408 - [Epoch 1 32000/60000][Iteration 125][Wall Clock 15.173446994s] Trained 256 records in 0.094555996 seconds. Throughput is 2707.3904 records/second. Loss is 2.1230376. Sequentialfabab260's hyper parameters: Current learning rate is 0.00975800156128025. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:14 INFO  DistriOptimizer$:408 - [Epoch 1 32256/60000][Iteration 126][Wall Clock 15.265108561s] Trained 256 records in 0.091661567 seconds. Throughput is 2792.8828 records/second. Loss is 2.129662. Sequentialfabab260's hyper parameters: Current learning rate is 0.009756097560975611. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:14 INFO  DistriOptimizer$:408 - [Epoch 1 32512/60000][Iteration 127][Wall Clock 15.351768493s] Trained 256 records in 0.086659932 seconds. Throughput is 2954.0757 records/second. Loss is 2.1407773. Sequentialfabab260's hyper parameters: Current learning rate is 0.009754194303550527. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:14 INFO  DistriOptimizer$:408 - [Epoch 1 32768/60000][Iteration 128][Wall Clock 15.441000501s] Trained 256 records in 0.089232008 seconds. Throughput is 2868.9258 records/second. Loss is 2.1051204. Sequentialfabab260's hyper parameters: Current learning rate is 0.009752291788570313. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:15 INFO  DistriOptimizer$:408 - [Epoch 1 33024/60000][Iteration 129][Wall Clock 15.547817077s] Trained 256 records in 0.106816576 seconds. Throughput is 2396.6318 records/second. Loss is 2.1369672. Sequentialfabab260's hyper parameters: Current learning rate is 0.009750390015600624. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:15 INFO  DistriOptimizer$:408 - [Epoch 1 33280/60000][Iteration 130][Wall Clock 15.637037493s] Trained 256 records in 0.089220416 seconds. Throughput is 2869.2983 records/second. Loss is 2.1176221. Sequentialfabab260's hyper parameters: Current learning rate is 0.009748488984207448. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:15 INFO  DistriOptimizer$:408 - [Epoch 1 33536/60000][Iteration 131][Wall Clock 15.723246405s] Trained 256 records in 0.086208912 seconds. Throughput is 2969.5308 records/second. Loss is 2.077725. Sequentialfabab260's hyper parameters: Current learning rate is 0.009746588693957114. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:15 INFO  DistriOptimizer$:408 - [Epoch 1 33792/60000][Iteration 132][Wall Clock 15.807330907s] Trained 256 records in 0.084084502 seconds. Throughput is 3044.5562 records/second. Loss is 2.106146. Sequentialfabab260's hyper parameters: Current learning rate is 0.009744689144416294. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:15 INFO  DistriOptimizer$:408 - [Epoch 1 34048/60000][Iteration 133][Wall Clock 15.883112471s] Trained 256 records in 0.075781564 seconds. Throughput is 3378.1304 records/second. Loss is 2.1001182. Sequentialfabab260's hyper parameters: Current learning rate is 0.009742790335151987. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:15 INFO  DistriOptimizer$:408 - [Epoch 1 34304/60000][Iteration 134][Wall Clock 15.985478634s] Trained 256 records in 0.102366163 seconds. Throughput is 2500.8264 records/second. Loss is 2.0992818. Sequentialfabab260's hyper parameters: Current learning rate is 0.009740892265731542. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:15 INFO  DistriOptimizer$:408 - [Epoch 1 34560/60000][Iteration 135][Wall Clock 16.065530635s] Trained 256 records in 0.080052001 seconds. Throughput is 3197.9211 records/second. Loss is 2.093573. Sequentialfabab260's hyper parameters: Current learning rate is 0.009738994935722634. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:15 INFO  DistriOptimizer$:408 - [Epoch 1 34816/60000][Iteration 136][Wall Clock 16.16010866s] Trained 256 records in 0.094578025 seconds. Throughput is 2706.7598 records/second. Loss is 2.0827475. Sequentialfabab260's hyper parameters: Current learning rate is 0.009737098344693282. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:15 INFO  DistriOptimizer$:408 - [Epoch 1 35072/60000][Iteration 137][Wall Clock 16.248750487s] Trained 256 records in 0.088641827 seconds. Throughput is 2888.027 records/second. Loss is 2.062025. Sequentialfabab260's hyper parameters: Current learning rate is 0.009735202492211837. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:15 INFO  DistriOptimizer$:408 - [Epoch 1 35328/60000][Iteration 138][Wall Clock 16.333320331s] Trained 256 records in 0.084569844 seconds. Throughput is 3027.0837 records/second. Loss is 2.1284947. Sequentialfabab260's hyper parameters: Current learning rate is 0.009733307377846992. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:15 INFO  DistriOptimizer$:408 - [Epoch 1 35584/60000][Iteration 139][Wall Clock 16.436819151s] Trained 256 records in 0.10349882 seconds. Throughput is 2473.4583 records/second. Loss is 2.095836. Sequentialfabab260's hyper parameters: Current learning rate is 0.009731413001167769. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:15 INFO  DistriOptimizer$:408 - [Epoch 1 35840/60000][Iteration 140][Wall Clock 16.536910028s] Trained 256 records in 0.100090877 seconds. Throughput is 2557.6758 records/second. Loss is 2.091846. Sequentialfabab260's hyper parameters: Current learning rate is 0.00972951936174353. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:16 INFO  DistriOptimizer$:408 - [Epoch 1 36096/60000][Iteration 141][Wall Clock 16.628552118s] Trained 256 records in 0.09164209 seconds. Throughput is 2793.4763 records/second. Loss is 2.0876992. Sequentialfabab260's hyper parameters: Current learning rate is 0.009727626459143969. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:16 INFO  DistriOptimizer$:408 - [Epoch 1 36352/60000][Iteration 142][Wall Clock 16.713185333s] Trained 256 records in 0.084633215 seconds. Throughput is 3024.8171 records/second. Loss is 2.0853462. Sequentialfabab260's hyper parameters: Current learning rate is 0.009725734292939117. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:16 INFO  DistriOptimizer$:408 - [Epoch 1 36608/60000][Iteration 143][Wall Clock 16.804253671s] Trained 256 records in 0.091068338 seconds. Throughput is 2811.076 records/second. Loss is 2.0665207. Sequentialfabab260's hyper parameters: Current learning rate is 0.009723842862699339. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:16 INFO  DistriOptimizer$:408 - [Epoch 1 36864/60000][Iteration 144][Wall Clock 16.90186827s] Trained 256 records in 0.097614599 seconds. Throughput is 2622.5586 records/second. Loss is 2.062241. Sequentialfabab260's hyper parameters: Current learning rate is 0.009721952167995334. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:16 INFO  DistriOptimizer$:408 - [Epoch 1 37120/60000][Iteration 145][Wall Clock 16.997622238s] Trained 256 records in 0.095753968 seconds. Throughput is 2673.5186 records/second. Loss is 2.072408. Sequentialfabab260's hyper parameters: Current learning rate is 0.009720062208398135. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:16 INFO  DistriOptimizer$:408 - [Epoch 1 37376/60000][Iteration 146][Wall Clock 17.080422665s] Trained 256 records in 0.082800427 seconds. Throughput is 3091.7715 records/second. Loss is 2.0716844. Sequentialfabab260's hyper parameters: Current learning rate is 0.009718172983479108. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:16 INFO  DistriOptimizer$:408 - [Epoch 1 37632/60000][Iteration 147][Wall Clock 17.161548917s] Trained 256 records in 0.081126252 seconds. Throughput is 3155.5754 records/second. Loss is 2.1067326. Sequentialfabab260's hyper parameters: Current learning rate is 0.009716284492809951. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:16 INFO  DistriOptimizer$:408 - [Epoch 1 37888/60000][Iteration 148][Wall Clock 17.243362884s] Trained 256 records in 0.081813967 seconds. Throughput is 3129.05 records/second. Loss is 2.0696695. Sequentialfabab260's hyper parameters: Current learning rate is 0.009714396735962695. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:16 INFO  DistriOptimizer$:408 - [Epoch 1 38144/60000][Iteration 149][Wall Clock 17.349471618s] Trained 256 records in 0.106108734 seconds. Throughput is 2412.6196 records/second. Loss is 2.064696. Sequentialfabab260's hyper parameters: Current learning rate is 0.009712509712509712. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:16 INFO  DistriOptimizer$:408 - [Epoch 1 38400/60000][Iteration 150][Wall Clock 17.439260932s] Trained 256 records in 0.089789314 seconds. Throughput is 2851.1187 records/second. Loss is 2.0750172. Sequentialfabab260's hyper parameters: Current learning rate is 0.009710623422023694. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:16 INFO  DistriOptimizer$:408 - [Epoch 1 38656/60000][Iteration 151][Wall Clock 17.521508245s] Trained 256 records in 0.082247313 seconds. Throughput is 3112.5637 records/second. Loss is 2.0656679. Sequentialfabab260's hyper parameters: Current learning rate is 0.009708737864077669. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:17 INFO  DistriOptimizer$:408 - [Epoch 1 38912/60000][Iteration 152][Wall Clock 17.604333812s] Trained 256 records in 0.082825567 seconds. Throughput is 3090.8333 records/second. Loss is 2.0483687. Sequentialfabab260's hyper parameters: Current learning rate is 0.009706853038245. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:17 INFO  DistriOptimizer$:408 - [Epoch 1 39168/60000][Iteration 153][Wall Clock 17.692874932s] Trained 256 records in 0.08854112 seconds. Throughput is 2891.312 records/second. Loss is 2.0351336. Sequentialfabab260's hyper parameters: Current learning rate is 0.00970496894409938. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:17 INFO  DistriOptimizer$:408 - [Epoch 1 39424/60000][Iteration 154][Wall Clock 17.791245579s] Trained 256 records in 0.098370647 seconds. Throughput is 2602.4023 records/second. Loss is 2.064065. Sequentialfabab260's hyper parameters: Current learning rate is 0.009703085581214826. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:17 INFO  DistriOptimizer$:408 - [Epoch 1 39680/60000][Iteration 155][Wall Clock 17.877961639s] Trained 256 records in 0.08671606 seconds. Throughput is 2952.1636 records/second. Loss is 2.0096633. Sequentialfabab260's hyper parameters: Current learning rate is 0.009701202949165696. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:17 INFO  DistriOptimizer$:408 - [Epoch 1 39936/60000][Iteration 156][Wall Clock 17.9657759s] Trained 256 records in 0.087814261 seconds. Throughput is 2915.244 records/second. Loss is 2.0679023. Sequentialfabab260's hyper parameters: Current learning rate is 0.009699321047526674. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:17 INFO  DistriOptimizer$:408 - [Epoch 1 40192/60000][Iteration 157][Wall Clock 18.051259902s] Trained 256 records in 0.085484002 seconds. Throughput is 2994.7122 records/second. Loss is 2.0567312. Sequentialfabab260's hyper parameters: Current learning rate is 0.00969743987587277. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:17 INFO  DistriOptimizer$:408 - [Epoch 1 40448/60000][Iteration 158][Wall Clock 18.133954581s] Trained 256 records in 0.082694679 seconds. Throughput is 3095.725 records/second. Loss is 2.0294652. Sequentialfabab260's hyper parameters: Current learning rate is 0.009695559433779328. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:17 INFO  DistriOptimizer$:408 - [Epoch 1 40704/60000][Iteration 159][Wall Clock 18.231652783s] Trained 256 records in 0.097698202 seconds. Throughput is 2620.3142 records/second. Loss is 2.018095. Sequentialfabab260's hyper parameters: Current learning rate is 0.009693679720822024. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:17 INFO  DistriOptimizer$:408 - [Epoch 1 40960/60000][Iteration 160][Wall Clock 18.31888221s] Trained 256 records in 0.087229427 seconds. Throughput is 2934.7893 records/second. Loss is 2.018436. Sequentialfabab260's hyper parameters: Current learning rate is 0.009691800736576855. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:17 INFO  DistriOptimizer$:408 - [Epoch 1 41216/60000][Iteration 161][Wall Clock 18.397456086s] Trained 256 records in 0.078573876 seconds. Throughput is 3258.0803 records/second. Loss is 2.042728. Sequentialfabab260's hyper parameters: Current learning rate is 0.009689922480620155. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:17 INFO  DistriOptimizer$:408 - [Epoch 1 41472/60000][Iteration 162][Wall Clock 18.485678442s] Trained 256 records in 0.088222356 seconds. Throughput is 2901.7588 records/second. Loss is 2.024766. Sequentialfabab260's hyper parameters: Current learning rate is 0.00968804495252858. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:18 INFO  DistriOptimizer$:408 - [Epoch 1 41728/60000][Iteration 163][Wall Clock 18.569698534s] Trained 256 records in 0.084020092 seconds. Throughput is 3046.8901 records/second. Loss is 2.021907. Sequentialfabab260's hyper parameters: Current learning rate is 0.009686168151879117. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:18 INFO  DistriOptimizer$:408 - [Epoch 1 41984/60000][Iteration 164][Wall Clock 18.675143927s] Trained 256 records in 0.105445393 seconds. Throughput is 2427.7969 records/second. Loss is 2.0041027. Sequentialfabab260's hyper parameters: Current learning rate is 0.00968429207824908. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:18 INFO  DistriOptimizer$:408 - [Epoch 1 42240/60000][Iteration 165][Wall Clock 18.767859144s] Trained 256 records in 0.092715217 seconds. Throughput is 2761.1433 records/second. Loss is 2.0106056. Sequentialfabab260's hyper parameters: Current learning rate is 0.009682416731216113. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:18 INFO  DistriOptimizer$:408 - [Epoch 1 42496/60000][Iteration 166][Wall Clock 18.854369188s] Trained 256 records in 0.086510044 seconds. Throughput is 2959.1938 records/second. Loss is 1.9875635. Sequentialfabab260's hyper parameters: Current learning rate is 0.009680542110358181. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:18 INFO  DistriOptimizer$:408 - [Epoch 1 42752/60000][Iteration 167][Wall Clock 18.953472861s] Trained 256 records in 0.099103673 seconds. Throughput is 2583.1536 records/second. Loss is 1.9898055. Sequentialfabab260's hyper parameters: Current learning rate is 0.009678668215253582. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:18 INFO  DistriOptimizer$:408 - [Epoch 1 43008/60000][Iteration 168][Wall Clock 19.029560678s] Trained 256 records in 0.076087817 seconds. Throughput is 3364.5334 records/second. Loss is 2.0072105. Sequentialfabab260's hyper parameters: Current learning rate is 0.009676795045480935. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:18 INFO  DistriOptimizer$:408 - [Epoch 1 43264/60000][Iteration 169][Wall Clock 19.13874396s] Trained 256 records in 0.109183282 seconds. Throughput is 2344.6814 records/second. Loss is 2.0252967. Sequentialfabab260's hyper parameters: Current learning rate is 0.009674922600619194. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:18 INFO  DistriOptimizer$:408 - [Epoch 1 43520/60000][Iteration 170][Wall Clock 19.222571229s] Trained 256 records in 0.083827269 seconds. Throughput is 3053.8987 records/second. Loss is 2.0407863. Sequentialfabab260's hyper parameters: Current learning rate is 0.009673050880247629. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:18 INFO  DistriOptimizer$:408 - [Epoch 1 43776/60000][Iteration 171][Wall Clock 19.306217826s] Trained 256 records in 0.083646597 seconds. Throughput is 3060.495 records/second. Loss is 1.9931965. Sequentialfabab260's hyper parameters: Current learning rate is 0.009671179883945842. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:18 INFO  DistriOptimizer$:408 - [Epoch 1 44032/60000][Iteration 172][Wall Clock 19.391015384s] Trained 256 records in 0.084797558 seconds. Throughput is 3018.9548 records/second. Loss is 1.9974179. Sequentialfabab260's hyper parameters: Current learning rate is 0.009669309611293754. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:18 INFO  DistriOptimizer$:408 - [Epoch 1 44288/60000][Iteration 173][Wall Clock 19.475487764s] Trained 256 records in 0.08447238 seconds. Throughput is 3030.5764 records/second. Loss is 1.9924338. Sequentialfabab260's hyper parameters: Current learning rate is 0.009667440061871617. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:19 INFO  DistriOptimizer$:408 - [Epoch 1 44544/60000][Iteration 174][Wall Clock 19.575437566s] Trained 256 records in 0.099949802 seconds. Throughput is 2561.286 records/second. Loss is 1.9701304. Sequentialfabab260's hyper parameters: Current learning rate is 0.009665571235260004. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:19 INFO  DistriOptimizer$:408 - [Epoch 1 44800/60000][Iteration 175][Wall Clock 19.667597509s] Trained 256 records in 0.092159943 seconds. Throughput is 2777.7795 records/second. Loss is 1.9931269. Sequentialfabab260's hyper parameters: Current learning rate is 0.009663703131039815. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:19 INFO  DistriOptimizer$:408 - [Epoch 1 45056/60000][Iteration 176][Wall Clock 19.757503814s] Trained 256 records in 0.089906305 seconds. Throughput is 2847.4087 records/second. Loss is 1.994551. Sequentialfabab260's hyper parameters: Current learning rate is 0.009661835748792272. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:19 INFO  DistriOptimizer$:408 - [Epoch 1 45312/60000][Iteration 177][Wall Clock 19.840599593s] Trained 256 records in 0.083095779 seconds. Throughput is 3080.7822 records/second. Loss is 1.9861522. Sequentialfabab260's hyper parameters: Current learning rate is 0.00965996908809892. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:19 INFO  DistriOptimizer$:408 - [Epoch 1 45568/60000][Iteration 178][Wall Clock 19.93307188s] Trained 256 records in 0.092472287 seconds. Throughput is 2768.397 records/second. Loss is 1.9673793. Sequentialfabab260's hyper parameters: Current learning rate is 0.009658103148541626. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:19 INFO  DistriOptimizer$:408 - [Epoch 1 45824/60000][Iteration 179][Wall Clock 20.032571539s] Trained 256 records in 0.099499659 seconds. Throughput is 2572.8733 records/second. Loss is 1.9903693. Sequentialfabab260's hyper parameters: Current learning rate is 0.009656237929702587. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:19 INFO  DistriOptimizer$:408 - [Epoch 1 46080/60000][Iteration 180][Wall Clock 20.120547653s] Trained 256 records in 0.087976114 seconds. Throughput is 2909.8809 records/second. Loss is 1.9651966. Sequentialfabab260's hyper parameters: Current learning rate is 0.009654373431164317. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:19 INFO  DistriOptimizer$:408 - [Epoch 1 46336/60000][Iteration 181][Wall Clock 20.202654682s] Trained 256 records in 0.082107029 seconds. Throughput is 3117.8816 records/second. Loss is 1.9341981. Sequentialfabab260's hyper parameters: Current learning rate is 0.009652509652509652. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:19 INFO  DistriOptimizer$:408 - [Epoch 1 46592/60000][Iteration 182][Wall Clock 20.289111853s] Trained 256 records in 0.086457171 seconds. Throughput is 2961.0037 records/second. Loss is 1.9813243. Sequentialfabab260's hyper parameters: Current learning rate is 0.009650646593321753. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:19 INFO  DistriOptimizer$:408 - [Epoch 1 46848/60000][Iteration 183][Wall Clock 20.369817329s] Trained 256 records in 0.080705476 seconds. Throughput is 3172.0276 records/second. Loss is 1.966174. Sequentialfabab260's hyper parameters: Current learning rate is 0.0096487842531841. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:19 INFO  DistriOptimizer$:408 - [Epoch 1 47104/60000][Iteration 184][Wall Clock 20.464511353s] Trained 256 records in 0.094694024 seconds. Throughput is 2703.444 records/second. Loss is 1.9799043. Sequentialfabab260's hyper parameters: Current learning rate is 0.009646922631680495. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:20 INFO  DistriOptimizer$:408 - [Epoch 1 47360/60000][Iteration 185][Wall Clock 20.556846096s] Trained 256 records in 0.092334743 seconds. Throughput is 2772.521 records/second. Loss is 1.9179269. Sequentialfabab260's hyper parameters: Current learning rate is 0.009645061728395063. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:20 INFO  DistriOptimizer$:408 - [Epoch 1 47616/60000][Iteration 186][Wall Clock 20.643420547s] Trained 256 records in 0.086574451 seconds. Throughput is 2956.9924 records/second. Loss is 1.9024743. Sequentialfabab260's hyper parameters: Current learning rate is 0.009643201542912247. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:20 INFO  DistriOptimizer$:408 - [Epoch 1 47872/60000][Iteration 187][Wall Clock 20.73260316s] Trained 256 records in 0.089182613 seconds. Throughput is 2870.5146 records/second. Loss is 1.9029992. Sequentialfabab260's hyper parameters: Current learning rate is 0.009641342074816815. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:20 INFO  DistriOptimizer$:408 - [Epoch 1 48128/60000][Iteration 188][Wall Clock 20.827347443s] Trained 256 records in 0.094744283 seconds. Throughput is 2702.01 records/second. Loss is 1.9223537. Sequentialfabab260's hyper parameters: Current learning rate is 0.009639483323693849. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:20 INFO  DistriOptimizer$:408 - [Epoch 1 48384/60000][Iteration 189][Wall Clock 20.938350756s] Trained 256 records in 0.111003313 seconds. Throughput is 2306.2375 records/second. Loss is 1.9594839. Sequentialfabab260's hyper parameters: Current learning rate is 0.009637625289128758. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:20 INFO  DistriOptimizer$:408 - [Epoch 1 48640/60000][Iteration 190][Wall Clock 21.038515384s] Trained 256 records in 0.100164628 seconds. Throughput is 2555.7925 records/second. Loss is 1.8583488. Sequentialfabab260's hyper parameters: Current learning rate is 0.009635767970707265. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:20 INFO  DistriOptimizer$:408 - [Epoch 1 48896/60000][Iteration 191][Wall Clock 21.1160801s] Trained 256 records in 0.077564716 seconds. Throughput is 3300.4697 records/second. Loss is 1.9238513. Sequentialfabab260's hyper parameters: Current learning rate is 0.009633911368015413. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:20 INFO  DistriOptimizer$:408 - [Epoch 1 49152/60000][Iteration 192][Wall Clock 21.202415429s] Trained 256 records in 0.086335329 seconds. Throughput is 2965.1824 records/second. Loss is 1.9340804. Sequentialfabab260's hyper parameters: Current learning rate is 0.009632055480639569. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:20 INFO  DistriOptimizer$:408 - [Epoch 1 49408/60000][Iteration 193][Wall Clock 21.284275759s] Trained 256 records in 0.08186033 seconds. Throughput is 3127.2778 records/second. Loss is 1.8722697. Sequentialfabab260's hyper parameters: Current learning rate is 0.009630200308166411. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:20 INFO  DistriOptimizer$:408 - [Epoch 1 49664/60000][Iteration 194][Wall Clock 21.38683123s] Trained 256 records in 0.102555471 seconds. Throughput is 2496.2102 records/second. Loss is 1.8758216. Sequentialfabab260's hyper parameters: Current learning rate is 0.00962834585018294. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:20 INFO  DistriOptimizer$:408 - [Epoch 1 49920/60000][Iteration 195][Wall Clock 21.475044368s] Trained 256 records in 0.088213138 seconds. Throughput is 2902.062 records/second. Loss is 1.9130294. Sequentialfabab260's hyper parameters: Current learning rate is 0.009626492106276474. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:21 INFO  DistriOptimizer$:408 - [Epoch 1 50176/60000][Iteration 196][Wall Clock 21.560250649s] Trained 256 records in 0.085206281 seconds. Throughput is 3004.4734 records/second. Loss is 1.9088538. Sequentialfabab260's hyper parameters: Current learning rate is 0.00962463907603465. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:21 INFO  DistriOptimizer$:408 - [Epoch 1 50432/60000][Iteration 197][Wall Clock 21.644642859s] Trained 256 records in 0.08439221 seconds. Throughput is 3033.4553 records/second. Loss is 1.9110259. Sequentialfabab260's hyper parameters: Current learning rate is 0.009622786759045421. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:21 INFO  DistriOptimizer$:408 - [Epoch 1 50688/60000][Iteration 198][Wall Clock 21.729918757s] Trained 256 records in 0.085275898 seconds. Throughput is 3002.0208 records/second. Loss is 1.8849747. Sequentialfabab260's hyper parameters: Current learning rate is 0.009620935154897056. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:21 INFO  DistriOptimizer$:408 - [Epoch 1 50944/60000][Iteration 199][Wall Clock 21.815247794s] Trained 256 records in 0.085329037 seconds. Throughput is 3000.1511 records/second. Loss is 1.8692712. Sequentialfabab260's hyper parameters: Current learning rate is 0.009619084263178144. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:21 INFO  DistriOptimizer$:408 - [Epoch 1 51200/60000][Iteration 200][Wall Clock 21.901346439s] Trained 256 records in 0.086098645 seconds. Throughput is 2973.3335 records/second. Loss is 1.9230539. Sequentialfabab260's hyper parameters: Current learning rate is 0.009617234083477592. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:21 INFO  DistriOptimizer$:408 - [Epoch 1 51456/60000][Iteration 201][Wall Clock 21.988228404s] Trained 256 records in 0.086881965 seconds. Throughput is 2946.5264 records/second. Loss is 1.9147274. Sequentialfabab260's hyper parameters: Current learning rate is 0.009615384615384616. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:21 INFO  DistriOptimizer$:408 - [Epoch 1 51712/60000][Iteration 202][Wall Clock 22.070444371s] Trained 256 records in 0.082215967 seconds. Throughput is 3113.7505 records/second. Loss is 1.8765382. Sequentialfabab260's hyper parameters: Current learning rate is 0.009613535858488752. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:21 INFO  DistriOptimizer$:408 - [Epoch 1 51968/60000][Iteration 203][Wall Clock 22.147530452s] Trained 256 records in 0.077086081 seconds. Throughput is 3320.9626 records/second. Loss is 1.8751571. Sequentialfabab260's hyper parameters: Current learning rate is 0.009611687812379853. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:21 INFO  DistriOptimizer$:408 - [Epoch 1 52224/60000][Iteration 204][Wall Clock 22.243256598s] Trained 256 records in 0.095726146 seconds. Throughput is 2674.2954 records/second. Loss is 1.9193482. Sequentialfabab260's hyper parameters: Current learning rate is 0.009609840476648089. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:21 INFO  DistriOptimizer$:408 - [Epoch 1 52480/60000][Iteration 205][Wall Clock 22.330562794s] Trained 256 records in 0.087306196 seconds. Throughput is 2932.209 records/second. Loss is 1.8915387. Sequentialfabab260's hyper parameters: Current learning rate is 0.009607993850883937. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:21 INFO  DistriOptimizer$:408 - [Epoch 1 52736/60000][Iteration 206][Wall Clock 22.441372522s] Trained 256 records in 0.110809728 seconds. Throughput is 2310.2664 records/second. Loss is 1.8794438. Sequentialfabab260's hyper parameters: Current learning rate is 0.009606147934678195. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:22 INFO  DistriOptimizer$:408 - [Epoch 1 52992/60000][Iteration 207][Wall Clock 22.520998167s] Trained 256 records in 0.079625645 seconds. Throughput is 3215.0447 records/second. Loss is 1.8553413. Sequentialfabab260's hyper parameters: Current learning rate is 0.009604302727621975. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:22 INFO  DistriOptimizer$:408 - [Epoch 1 53248/60000][Iteration 208][Wall Clock 22.601843558s] Trained 256 records in 0.080845391 seconds. Throughput is 3166.5378 records/second. Loss is 1.878062. Sequentialfabab260's hyper parameters: Current learning rate is 0.009602458229306702. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:22 INFO  DistriOptimizer$:408 - [Epoch 1 53504/60000][Iteration 209][Wall Clock 22.697474786s] Trained 256 records in 0.095631228 seconds. Throughput is 2676.95 records/second. Loss is 1.8453436. Sequentialfabab260's hyper parameters: Current learning rate is 0.009600614439324116. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:22 INFO  DistriOptimizer$:408 - [Epoch 1 53760/60000][Iteration 210][Wall Clock 22.786013872s] Trained 256 records in 0.088539086 seconds. Throughput is 2891.3784 records/second. Loss is 1.9158354. Sequentialfabab260's hyper parameters: Current learning rate is 0.00959877135726627. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:22 INFO  DistriOptimizer$:408 - [Epoch 1 54016/60000][Iteration 211][Wall Clock 22.856651475s] Trained 256 records in 0.070637603 seconds. Throughput is 3624.1318 records/second. Loss is 1.8126591. Sequentialfabab260's hyper parameters: Current learning rate is 0.009596928982725527. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:22 INFO  DistriOptimizer$:408 - [Epoch 1 54272/60000][Iteration 212][Wall Clock 22.943102782s] Trained 256 records in 0.086451307 seconds. Throughput is 2961.2046 records/second. Loss is 1.861732. Sequentialfabab260's hyper parameters: Current learning rate is 0.009595087315294569. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:22 INFO  DistriOptimizer$:408 - [Epoch 1 54528/60000][Iteration 213][Wall Clock 23.025280815s] Trained 256 records in 0.082178033 seconds. Throughput is 3115.1877 records/second. Loss is 1.8498429. Sequentialfabab260's hyper parameters: Current learning rate is 0.009593246354566386. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:22 INFO  DistriOptimizer$:408 - [Epoch 1 54784/60000][Iteration 214][Wall Clock 23.118508493s] Trained 256 records in 0.093227678 seconds. Throughput is 2745.9656 records/second. Loss is 1.8568722. Sequentialfabab260's hyper parameters: Current learning rate is 0.00959140610013428. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:22 INFO  DistriOptimizer$:408 - [Epoch 1 55040/60000][Iteration 215][Wall Clock 23.202913274s] Trained 256 records in 0.084404781 seconds. Throughput is 3033.0034 records/second. Loss is 1.8090868. Sequentialfabab260's hyper parameters: Current learning rate is 0.009589566551591868. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:22 INFO  DistriOptimizer$:408 - [Epoch 1 55296/60000][Iteration 216][Wall Clock 23.28855313s] Trained 256 records in 0.085639856 seconds. Throughput is 2989.2625 records/second. Loss is 1.8065006. Sequentialfabab260's hyper parameters: Current learning rate is 0.009587727708533078. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:22 INFO  DistriOptimizer$:408 - [Epoch 1 55552/60000][Iteration 217][Wall Clock 23.370777367s] Trained 256 records in 0.082224237 seconds. Throughput is 3113.4373 records/second. Loss is 1.8426158. Sequentialfabab260's hyper parameters: Current learning rate is 0.00958588957055215. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:22 INFO  DistriOptimizer$:408 - [Epoch 1 55808/60000][Iteration 218][Wall Clock 23.45151412s] Trained 256 records in 0.080736753 seconds. Throughput is 3170.7986 records/second. Loss is 1.8758416. Sequentialfabab260's hyper parameters: Current learning rate is 0.009584052137243625. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:23 INFO  DistriOptimizer$:408 - [Epoch 1 56064/60000][Iteration 219][Wall Clock 23.546343321s] Trained 256 records in 0.094829201 seconds. Throughput is 2699.5903 records/second. Loss is 1.8310223. Sequentialfabab260's hyper parameters: Current learning rate is 0.009582215408202376. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:23 INFO  DistriOptimizer$:408 - [Epoch 1 56320/60000][Iteration 220][Wall Clock 23.629024749s] Trained 256 records in 0.082681428 seconds. Throughput is 3096.2214 records/second. Loss is 1.8358274. Sequentialfabab260's hyper parameters: Current learning rate is 0.009580379383023568. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:23 INFO  DistriOptimizer$:408 - [Epoch 1 56576/60000][Iteration 221][Wall Clock 23.716613522s] Trained 256 records in 0.087588773 seconds. Throughput is 2922.749 records/second. Loss is 1.8451266. Sequentialfabab260's hyper parameters: Current learning rate is 0.009578544061302681. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:23 INFO  DistriOptimizer$:408 - [Epoch 1 56832/60000][Iteration 222][Wall Clock 23.808316305s] Trained 256 records in 0.091702783 seconds. Throughput is 2791.6274 records/second. Loss is 1.8200121. Sequentialfabab260's hyper parameters: Current learning rate is 0.00957670944263551. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:23 INFO  DistriOptimizer$:408 - [Epoch 1 57088/60000][Iteration 223][Wall Clock 23.893933065s] Trained 256 records in 0.08561676 seconds. Throughput is 2990.0688 records/second. Loss is 1.8854162. Sequentialfabab260's hyper parameters: Current learning rate is 0.009574875526618154. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:23 INFO  DistriOptimizer$:408 - [Epoch 1 57344/60000][Iteration 224][Wall Clock 24.001750381s] Trained 256 records in 0.107817316 seconds. Throughput is 2374.3867 records/second. Loss is 1.8187839. Sequentialfabab260's hyper parameters: Current learning rate is 0.009573042312847023. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:23 INFO  DistriOptimizer$:408 - [Epoch 1 57600/60000][Iteration 225][Wall Clock 24.088073231s] Trained 256 records in 0.08632285 seconds. Throughput is 2965.611 records/second. Loss is 1.8286986. Sequentialfabab260's hyper parameters: Current learning rate is 0.009571209800918837. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:23 INFO  DistriOptimizer$:408 - [Epoch 1 57856/60000][Iteration 226][Wall Clock 24.178055318s] Trained 256 records in 0.089982087 seconds. Throughput is 2845.0107 records/second. Loss is 1.7590343. Sequentialfabab260's hyper parameters: Current learning rate is 0.009569377990430623. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:23 INFO  DistriOptimizer$:408 - [Epoch 1 58112/60000][Iteration 227][Wall Clock 24.250198745s] Trained 256 records in 0.072143427 seconds. Throughput is 3548.4868 records/second. Loss is 1.858691. Sequentialfabab260's hyper parameters: Current learning rate is 0.009567546880979718. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:23 INFO  DistriOptimizer$:408 - [Epoch 1 58368/60000][Iteration 228][Wall Clock 24.334056556s] Trained 256 records in 0.083857811 seconds. Throughput is 3052.7866 records/second. Loss is 1.8414598. Sequentialfabab260's hyper parameters: Current learning rate is 0.009565716472163765. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:23 INFO  DistriOptimizer$:408 - [Epoch 1 58624/60000][Iteration 229][Wall Clock 24.452300457s] Trained 256 records in 0.118243901 seconds. Throughput is 2165.0166 records/second. Loss is 1.7588853. Sequentialfabab260's hyper parameters: Current learning rate is 0.009563886763580718. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:24 INFO  DistriOptimizer$:408 - [Epoch 1 58880/60000][Iteration 230][Wall Clock 24.53966967s] Trained 256 records in 0.087369213 seconds. Throughput is 2930.094 records/second. Loss is 1.7965853. Sequentialfabab260's hyper parameters: Current learning rate is 0.009562057754828839. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:24 INFO  DistriOptimizer$:408 - [Epoch 1 59136/60000][Iteration 231][Wall Clock 24.62596194s] Trained 256 records in 0.08629227 seconds. Throughput is 2966.662 records/second. Loss is 1.8211774. Sequentialfabab260's hyper parameters: Current learning rate is 0.009560229445506692. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:24 INFO  DistriOptimizer$:408 - [Epoch 1 59392/60000][Iteration 232][Wall Clock 24.707597463s] Trained 256 records in 0.081635523 seconds. Throughput is 3135.89 records/second. Loss is 1.755847. Sequentialfabab260's hyper parameters: Current learning rate is 0.009558401835213153. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:24 INFO  DistriOptimizer$:408 - [Epoch 1 59648/60000][Iteration 233][Wall Clock 24.791942001s] Trained 256 records in 0.084344538 seconds. Throughput is 3035.17 records/second. Loss is 1.7670459. Sequentialfabab260's hyper parameters: Current learning rate is 0.0095565749235474. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:24 INFO  DistriOptimizer$:408 - [Epoch 1 59904/60000][Iteration 234][Wall Clock 24.891671848s] Trained 256 records in 0.099729847 seconds. Throughput is 2566.9348 records/second. Loss is 1.818531. Sequentialfabab260's hyper parameters: Current learning rate is 0.009554748710108925. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:24 INFO  DistriOptimizer$:408 - [Epoch 1 60160/60000][Iteration 235][Wall Clock 24.968038268s] Trained 256 records in 0.07636642 seconds. Throughput is 3352.2588 records/second. Loss is 1.7990505. Sequentialfabab260's hyper parameters: Current learning rate is 0.009552923194497517. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:24 INFO  DistriOptimizer$:452 - [Epoch 1 60160/60000][Iteration 235][Wall Clock 24.968038268s] Epoch finished. Wall clock time is 25082.811966 ms
2019-10-17 12:21:24 INFO  DistriOptimizer$:111 - [Epoch 1 60160/60000][Iteration 235][Wall Clock 24.968038268s] Validate model...
2019-10-17 12:21:24 INFO  DistriOptimizer$:178 - [Epoch 1 60160/60000][Iteration 235][Wall Clock 24.968038268s] validate model throughput is 30569.387 records/second
2019-10-17 12:21:24 INFO  DistriOptimizer$:181 - [Epoch 1 60160/60000][Iteration 235][Wall Clock 24.968038268s] Top1Accuracy is Accuracy(correct: 5560, count: 10000, accuracy: 0.556)
2019-10-17 12:21:24 INFO  DistriOptimizer$:221 - [Wall Clock 25.082811966s] Save model to /tmp/lenet5/20191017_122059
2019-10-17 12:21:24 INFO  DistriOptimizer$:226 - [Wall Clock 25.082811966s] Save optimMethod com.intel.analytics.bigdl.optim.SGD@9429a82 to /tmp/lenet5/20191017_122059
2019-10-17 12:21:25 INFO  DistriOptimizer$:408 - [Epoch 2 256/60000][Iteration 236][Wall Clock 25.195352077s] Trained 256 records in 0.112540111 seconds. Throughput is 2274.7446 records/second. Loss is 1.7579017. Sequentialfabab260's hyper parameters: Current learning rate is 0.009551098376313277. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:25 INFO  DistriOptimizer$:408 - [Epoch 2 512/60000][Iteration 237][Wall Clock 25.310139141s] Trained 256 records in 0.114787064 seconds. Throughput is 2230.2166 records/second. Loss is 1.8242733. Sequentialfabab260's hyper parameters: Current learning rate is 0.00954927425515661. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:25 INFO  DistriOptimizer$:408 - [Epoch 2 768/60000][Iteration 238][Wall Clock 25.402200571s] Trained 256 records in 0.09206143 seconds. Throughput is 2780.752 records/second. Loss is 1.7315074. Sequentialfabab260's hyper parameters: Current learning rate is 0.009547450830628221. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:25 INFO  DistriOptimizer$:408 - [Epoch 2 1024/60000][Iteration 239][Wall Clock 25.494449586s] Trained 256 records in 0.092249015 seconds. Throughput is 2775.0974 records/second. Loss is 1.7930174. Sequentialfabab260's hyper parameters: Current learning rate is 0.009545628102329133. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:25 INFO  DistriOptimizer$:408 - [Epoch 2 1280/60000][Iteration 240][Wall Clock 25.590553838s] Trained 256 records in 0.096104252 seconds. Throughput is 2663.774 records/second. Loss is 1.7789682. Sequentialfabab260's hyper parameters: Current learning rate is 0.00954380606986066. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:25 INFO  DistriOptimizer$:408 - [Epoch 2 1536/60000][Iteration 241][Wall Clock 25.678811621s] Trained 256 records in 0.088257783 seconds. Throughput is 2900.594 records/second. Loss is 1.7536428. Sequentialfabab260's hyper parameters: Current learning rate is 0.009541984732824428. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:25 INFO  DistriOptimizer$:408 - [Epoch 2 1792/60000][Iteration 242][Wall Clock 25.779978798s] Trained 256 records in 0.101167177 seconds. Throughput is 2530.4648 records/second. Loss is 1.7709534. Sequentialfabab260's hyper parameters: Current learning rate is 0.009540164090822362. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:25 INFO  DistriOptimizer$:408 - [Epoch 2 2048/60000][Iteration 243][Wall Clock 25.876287482s] Trained 256 records in 0.096308684 seconds. Throughput is 2658.1196 records/second. Loss is 1.7359293. Sequentialfabab260's hyper parameters: Current learning rate is 0.009538344143456697. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:25 INFO  DistriOptimizer$:408 - [Epoch 2 2304/60000][Iteration 244][Wall Clock 25.976468845s] Trained 256 records in 0.100181363 seconds. Throughput is 2555.3655 records/second. Loss is 1.7598132. Sequentialfabab260's hyper parameters: Current learning rate is 0.009536524890329964. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:25 INFO  DistriOptimizer$:408 - [Epoch 2 2560/60000][Iteration 245][Wall Clock 26.069428591s] Trained 256 records in 0.092959746 seconds. Throughput is 2753.8801 records/second. Loss is 1.7403804. Sequentialfabab260's hyper parameters: Current learning rate is 0.009534706331045004. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:25 INFO  DistriOptimizer$:408 - [Epoch 2 2816/60000][Iteration 246][Wall Clock 26.154684957s] Trained 256 records in 0.085256366 seconds. Throughput is 3002.7083 records/second. Loss is 1.7500563. Sequentialfabab260's hyper parameters: Current learning rate is 0.009532888465204958. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:26 INFO  DistriOptimizer$:408 - [Epoch 2 3072/60000][Iteration 247][Wall Clock 26.277123542s] Trained 256 records in 0.122438585 seconds. Throughput is 2090.8442 records/second. Loss is 1.754462. Sequentialfabab260's hyper parameters: Current learning rate is 0.009531071292413268. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:26 INFO  DistriOptimizer$:408 - [Epoch 2 3328/60000][Iteration 248][Wall Clock 26.373546454s] Trained 256 records in 0.096422912 seconds. Throughput is 2654.9707 records/second. Loss is 1.7669587. Sequentialfabab260's hyper parameters: Current learning rate is 0.009529254812273681. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:26 INFO  DistriOptimizer$:408 - [Epoch 2 3584/60000][Iteration 249][Wall Clock 26.463303215s] Trained 256 records in 0.089756761 seconds. Throughput is 2852.1528 records/second. Loss is 1.7112622. Sequentialfabab260's hyper parameters: Current learning rate is 0.009527439024390244. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:26 INFO  DistriOptimizer$:408 - [Epoch 2 3840/60000][Iteration 250][Wall Clock 26.584509058s] Trained 256 records in 0.121205843 seconds. Throughput is 2112.1094 records/second. Loss is 1.7373482. Sequentialfabab260's hyper parameters: Current learning rate is 0.009525623928367307. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:26 INFO  DistriOptimizer$:408 - [Epoch 2 4096/60000][Iteration 251][Wall Clock 26.684837827s] Trained 256 records in 0.100328769 seconds. Throughput is 2551.611 records/second. Loss is 1.7345067. Sequentialfabab260's hyper parameters: Current learning rate is 0.009523809523809523. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:26 INFO  DistriOptimizer$:408 - [Epoch 2 4352/60000][Iteration 252][Wall Clock 26.798149027s] Trained 256 records in 0.1133112 seconds. Throughput is 2259.2646 records/second. Loss is 1.7214917. Sequentialfabab260's hyper parameters: Current learning rate is 0.009521995810321843. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:26 INFO  DistriOptimizer$:408 - [Epoch 2 4608/60000][Iteration 253][Wall Clock 26.89451071s] Trained 256 records in 0.096361683 seconds. Throughput is 2656.6577 records/second. Loss is 1.701221. Sequentialfabab260's hyper parameters: Current learning rate is 0.00952018278750952. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:26 INFO  DistriOptimizer$:408 - [Epoch 2 4864/60000][Iteration 254][Wall Clock 26.995370726s] Trained 256 records in 0.100860016 seconds. Throughput is 2538.1714 records/second. Loss is 1.7676623. Sequentialfabab260's hyper parameters: Current learning rate is 0.009518370454978109. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:26 INFO  DistriOptimizer$:408 - [Epoch 2 5120/60000][Iteration 255][Wall Clock 27.078779627s] Trained 256 records in 0.083408901 seconds. Throughput is 3069.2168 records/second. Loss is 1.6996427. Sequentialfabab260's hyper parameters: Current learning rate is 0.00951655881233346. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:27 INFO  DistriOptimizer$:408 - [Epoch 2 5376/60000][Iteration 256][Wall Clock 27.167598279s] Trained 256 records in 0.088818652 seconds. Throughput is 2882.2773 records/second. Loss is 1.7581182. Sequentialfabab260's hyper parameters: Current learning rate is 0.009514747859181733. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:27 INFO  DistriOptimizer$:408 - [Epoch 2 5632/60000][Iteration 257][Wall Clock 27.253877671s] Trained 256 records in 0.086279392 seconds. Throughput is 2967.1047 records/second. Loss is 1.6608145. Sequentialfabab260's hyper parameters: Current learning rate is 0.009512937595129377. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:27 INFO  DistriOptimizer$:408 - [Epoch 2 5888/60000][Iteration 258][Wall Clock 27.346459127s] Trained 256 records in 0.092581456 seconds. Throughput is 2765.1326 records/second. Loss is 1.7152578. Sequentialfabab260's hyper parameters: Current learning rate is 0.009511128019783146. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:27 INFO  DistriOptimizer$:408 - [Epoch 2 6144/60000][Iteration 259][Wall Clock 27.434374817s] Trained 256 records in 0.08791569 seconds. Throughput is 2911.8806 records/second. Loss is 1.6932294. Sequentialfabab260's hyper parameters: Current learning rate is 0.009509319132750094. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:27 INFO  DistriOptimizer$:408 - [Epoch 2 6400/60000][Iteration 260][Wall Clock 27.510644004s] Trained 256 records in 0.076269187 seconds. Throughput is 3356.5325 records/second. Loss is 1.6674693. Sequentialfabab260's hyper parameters: Current learning rate is 0.009507510933637574. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:27 INFO  DistriOptimizer$:408 - [Epoch 2 6656/60000][Iteration 261][Wall Clock 27.59968978s] Trained 256 records in 0.089045776 seconds. Throughput is 2874.9258 records/second. Loss is 1.7196825. Sequentialfabab260's hyper parameters: Current learning rate is 0.009505703422053232. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:27 INFO  DistriOptimizer$:408 - [Epoch 2 6912/60000][Iteration 262][Wall Clock 27.67731231s] Trained 256 records in 0.07762253 seconds. Throughput is 3298.0115 records/second. Loss is 1.7469908. Sequentialfabab260's hyper parameters: Current learning rate is 0.009503896597605019. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:27 INFO  DistriOptimizer$:408 - [Epoch 2 7168/60000][Iteration 263][Wall Clock 27.774499682s] Trained 256 records in 0.097187372 seconds. Throughput is 2634.0872 records/second. Loss is 1.6398371. Sequentialfabab260's hyper parameters: Current learning rate is 0.00950209045990118. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:27 INFO  DistriOptimizer$:408 - [Epoch 2 7424/60000][Iteration 264][Wall Clock 27.858447892s] Trained 256 records in 0.08394821 seconds. Throughput is 3049.4993 records/second. Loss is 1.7139051. Sequentialfabab260's hyper parameters: Current learning rate is 0.009500285008550257. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:27 INFO  DistriOptimizer$:408 - [Epoch 2 7680/60000][Iteration 265][Wall Clock 27.938019307s] Trained 256 records in 0.079571415 seconds. Throughput is 3217.2356 records/second. Loss is 1.7052909. Sequentialfabab260's hyper parameters: Current learning rate is 0.009498480243161096. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:27 INFO  DistriOptimizer$:408 - [Epoch 2 7936/60000][Iteration 266][Wall Clock 28.019374788s] Trained 256 records in 0.081355481 seconds. Throughput is 3146.684 records/second. Loss is 1.6621476. Sequentialfabab260's hyper parameters: Current learning rate is 0.009496676163342831. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:27 INFO  DistriOptimizer$:408 - [Epoch 2 8192/60000][Iteration 267][Wall Clock 28.107705965s] Trained 256 records in 0.088331177 seconds. Throughput is 2898.1838 records/second. Loss is 1.692561. Sequentialfabab260's hyper parameters: Current learning rate is 0.0094948727687049. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:28 INFO  DistriOptimizer$:408 - [Epoch 2 8448/60000][Iteration 268][Wall Clock 28.202036019s] Trained 256 records in 0.094330054 seconds. Throughput is 2713.8752 records/second. Loss is 1.6774492. Sequentialfabab260's hyper parameters: Current learning rate is 0.009493070058857035. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:28 INFO  DistriOptimizer$:408 - [Epoch 2 8704/60000][Iteration 269][Wall Clock 28.282458362s] Trained 256 records in 0.080422343 seconds. Throughput is 3183.195 records/second. Loss is 1.7286803. Sequentialfabab260's hyper parameters: Current learning rate is 0.009491268033409262. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:28 INFO  DistriOptimizer$:408 - [Epoch 2 8960/60000][Iteration 270][Wall Clock 28.366898973s] Trained 256 records in 0.084440611 seconds. Throughput is 3031.7166 records/second. Loss is 1.7141869. Sequentialfabab260's hyper parameters: Current learning rate is 0.00948946669197191. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:28 INFO  DistriOptimizer$:408 - [Epoch 2 9216/60000][Iteration 271][Wall Clock 28.457502806s] Trained 256 records in 0.090603833 seconds. Throughput is 2825.4875 records/second. Loss is 1.6895854. Sequentialfabab260's hyper parameters: Current learning rate is 0.009487666034155597. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:28 INFO  DistriOptimizer$:408 - [Epoch 2 9472/60000][Iteration 272][Wall Clock 28.579157346s] Trained 256 records in 0.12165454 seconds. Throughput is 2104.3193 records/second. Loss is 1.6939212. Sequentialfabab260's hyper parameters: Current learning rate is 0.00948586605957124. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:28 INFO  DistriOptimizer$:408 - [Epoch 2 9728/60000][Iteration 273][Wall Clock 28.67056959s] Trained 256 records in 0.091412244 seconds. Throughput is 2800.5 records/second. Loss is 1.6296725. Sequentialfabab260's hyper parameters: Current learning rate is 0.009484066767830045. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:28 INFO  DistriOptimizer$:408 - [Epoch 2 9984/60000][Iteration 274][Wall Clock 28.762948802s] Trained 256 records in 0.092379212 seconds. Throughput is 2771.1863 records/second. Loss is 1.633218. Sequentialfabab260's hyper parameters: Current learning rate is 0.009482268158543524. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:28 INFO  DistriOptimizer$:408 - [Epoch 2 10240/60000][Iteration 275][Wall Clock 28.868642049s] Trained 256 records in 0.105693247 seconds. Throughput is 2422.1038 records/second. Loss is 1.6124061. Sequentialfabab260's hyper parameters: Current learning rate is 0.009480470231323474. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:28 INFO  DistriOptimizer$:408 - [Epoch 2 10496/60000][Iteration 276][Wall Clock 28.960626381s] Trained 256 records in 0.091984332 seconds. Throughput is 2783.0828 records/second. Loss is 1.6725422. Sequentialfabab260's hyper parameters: Current learning rate is 0.009478672985781991. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:28 INFO  DistriOptimizer$:408 - [Epoch 2 10752/60000][Iteration 277][Wall Clock 29.042807723s] Trained 256 records in 0.082181342 seconds. Throughput is 3115.0623 records/second. Loss is 1.648057. Sequentialfabab260's hyper parameters: Current learning rate is 0.009476876421531465. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:28 INFO  DistriOptimizer$:408 - [Epoch 2 11008/60000][Iteration 278][Wall Clock 29.130008565s] Trained 256 records in 0.087200842 seconds. Throughput is 2935.7515 records/second. Loss is 1.5850649. Sequentialfabab260's hyper parameters: Current learning rate is 0.009475080538184574. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:29 INFO  DistriOptimizer$:408 - [Epoch 2 11264/60000][Iteration 279][Wall Clock 29.20726467s] Trained 256 records in 0.077256105 seconds. Throughput is 3313.654 records/second. Loss is 1.6024308. Sequentialfabab260's hyper parameters: Current learning rate is 0.0094732853353543. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:29 INFO  DistriOptimizer$:408 - [Epoch 2 11520/60000][Iteration 280][Wall Clock 29.289580305s] Trained 256 records in 0.082315635 seconds. Throughput is 3109.9802 records/second. Loss is 1.6489738. Sequentialfabab260's hyper parameters: Current learning rate is 0.009471490812653912. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:29 INFO  DistriOptimizer$:408 - [Epoch 2 11776/60000][Iteration 281][Wall Clock 29.366826835s] Trained 256 records in 0.07724653 seconds. Throughput is 3314.0647 records/second. Loss is 1.6759589. Sequentialfabab260's hyper parameters: Current learning rate is 0.00946969696969697. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:29 INFO  DistriOptimizer$:408 - [Epoch 2 12032/60000][Iteration 282][Wall Clock 29.448893339s] Trained 256 records in 0.082066504 seconds. Throughput is 3119.4211 records/second. Loss is 1.6498513. Sequentialfabab260's hyper parameters: Current learning rate is 0.00946790380609733. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:29 INFO  DistriOptimizer$:408 - [Epoch 2 12288/60000][Iteration 283][Wall Clock 29.550709014s] Trained 256 records in 0.101815675 seconds. Throughput is 2514.3477 records/second. Loss is 1.6451871. Sequentialfabab260's hyper parameters: Current learning rate is 0.00946611132146914. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:29 INFO  DistriOptimizer$:408 - [Epoch 2 12544/60000][Iteration 284][Wall Clock 29.631070898s] Trained 256 records in 0.080361884 seconds. Throughput is 3185.59 records/second. Loss is 1.6287842. Sequentialfabab260's hyper parameters: Current learning rate is 0.00946431951542684. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:29 INFO  DistriOptimizer$:408 - [Epoch 2 12800/60000][Iteration 285][Wall Clock 29.71260114s] Trained 256 records in 0.081530242 seconds. Throughput is 3139.9392 records/second. Loss is 1.5812674. Sequentialfabab260's hyper parameters: Current learning rate is 0.009462528387585163. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:29 INFO  DistriOptimizer$:408 - [Epoch 2 13056/60000][Iteration 286][Wall Clock 29.789327721s] Trained 256 records in 0.076726581 seconds. Throughput is 3336.5232 records/second. Loss is 1.5515845. Sequentialfabab260's hyper parameters: Current learning rate is 0.00946073793755913. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:29 INFO  DistriOptimizer$:408 - [Epoch 2 13312/60000][Iteration 287][Wall Clock 29.865578094s] Trained 256 records in 0.076250373 seconds. Throughput is 3357.3606 records/second. Loss is 1.5852275. Sequentialfabab260's hyper parameters: Current learning rate is 0.009458948164964056. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:29 INFO  DistriOptimizer$:408 - [Epoch 2 13568/60000][Iteration 288][Wall Clock 29.958358477s] Trained 256 records in 0.092780383 seconds. Throughput is 2759.204 records/second. Loss is 1.6645864. Sequentialfabab260's hyper parameters: Current learning rate is 0.00945715906941555. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:29 INFO  DistriOptimizer$:408 - [Epoch 2 13824/60000][Iteration 289][Wall Clock 30.033606207s] Trained 256 records in 0.07524773 seconds. Throughput is 3402.0962 records/second. Loss is 1.5863411. Sequentialfabab260's hyper parameters: Current learning rate is 0.0094553706505295. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:29 INFO  DistriOptimizer$:408 - [Epoch 2 14080/60000][Iteration 290][Wall Clock 30.12301883s] Trained 256 records in 0.089412623 seconds. Throughput is 2863.1304 records/second. Loss is 1.6350664. Sequentialfabab260's hyper parameters: Current learning rate is 0.009453582907922102. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:30 INFO  DistriOptimizer$:408 - [Epoch 2 14336/60000][Iteration 291][Wall Clock 30.203487358s] Trained 256 records in 0.080468528 seconds. Throughput is 3181.368 records/second. Loss is 1.5908043. Sequentialfabab260's hyper parameters: Current learning rate is 0.00945179584120983. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:30 INFO  DistriOptimizer$:408 - [Epoch 2 14592/60000][Iteration 292][Wall Clock 30.287011019s] Trained 256 records in 0.083523661 seconds. Throughput is 3064.9998 records/second. Loss is 1.5894325. Sequentialfabab260's hyper parameters: Current learning rate is 0.00945000945000945. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:30 INFO  DistriOptimizer$:408 - [Epoch 2 14848/60000][Iteration 293][Wall Clock 30.378100622s] Trained 256 records in 0.091089603 seconds. Throughput is 2810.4194 records/second. Loss is 1.5680832. Sequentialfabab260's hyper parameters: Current learning rate is 0.00944822373393802. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:30 INFO  DistriOptimizer$:408 - [Epoch 2 15104/60000][Iteration 294][Wall Clock 30.469727125s] Trained 256 records in 0.091626503 seconds. Throughput is 2793.9514 records/second. Loss is 1.5759301. Sequentialfabab260's hyper parameters: Current learning rate is 0.009446438692612885. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:30 INFO  DistriOptimizer$:408 - [Epoch 2 15360/60000][Iteration 295][Wall Clock 30.549108628s] Trained 256 records in 0.079381503 seconds. Throughput is 3224.9326 records/second. Loss is 1.5981317. Sequentialfabab260's hyper parameters: Current learning rate is 0.009444654325651681. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:30 INFO  DistriOptimizer$:408 - [Epoch 2 15616/60000][Iteration 296][Wall Clock 30.643298662s] Trained 256 records in 0.094190034 seconds. Throughput is 2717.9097 records/second. Loss is 1.5749148. Sequentialfabab260's hyper parameters: Current learning rate is 0.009442870632672334. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:30 INFO  DistriOptimizer$:408 - [Epoch 2 15872/60000][Iteration 297][Wall Clock 30.731455436s] Trained 256 records in 0.088156774 seconds. Throughput is 2903.9175 records/second. Loss is 1.5493939. Sequentialfabab260's hyper parameters: Current learning rate is 0.009441087613293053. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:30 INFO  DistriOptimizer$:408 - [Epoch 2 16128/60000][Iteration 298][Wall Clock 30.817911972s] Trained 256 records in 0.086456536 seconds. Throughput is 2961.0254 records/second. Loss is 1.5615073. Sequentialfabab260's hyper parameters: Current learning rate is 0.00943930526713234. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:30 INFO  DistriOptimizer$:408 - [Epoch 2 16384/60000][Iteration 299][Wall Clock 30.915487701s] Trained 256 records in 0.097575729 seconds. Throughput is 2623.6033 records/second. Loss is 1.5084952. Sequentialfabab260's hyper parameters: Current learning rate is 0.009437523593808984. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:30 INFO  DistriOptimizer$:408 - [Epoch 2 16640/60000][Iteration 300][Wall Clock 31.001580343s] Trained 256 records in 0.086092642 seconds. Throughput is 2973.541 records/second. Loss is 1.5807488. Sequentialfabab260's hyper parameters: Current learning rate is 0.009435742592942064. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:30 INFO  DistriOptimizer$:408 - [Epoch 2 16896/60000][Iteration 301][Wall Clock 31.084669264s] Trained 256 records in 0.083088921 seconds. Throughput is 3081.0366 records/second. Loss is 1.5105898. Sequentialfabab260's hyper parameters: Current learning rate is 0.009433962264150943. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:31 INFO  DistriOptimizer$:408 - [Epoch 2 17152/60000][Iteration 302][Wall Clock 31.17171989s] Trained 256 records in 0.087050626 seconds. Throughput is 2940.8176 records/second. Loss is 1.558923. Sequentialfabab260's hyper parameters: Current learning rate is 0.009432182607055273. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:31 INFO  DistriOptimizer$:408 - [Epoch 2 17408/60000][Iteration 303][Wall Clock 31.245767314s] Trained 256 records in 0.074047424 seconds. Throughput is 3457.244 records/second. Loss is 1.572924. Sequentialfabab260's hyper parameters: Current learning rate is 0.00943040362127499. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:31 INFO  DistriOptimizer$:408 - [Epoch 2 17664/60000][Iteration 304][Wall Clock 31.3184842s] Trained 256 records in 0.072716886 seconds. Throughput is 3520.503 records/second. Loss is 1.5564822. Sequentialfabab260's hyper parameters: Current learning rate is 0.009428625306430323. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:31 INFO  DistriOptimizer$:408 - [Epoch 2 17920/60000][Iteration 305][Wall Clock 31.400583751s] Trained 256 records in 0.082099551 seconds. Throughput is 3118.1658 records/second. Loss is 1.5142502. Sequentialfabab260's hyper parameters: Current learning rate is 0.009426847662141781. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:31 INFO  DistriOptimizer$:408 - [Epoch 2 18176/60000][Iteration 306][Wall Clock 31.473216196s] Trained 256 records in 0.072632445 seconds. Throughput is 3524.5955 records/second. Loss is 1.4821918. Sequentialfabab260's hyper parameters: Current learning rate is 0.009425070688030161. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:31 INFO  DistriOptimizer$:408 - [Epoch 2 18432/60000][Iteration 307][Wall Clock 31.558935193s] Trained 256 records in 0.085718997 seconds. Throughput is 2986.5024 records/second. Loss is 1.5201098. Sequentialfabab260's hyper parameters: Current learning rate is 0.009423294383716549. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:31 INFO  DistriOptimizer$:408 - [Epoch 2 18688/60000][Iteration 308][Wall Clock 31.644557445s] Trained 256 records in 0.085622252 seconds. Throughput is 2989.877 records/second. Loss is 1.4968994. Sequentialfabab260's hyper parameters: Current learning rate is 0.009421518748822312. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:31 INFO  DistriOptimizer$:408 - [Epoch 2 18944/60000][Iteration 309][Wall Clock 31.730836803s] Trained 256 records in 0.086279358 seconds. Throughput is 2967.1062 records/second. Loss is 1.4991965. Sequentialfabab260's hyper parameters: Current learning rate is 0.009419743782969102. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:31 INFO  DistriOptimizer$:408 - [Epoch 2 19200/60000][Iteration 310][Wall Clock 31.804876978s] Trained 256 records in 0.074040175 seconds. Throughput is 3457.5823 records/second. Loss is 1.5595917. Sequentialfabab260's hyper parameters: Current learning rate is 0.009417969485778865. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:31 INFO  DistriOptimizer$:408 - [Epoch 2 19456/60000][Iteration 311][Wall Clock 31.879552151s] Trained 256 records in 0.074675173 seconds. Throughput is 3428.181 records/second. Loss is 1.5129297. Sequentialfabab260's hyper parameters: Current learning rate is 0.009416195856873822. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:31 INFO  DistriOptimizer$:408 - [Epoch 2 19712/60000][Iteration 312][Wall Clock 31.971487089s] Trained 256 records in 0.091934938 seconds. Throughput is 2784.578 records/second. Loss is 1.5228981. Sequentialfabab260's hyper parameters: Current learning rate is 0.009414422895876483. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:31 INFO  DistriOptimizer$:408 - [Epoch 2 19968/60000][Iteration 313][Wall Clock 32.066008201s] Trained 256 records in 0.094521112 seconds. Throughput is 2708.3896 records/second. Loss is 1.4985996. Sequentialfabab260's hyper parameters: Current learning rate is 0.00941265060240964. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:32 INFO  DistriOptimizer$:408 - [Epoch 2 20224/60000][Iteration 314][Wall Clock 32.152677242s] Trained 256 records in 0.086669041 seconds. Throughput is 2953.7651 records/second. Loss is 1.513277. Sequentialfabab260's hyper parameters: Current learning rate is 0.009410878976096368. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:32 INFO  DistriOptimizer$:408 - [Epoch 2 20480/60000][Iteration 315][Wall Clock 32.247597872s] Trained 256 records in 0.09492063 seconds. Throughput is 2696.9902 records/second. Loss is 1.5231957. Sequentialfabab260's hyper parameters: Current learning rate is 0.00940910801656003. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:32 INFO  DistriOptimizer$:408 - [Epoch 2 20736/60000][Iteration 316][Wall Clock 32.328557653s] Trained 256 records in 0.080959781 seconds. Throughput is 3162.064 records/second. Loss is 1.4601347. Sequentialfabab260's hyper parameters: Current learning rate is 0.009407337723424272. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:32 INFO  DistriOptimizer$:408 - [Epoch 2 20992/60000][Iteration 317][Wall Clock 32.418600819s] Trained 256 records in 0.090043166 seconds. Throughput is 2843.0808 records/second. Loss is 1.5250342. Sequentialfabab260's hyper parameters: Current learning rate is 0.009405568096313018. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:32 INFO  DistriOptimizer$:408 - [Epoch 2 21248/60000][Iteration 318][Wall Clock 32.502438309s] Trained 256 records in 0.08383749 seconds. Throughput is 3053.5266 records/second. Loss is 1.5381685. Sequentialfabab260's hyper parameters: Current learning rate is 0.00940379913485048. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:32 INFO  DistriOptimizer$:408 - [Epoch 2 21504/60000][Iteration 319][Wall Clock 32.588986572s] Trained 256 records in 0.086548263 seconds. Throughput is 2957.8872 records/second. Loss is 1.4898839. Sequentialfabab260's hyper parameters: Current learning rate is 0.00940203083866115. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:32 INFO  DistriOptimizer$:408 - [Epoch 2 21760/60000][Iteration 320][Wall Clock 32.667939916s] Trained 256 records in 0.078953344 seconds. Throughput is 3242.4214 records/second. Loss is 1.483587. Sequentialfabab260's hyper parameters: Current learning rate is 0.009400263207369806. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:32 INFO  DistriOptimizer$:408 - [Epoch 2 22016/60000][Iteration 321][Wall Clock 32.773648507s] Trained 256 records in 0.105708591 seconds. Throughput is 2421.752 records/second. Loss is 1.5201813. Sequentialfabab260's hyper parameters: Current learning rate is 0.009398496240601503. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:32 INFO  DistriOptimizer$:408 - [Epoch 2 22272/60000][Iteration 322][Wall Clock 32.851417212s] Trained 256 records in 0.077768705 seconds. Throughput is 3291.8125 records/second. Loss is 1.5033839. Sequentialfabab260's hyper parameters: Current learning rate is 0.009396729937981583. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:32 INFO  DistriOptimizer$:408 - [Epoch 2 22528/60000][Iteration 323][Wall Clock 32.935987676s] Trained 256 records in 0.084570464 seconds. Throughput is 3027.0615 records/second. Loss is 1.4977586. Sequentialfabab260's hyper parameters: Current learning rate is 0.009394964299135663. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:32 INFO  DistriOptimizer$:408 - [Epoch 2 22784/60000][Iteration 324][Wall Clock 33.046152636s] Trained 256 records in 0.11016496 seconds. Throughput is 2323.7878 records/second. Loss is 1.5359827. Sequentialfabab260's hyper parameters: Current learning rate is 0.00939319932368965. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:32 INFO  DistriOptimizer$:408 - [Epoch 2 23040/60000][Iteration 325][Wall Clock 33.131109965s] Trained 256 records in 0.084957329 seconds. Throughput is 3013.2773 records/second. Loss is 1.5307245. Sequentialfabab260's hyper parameters: Current learning rate is 0.009391435011269723. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:33 INFO  DistriOptimizer$:408 - [Epoch 2 23296/60000][Iteration 326][Wall Clock 33.213642053s] Trained 256 records in 0.082532088 seconds. Throughput is 3101.824 records/second. Loss is 1.4462672. Sequentialfabab260's hyper parameters: Current learning rate is 0.009389671361502348. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:33 INFO  DistriOptimizer$:408 - [Epoch 2 23552/60000][Iteration 327][Wall Clock 33.294121188s] Trained 256 records in 0.080479135 seconds. Throughput is 3180.9485 records/second. Loss is 1.5042608. Sequentialfabab260's hyper parameters: Current learning rate is 0.009387908374014271. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:33 INFO  DistriOptimizer$:408 - [Epoch 2 23808/60000][Iteration 328][Wall Clock 33.365403619s] Trained 256 records in 0.071282431 seconds. Throughput is 3591.3477 records/second. Loss is 1.4455502. Sequentialfabab260's hyper parameters: Current learning rate is 0.009386146048432515. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:33 INFO  DistriOptimizer$:408 - [Epoch 2 24064/60000][Iteration 329][Wall Clock 33.438508783s] Trained 256 records in 0.073105164 seconds. Throughput is 3501.8047 records/second. Loss is 1.5422643. Sequentialfabab260's hyper parameters: Current learning rate is 0.009384384384384385. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:33 INFO  DistriOptimizer$:408 - [Epoch 2 24320/60000][Iteration 330][Wall Clock 33.51590071s] Trained 256 records in 0.077391927 seconds. Throughput is 3307.8384 records/second. Loss is 1.4413246. Sequentialfabab260's hyper parameters: Current learning rate is 0.009382623381497467. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:33 INFO  DistriOptimizer$:408 - [Epoch 2 24576/60000][Iteration 331][Wall Clock 33.601638308s] Trained 256 records in 0.085737598 seconds. Throughput is 2985.8545 records/second. Loss is 1.4682283. Sequentialfabab260's hyper parameters: Current learning rate is 0.009380863039399624. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:33 INFO  DistriOptimizer$:408 - [Epoch 2 24832/60000][Iteration 332][Wall Clock 33.687507459s] Trained 256 records in 0.085869151 seconds. Throughput is 2981.2803 records/second. Loss is 1.4346869. Sequentialfabab260's hyper parameters: Current learning rate is 0.009379103357719002. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:33 INFO  DistriOptimizer$:408 - [Epoch 2 25088/60000][Iteration 333][Wall Clock 33.767661616s] Trained 256 records in 0.080154157 seconds. Throughput is 3193.8455 records/second. Loss is 1.4855137. Sequentialfabab260's hyper parameters: Current learning rate is 0.009377344336084021. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:33 INFO  DistriOptimizer$:408 - [Epoch 2 25344/60000][Iteration 334][Wall Clock 33.848837839s] Trained 256 records in 0.081176223 seconds. Throughput is 3153.6328 records/second. Loss is 1.4494995. Sequentialfabab260's hyper parameters: Current learning rate is 0.009375585974123383. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:33 INFO  DistriOptimizer$:408 - [Epoch 2 25600/60000][Iteration 335][Wall Clock 33.929941469s] Trained 256 records in 0.08110363 seconds. Throughput is 3156.4556 records/second. Loss is 1.4428862. Sequentialfabab260's hyper parameters: Current learning rate is 0.009373828271466067. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:33 INFO  DistriOptimizer$:408 - [Epoch 2 25856/60000][Iteration 336][Wall Clock 34.036100439s] Trained 256 records in 0.10615897 seconds. Throughput is 2411.4778 records/second. Loss is 1.464012. Sequentialfabab260's hyper parameters: Current learning rate is 0.009372071227741332. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:33 INFO  DistriOptimizer$:408 - [Epoch 2 26112/60000][Iteration 337][Wall Clock 34.118632924s] Trained 256 records in 0.082532485 seconds. Throughput is 3101.8088 records/second. Loss is 1.4292547. Sequentialfabab260's hyper parameters: Current learning rate is 0.009370314842578711. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:34 INFO  DistriOptimizer$:408 - [Epoch 2 26368/60000][Iteration 338][Wall Clock 34.19785951s] Trained 256 records in 0.079226586 seconds. Throughput is 3231.2388 records/second. Loss is 1.483966. Sequentialfabab260's hyper parameters: Current learning rate is 0.00936855911560802. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:34 INFO  DistriOptimizer$:408 - [Epoch 2 26624/60000][Iteration 339][Wall Clock 34.275260995s] Trained 256 records in 0.077401485 seconds. Throughput is 3307.4302 records/second. Loss is 1.4489931. Sequentialfabab260's hyper parameters: Current learning rate is 0.009366804046459348. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:34 INFO  DistriOptimizer$:408 - [Epoch 2 26880/60000][Iteration 340][Wall Clock 34.358022363s] Trained 256 records in 0.082761368 seconds. Throughput is 3093.2305 records/second. Loss is 1.4864786. Sequentialfabab260's hyper parameters: Current learning rate is 0.009365049634763064. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:34 INFO  DistriOptimizer$:408 - [Epoch 2 27136/60000][Iteration 341][Wall Clock 34.451657812s] Trained 256 records in 0.093635449 seconds. Throughput is 2734.0073 records/second. Loss is 1.4235055. Sequentialfabab260's hyper parameters: Current learning rate is 0.009363295880149813. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:34 INFO  DistriOptimizer$:408 - [Epoch 2 27392/60000][Iteration 342][Wall Clock 34.531081692s] Trained 256 records in 0.07942388 seconds. Throughput is 3223.212 records/second. Loss is 1.4357798. Sequentialfabab260's hyper parameters: Current learning rate is 0.009361542782250515. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:34 INFO  DistriOptimizer$:408 - [Epoch 2 27648/60000][Iteration 343][Wall Clock 34.606482519s] Trained 256 records in 0.075400827 seconds. Throughput is 3395.1882 records/second. Loss is 1.4495077. Sequentialfabab260's hyper parameters: Current learning rate is 0.009359790340696368. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:34 INFO  DistriOptimizer$:408 - [Epoch 2 27904/60000][Iteration 344][Wall Clock 34.692018031s] Trained 256 records in 0.085535512 seconds. Throughput is 2992.909 records/second. Loss is 1.4084252. Sequentialfabab260's hyper parameters: Current learning rate is 0.009358038555118848. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:34 INFO  DistriOptimizer$:408 - [Epoch 2 28160/60000][Iteration 345][Wall Clock 34.779577477s] Trained 256 records in 0.087559446 seconds. Throughput is 2923.728 records/second. Loss is 1.4472785. Sequentialfabab260's hyper parameters: Current learning rate is 0.009356287425149701. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:34 INFO  DistriOptimizer$:408 - [Epoch 2 28416/60000][Iteration 346][Wall Clock 34.873929457s] Trained 256 records in 0.09435198 seconds. Throughput is 2713.2446 records/second. Loss is 1.4425596. Sequentialfabab260's hyper parameters: Current learning rate is 0.009354536950420956. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:34 INFO  DistriOptimizer$:408 - [Epoch 2 28672/60000][Iteration 347][Wall Clock 34.96787234s] Trained 256 records in 0.093942883 seconds. Throughput is 2725.06 records/second. Loss is 1.3429213. Sequentialfabab260's hyper parameters: Current learning rate is 0.00935278713056491. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:34 INFO  DistriOptimizer$:408 - [Epoch 2 28928/60000][Iteration 348][Wall Clock 35.067554912s] Trained 256 records in 0.099682572 seconds. Throughput is 2568.152 records/second. Loss is 1.4526472. Sequentialfabab260's hyper parameters: Current learning rate is 0.009351037965214139. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 29184/60000][Iteration 349][Wall Clock 35.154727229s] Trained 256 records in 0.087172317 seconds. Throughput is 2936.7122 records/second. Loss is 1.4515274. Sequentialfabab260's hyper parameters: Current learning rate is 0.009349289454001495. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 29440/60000][Iteration 350][Wall Clock 35.228844087s] Trained 256 records in 0.074116858 seconds. Throughput is 3454.0051 records/second. Loss is 1.415232. Sequentialfabab260's hyper parameters: Current learning rate is 0.009347541596560104. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 29696/60000][Iteration 351][Wall Clock 35.30251345s] Trained 256 records in 0.073669363 seconds. Throughput is 3474.9858 records/second. Loss is 1.412033. Sequentialfabab260's hyper parameters: Current learning rate is 0.009345794392523364. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 29952/60000][Iteration 352][Wall Clock 35.371029037s] Trained 256 records in 0.068515587 seconds. Throughput is 3736.3762 records/second. Loss is 1.3743131. Sequentialfabab260's hyper parameters: Current learning rate is 0.009344047841524948. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 30208/60000][Iteration 353][Wall Clock 35.445103915s] Trained 256 records in 0.074074878 seconds. Throughput is 3455.9624 records/second. Loss is 1.399741. Sequentialfabab260's hyper parameters: Current learning rate is 0.009342301943198805. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 30464/60000][Iteration 354][Wall Clock 35.526043579s] Trained 256 records in 0.080939664 seconds. Throughput is 3162.8499 records/second. Loss is 1.4107597. Sequentialfabab260's hyper parameters: Current learning rate is 0.009340556697179153. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 30720/60000][Iteration 355][Wall Clock 35.603860971s] Trained 256 records in 0.077817392 seconds. Throughput is 3289.753 records/second. Loss is 1.4174882. Sequentialfabab260's hyper parameters: Current learning rate is 0.009338812103100487. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 30976/60000][Iteration 356][Wall Clock 35.68051534s] Trained 256 records in 0.076654369 seconds. Throughput is 3339.6663 records/second. Loss is 1.4316485. Sequentialfabab260's hyper parameters: Current learning rate is 0.009337068160597572. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 31232/60000][Iteration 357][Wall Clock 35.760709311s] Trained 256 records in 0.080193971 seconds. Throughput is 3192.2598 records/second. Loss is 1.403991. Sequentialfabab260's hyper parameters: Current learning rate is 0.009335324869305453. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 31488/60000][Iteration 358][Wall Clock 35.844833922s] Trained 256 records in 0.084124611 seconds. Throughput is 3043.1047 records/second. Loss is 1.4023174. Sequentialfabab260's hyper parameters: Current learning rate is 0.009333582228859437. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 31744/60000][Iteration 359][Wall Clock 35.925888592s] Trained 256 records in 0.08105467 seconds. Throughput is 3158.362 records/second. Loss is 1.385045. Sequentialfabab260's hyper parameters: Current learning rate is 0.00933184023889511. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 32000/60000][Iteration 360][Wall Clock 36.004455616s] Trained 256 records in 0.078567024 seconds. Throughput is 3258.3645 records/second. Loss is 1.3877858. Sequentialfabab260's hyper parameters: Current learning rate is 0.009330098899048329. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:35 INFO  DistriOptimizer$:408 - [Epoch 2 32256/60000][Iteration 361][Wall Clock 36.076766453s] Trained 256 records in 0.072310837 seconds. Throughput is 3540.2717 records/second. Loss is 1.3657181. Sequentialfabab260's hyper parameters: Current learning rate is 0.009328358208955223. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:36 INFO  DistriOptimizer$:408 - [Epoch 2 32512/60000][Iteration 362][Wall Clock 36.147625258s] Trained 256 records in 0.070858805 seconds. Throughput is 3612.8184 records/second. Loss is 1.374117. Sequentialfabab260's hyper parameters: Current learning rate is 0.009326618168252192. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:36 INFO  DistriOptimizer$:408 - [Epoch 2 32768/60000][Iteration 363][Wall Clock 36.225296486s] Trained 256 records in 0.077671228 seconds. Throughput is 3295.9436 records/second. Loss is 1.4230237. Sequentialfabab260's hyper parameters: Current learning rate is 0.009324878776575904. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:36 INFO  DistriOptimizer$:408 - [Epoch 2 33024/60000][Iteration 364][Wall Clock 36.310376478s] Trained 256 records in 0.085079992 seconds. Throughput is 3008.933 records/second. Loss is 1.4180539. Sequentialfabab260's hyper parameters: Current learning rate is 0.009323140033563304. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:36 INFO  DistriOptimizer$:408 - [Epoch 2 33280/60000][Iteration 365][Wall Clock 36.389160093s] Trained 256 records in 0.078783615 seconds. Throughput is 3249.4065 records/second. Loss is 1.3659052. Sequentialfabab260's hyper parameters: Current learning rate is 0.009321401938851604. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:36 INFO  DistriOptimizer$:408 - [Epoch 2 33536/60000][Iteration 366][Wall Clock 36.468794138s] Trained 256 records in 0.079634045 seconds. Throughput is 3214.7053 records/second. Loss is 1.4339724. Sequentialfabab260's hyper parameters: Current learning rate is 0.009319664492078286. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:36 INFO  DistriOptimizer$:408 - [Epoch 2 33792/60000][Iteration 367][Wall Clock 36.539002334s] Trained 256 records in 0.070208196 seconds. Throughput is 3646.2979 records/second. Loss is 1.4193184. Sequentialfabab260's hyper parameters: Current learning rate is 0.009317927692881103. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:36 INFO  DistriOptimizer$:408 - [Epoch 2 34048/60000][Iteration 368][Wall Clock 36.614879179s] Trained 256 records in 0.075876845 seconds. Throughput is 3373.8882 records/second. Loss is 1.3040142. Sequentialfabab260's hyper parameters: Current learning rate is 0.009316191540898081. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:36 INFO  DistriOptimizer$:408 - [Epoch 2 34304/60000][Iteration 369][Wall Clock 36.697132229s] Trained 256 records in 0.08225305 seconds. Throughput is 3112.3467 records/second. Loss is 1.2953212. Sequentialfabab260's hyper parameters: Current learning rate is 0.009314456035767513. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:36 INFO  DistriOptimizer$:408 - [Epoch 2 34560/60000][Iteration 370][Wall Clock 36.780526691s] Trained 256 records in 0.083394462 seconds. Throughput is 3069.7483 records/second. Loss is 1.3751104. Sequentialfabab260's hyper parameters: Current learning rate is 0.009312721177127956. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:36 INFO  DistriOptimizer$:408 - [Epoch 2 34816/60000][Iteration 371][Wall Clock 36.865205165s] Trained 256 records in 0.084678474 seconds. Throughput is 3023.2004 records/second. Loss is 1.3425988. Sequentialfabab260's hyper parameters: Current learning rate is 0.009310986964618248. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:36 INFO  DistriOptimizer$:408 - [Epoch 2 35072/60000][Iteration 372][Wall Clock 36.952168991s] Trained 256 records in 0.086963826 seconds. Throughput is 2943.7527 records/second. Loss is 1.3822483. Sequentialfabab260's hyper parameters: Current learning rate is 0.00930925339787749. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:36 INFO  DistriOptimizer$:408 - [Epoch 2 35328/60000][Iteration 373][Wall Clock 37.052139221s] Trained 256 records in 0.09997023 seconds. Throughput is 2560.7625 records/second. Loss is 1.30242. Sequentialfabab260's hyper parameters: Current learning rate is 0.009307520476545048. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:37 INFO  DistriOptimizer$:408 - [Epoch 2 35584/60000][Iteration 374][Wall Clock 37.144832818s] Trained 256 records in 0.092693597 seconds. Throughput is 2761.7874 records/second. Loss is 1.3476037. Sequentialfabab260's hyper parameters: Current learning rate is 0.009305788200260562. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:37 INFO  DistriOptimizer$:408 - [Epoch 2 35840/60000][Iteration 375][Wall Clock 37.2329062s] Trained 256 records in 0.088073382 seconds. Throughput is 2906.6672 records/second. Loss is 1.3762045. Sequentialfabab260's hyper parameters: Current learning rate is 0.009304056568663939. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:37 INFO  DistriOptimizer$:408 - [Epoch 2 36096/60000][Iteration 376][Wall Clock 37.318699195s] Trained 256 records in 0.085792995 seconds. Throughput is 2983.9265 records/second. Loss is 1.3732249. Sequentialfabab260's hyper parameters: Current learning rate is 0.009302325581395349. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:37 INFO  DistriOptimizer$:408 - [Epoch 2 36352/60000][Iteration 377][Wall Clock 37.405002578s] Trained 256 records in 0.086303383 seconds. Throughput is 2966.28 records/second. Loss is 1.3530433. Sequentialfabab260's hyper parameters: Current learning rate is 0.009300595238095238. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:37 INFO  DistriOptimizer$:408 - [Epoch 2 36608/60000][Iteration 378][Wall Clock 37.485469696s] Trained 256 records in 0.080467118 seconds. Throughput is 3181.4236 records/second. Loss is 1.375042. Sequentialfabab260's hyper parameters: Current learning rate is 0.009298865538404316. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:37 INFO  DistriOptimizer$:408 - [Epoch 2 36864/60000][Iteration 379][Wall Clock 37.569373311s] Trained 256 records in 0.083903615 seconds. Throughput is 3051.1199 records/second. Loss is 1.3082384. Sequentialfabab260's hyper parameters: Current learning rate is 0.009297136481963555. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:37 INFO  DistriOptimizer$:408 - [Epoch 2 37120/60000][Iteration 380][Wall Clock 37.654806853s] Trained 256 records in 0.085433542 seconds. Throughput is 2996.4812 records/second. Loss is 1.2979848. Sequentialfabab260's hyper parameters: Current learning rate is 0.009295408068414203. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:37 INFO  DistriOptimizer$:408 - [Epoch 2 37376/60000][Iteration 381][Wall Clock 37.736311666s] Trained 256 records in 0.081504813 seconds. Throughput is 3140.9187 records/second. Loss is 1.2954028. Sequentialfabab260's hyper parameters: Current learning rate is 0.00929368029739777. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:37 INFO  DistriOptimizer$:408 - [Epoch 2 37632/60000][Iteration 382][Wall Clock 37.825924688s] Trained 256 records in 0.089613022 seconds. Throughput is 2856.7278 records/second. Loss is 1.2717732. Sequentialfabab260's hyper parameters: Current learning rate is 0.00929195316855603. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:37 INFO  DistriOptimizer$:408 - [Epoch 2 37888/60000][Iteration 383][Wall Clock 37.904895708s] Trained 256 records in 0.07897102 seconds. Throughput is 3241.6956 records/second. Loss is 1.3313864. Sequentialfabab260's hyper parameters: Current learning rate is 0.00929022668153103. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:37 INFO  DistriOptimizer$:408 - [Epoch 2 38144/60000][Iteration 384][Wall Clock 37.978760363s] Trained 256 records in 0.073864655 seconds. Throughput is 3465.7983 records/second. Loss is 1.3043638. Sequentialfabab260's hyper parameters: Current learning rate is 0.009288500835965075. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:37 INFO  DistriOptimizer$:408 - [Epoch 2 38400/60000][Iteration 385][Wall Clock 38.056874072s] Trained 256 records in 0.078113709 seconds. Throughput is 3277.2734 records/second. Loss is 1.4005262. Sequentialfabab260's hyper parameters: Current learning rate is 0.009286775631500743. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:38 INFO  DistriOptimizer$:408 - [Epoch 2 38656/60000][Iteration 386][Wall Clock 38.126305842s] Trained 256 records in 0.06943177 seconds. Throughput is 3687.073 records/second. Loss is 1.2717582. Sequentialfabab260's hyper parameters: Current learning rate is 0.009285051067780874. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:38 INFO  DistriOptimizer$:408 - [Epoch 2 38912/60000][Iteration 387][Wall Clock 38.202732041s] Trained 256 records in 0.076426199 seconds. Throughput is 3349.6367 records/second. Loss is 1.356115. Sequentialfabab260's hyper parameters: Current learning rate is 0.00928332714444857. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:38 INFO  DistriOptimizer$:408 - [Epoch 2 39168/60000][Iteration 388][Wall Clock 38.27438199s] Trained 256 records in 0.071649949 seconds. Throughput is 3572.9265 records/second. Loss is 1.3171917. Sequentialfabab260's hyper parameters: Current learning rate is 0.009281603861147207. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:38 INFO  DistriOptimizer$:408 - [Epoch 2 39424/60000][Iteration 389][Wall Clock 38.356291783s] Trained 256 records in 0.081909793 seconds. Throughput is 3125.3894 records/second. Loss is 1.3195449. Sequentialfabab260's hyper parameters: Current learning rate is 0.009279881217520417. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:38 INFO  DistriOptimizer$:408 - [Epoch 2 39680/60000][Iteration 390][Wall Clock 38.444785595s] Trained 256 records in 0.088493812 seconds. Throughput is 2892.8577 records/second. Loss is 1.327673. Sequentialfabab260's hyper parameters: Current learning rate is 0.009278159213212098. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:38 INFO  DistriOptimizer$:408 - [Epoch 2 39936/60000][Iteration 391][Wall Clock 38.518901122s] Trained 256 records in 0.074115527 seconds. Throughput is 3454.067 records/second. Loss is 1.3318577. Sequentialfabab260's hyper parameters: Current learning rate is 0.00927643784786642. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:38 INFO  DistriOptimizer$:408 - [Epoch 2 40192/60000][Iteration 392][Wall Clock 38.611127464s] Trained 256 records in 0.092226342 seconds. Throughput is 2775.7795 records/second. Loss is 1.2887566. Sequentialfabab260's hyper parameters: Current learning rate is 0.009274717121127806. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:38 INFO  DistriOptimizer$:408 - [Epoch 2 40448/60000][Iteration 393][Wall Clock 38.700635839s] Trained 256 records in 0.089508375 seconds. Throughput is 2860.0674 records/second. Loss is 1.2379396. Sequentialfabab260's hyper parameters: Current learning rate is 0.009272997032640949. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:38 INFO  DistriOptimizer$:408 - [Epoch 2 40704/60000][Iteration 394][Wall Clock 38.780338666s] Trained 256 records in 0.079702827 seconds. Throughput is 3211.9314 records/second. Loss is 1.3082556. Sequentialfabab260's hyper parameters: Current learning rate is 0.009271277582050807. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:38 INFO  DistriOptimizer$:408 - [Epoch 2 40960/60000][Iteration 395][Wall Clock 38.864829946s] Trained 256 records in 0.08449128 seconds. Throughput is 3029.8984 records/second. Loss is 1.3015118. Sequentialfabab260's hyper parameters: Current learning rate is 0.009269558769002597. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:38 INFO  DistriOptimizer$:408 - [Epoch 2 41216/60000][Iteration 396][Wall Clock 38.957256671s] Trained 256 records in 0.092426725 seconds. Throughput is 2769.7617 records/second. Loss is 1.2753706. Sequentialfabab260's hyper parameters: Current learning rate is 0.009267840593141799. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:38 INFO  DistriOptimizer$:408 - [Epoch 2 41472/60000][Iteration 397][Wall Clock 39.049586292s] Trained 256 records in 0.092329621 seconds. Throughput is 2772.6746 records/second. Loss is 1.2980397. Sequentialfabab260's hyper parameters: Current learning rate is 0.009266123054114159. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:39 INFO  DistriOptimizer$:408 - [Epoch 2 41728/60000][Iteration 398][Wall Clock 39.143354617s] Trained 256 records in 0.093768325 seconds. Throughput is 2730.1328 records/second. Loss is 1.3165902. Sequentialfabab260's hyper parameters: Current learning rate is 0.009264406151565686. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:39 INFO  DistriOptimizer$:408 - [Epoch 2 41984/60000][Iteration 399][Wall Clock 39.232968104s] Trained 256 records in 0.089613487 seconds. Throughput is 2856.7126 records/second. Loss is 1.1868466. Sequentialfabab260's hyper parameters: Current learning rate is 0.009262689885142644. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:39 INFO  DistriOptimizer$:408 - [Epoch 2 42240/60000][Iteration 400][Wall Clock 39.329802896s] Trained 256 records in 0.096834792 seconds. Throughput is 2643.678 records/second. Loss is 1.2896614. Sequentialfabab260's hyper parameters: Current learning rate is 0.009260974254491572. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:39 INFO  DistriOptimizer$:408 - [Epoch 2 42496/60000][Iteration 401][Wall Clock 39.428274971s] Trained 256 records in 0.098472075 seconds. Throughput is 2599.722 records/second. Loss is 1.1946871. Sequentialfabab260's hyper parameters: Current learning rate is 0.009259259259259259. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:39 INFO  DistriOptimizer$:408 - [Epoch 2 42752/60000][Iteration 402][Wall Clock 39.513657025s] Trained 256 records in 0.085382054 seconds. Throughput is 2998.2883 records/second. Loss is 1.2840681. Sequentialfabab260's hyper parameters: Current learning rate is 0.00925754489909276. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:39 INFO  DistriOptimizer$:408 - [Epoch 2 43008/60000][Iteration 403][Wall Clock 39.594974575s] Trained 256 records in 0.08131755 seconds. Throughput is 3148.1519 records/second. Loss is 1.2147467. Sequentialfabab260's hyper parameters: Current learning rate is 0.009255831173639394. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:39 INFO  DistriOptimizer$:408 - [Epoch 2 43264/60000][Iteration 404][Wall Clock 39.714253789s] Trained 256 records in 0.119279214 seconds. Throughput is 2146.2249 records/second. Loss is 1.2892454. Sequentialfabab260's hyper parameters: Current learning rate is 0.009254118082546734. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:39 INFO  DistriOptimizer$:408 - [Epoch 2 43520/60000][Iteration 405][Wall Clock 39.78075658s] Trained 256 records in 0.066502791 seconds. Throughput is 3849.4624 records/second. Loss is 1.2734284. Sequentialfabab260's hyper parameters: Current learning rate is 0.009252405625462621. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:39 INFO  DistriOptimizer$:408 - [Epoch 2 43776/60000][Iteration 406][Wall Clock 39.855168999s] Trained 256 records in 0.074412419 seconds. Throughput is 3440.286 records/second. Loss is 1.2778234. Sequentialfabab260's hyper parameters: Current learning rate is 0.009250693802035153. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:39 INFO  DistriOptimizer$:408 - [Epoch 2 44032/60000][Iteration 407][Wall Clock 39.935953502s] Trained 256 records in 0.080784503 seconds. Throughput is 3168.9248 records/second. Loss is 1.2616293. Sequentialfabab260's hyper parameters: Current learning rate is 0.00924898261191269. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:39 INFO  DistriOptimizer$:408 - [Epoch 2 44288/60000][Iteration 408][Wall Clock 40.029436972s] Trained 256 records in 0.09348347 seconds. Throughput is 2738.4521 records/second. Loss is 1.2875698. Sequentialfabab260's hyper parameters: Current learning rate is 0.009247272054743851. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:39 INFO  DistriOptimizer$:408 - [Epoch 2 44544/60000][Iteration 409][Wall Clock 40.102026642s] Trained 256 records in 0.07258967 seconds. Throughput is 3526.6724 records/second. Loss is 1.2181399. Sequentialfabab260's hyper parameters: Current learning rate is 0.009245562130177515. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:40 INFO  DistriOptimizer$:408 - [Epoch 2 44800/60000][Iteration 410][Wall Clock 40.184577606s] Trained 256 records in 0.082550964 seconds. Throughput is 3101.1145 records/second. Loss is 1.1761343. Sequentialfabab260's hyper parameters: Current learning rate is 0.009243852837862821. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:40 INFO  DistriOptimizer$:408 - [Epoch 2 45056/60000][Iteration 411][Wall Clock 40.251654743s] Trained 256 records in 0.067077137 seconds. Throughput is 3816.5015 records/second. Loss is 1.2554959. Sequentialfabab260's hyper parameters: Current learning rate is 0.009242144177449167. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:40 INFO  DistriOptimizer$:408 - [Epoch 2 45312/60000][Iteration 412][Wall Clock 40.329758606s] Trained 256 records in 0.078103863 seconds. Throughput is 3277.6868 records/second. Loss is 1.1889665. Sequentialfabab260's hyper parameters: Current learning rate is 0.009240436148586212. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:40 INFO  DistriOptimizer$:408 - [Epoch 2 45568/60000][Iteration 413][Wall Clock 40.401330199s] Trained 256 records in 0.071571593 seconds. Throughput is 3576.838 records/second. Loss is 1.2254775. Sequentialfabab260's hyper parameters: Current learning rate is 0.009238728750923873. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:40 INFO  DistriOptimizer$:408 - [Epoch 2 45824/60000][Iteration 414][Wall Clock 40.47944948s] Trained 256 records in 0.078119281 seconds. Throughput is 3277.04 records/second. Loss is 1.2737012. Sequentialfabab260's hyper parameters: Current learning rate is 0.009237021984112323. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:40 INFO  DistriOptimizer$:408 - [Epoch 2 46080/60000][Iteration 415][Wall Clock 40.564131179s] Trained 256 records in 0.084681699 seconds. Throughput is 3023.0854 records/second. Loss is 1.156501. Sequentialfabab260's hyper parameters: Current learning rate is 0.009235315847801994. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:40 INFO  DistriOptimizer$:408 - [Epoch 2 46336/60000][Iteration 416][Wall Clock 40.643473832s] Trained 256 records in 0.079342653 seconds. Throughput is 3226.5117 records/second. Loss is 1.2539829. Sequentialfabab260's hyper parameters: Current learning rate is 0.009233610341643583. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:40 INFO  DistriOptimizer$:408 - [Epoch 2 46592/60000][Iteration 417][Wall Clock 40.713898892s] Trained 256 records in 0.07042506 seconds. Throughput is 3635.0696 records/second. Loss is 1.20893. Sequentialfabab260's hyper parameters: Current learning rate is 0.009231905465288036. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:40 INFO  DistriOptimizer$:408 - [Epoch 2 46848/60000][Iteration 418][Wall Clock 40.801761008s] Trained 256 records in 0.087862116 seconds. Throughput is 2913.656 records/second. Loss is 1.1952337. Sequentialfabab260's hyper parameters: Current learning rate is 0.009230201218386561. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:40 INFO  DistriOptimizer$:408 - [Epoch 2 47104/60000][Iteration 419][Wall Clock 40.875535478s] Trained 256 records in 0.07377447 seconds. Throughput is 3470.035 records/second. Loss is 1.170227. Sequentialfabab260's hyper parameters: Current learning rate is 0.009228497600590623. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:40 INFO  DistriOptimizer$:408 - [Epoch 2 47360/60000][Iteration 420][Wall Clock 40.94814989s] Trained 256 records in 0.072614412 seconds. Throughput is 3525.4712 records/second. Loss is 1.19555. Sequentialfabab260's hyper parameters: Current learning rate is 0.009226794611551946. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:40 INFO  DistriOptimizer$:408 - [Epoch 2 47616/60000][Iteration 421][Wall Clock 41.028802566s] Trained 256 records in 0.080652676 seconds. Throughput is 3174.1042 records/second. Loss is 1.1856862. Sequentialfabab260's hyper parameters: Current learning rate is 0.00922509225092251. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 47872/60000][Iteration 422][Wall Clock 41.123692553s] Trained 256 records in 0.094889987 seconds. Throughput is 2697.861 records/second. Loss is 1.2288198. Sequentialfabab260's hyper parameters: Current learning rate is 0.009223390518354546. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 48128/60000][Iteration 423][Wall Clock 41.195576641s] Trained 256 records in 0.071884088 seconds. Throughput is 3561.2888 records/second. Loss is 1.1959397. Sequentialfabab260's hyper parameters: Current learning rate is 0.009221689413500553. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 48384/60000][Iteration 424][Wall Clock 41.300569113s] Trained 256 records in 0.104992472 seconds. Throughput is 2438.27 records/second. Loss is 1.2332103. Sequentialfabab260's hyper parameters: Current learning rate is 0.009219988936013277. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 48640/60000][Iteration 425][Wall Clock 41.379980024s] Trained 256 records in 0.079410911 seconds. Throughput is 3223.7383 records/second. Loss is 1.2257738. Sequentialfabab260's hyper parameters: Current learning rate is 0.009218289085545723. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 48896/60000][Iteration 426][Wall Clock 41.454641745s] Trained 256 records in 0.074661721 seconds. Throughput is 3428.7983 records/second. Loss is 1.3106545. Sequentialfabab260's hyper parameters: Current learning rate is 0.009216589861751152. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 49152/60000][Iteration 427][Wall Clock 41.527158922s] Trained 256 records in 0.072517177 seconds. Throughput is 3530.198 records/second. Loss is 1.1698387. Sequentialfabab260's hyper parameters: Current learning rate is 0.009214891264283083. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 49408/60000][Iteration 428][Wall Clock 41.597477932s] Trained 256 records in 0.07031901 seconds. Throughput is 3640.5518 records/second. Loss is 1.1476676. Sequentialfabab260's hyper parameters: Current learning rate is 0.009213193292795284. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 49664/60000][Iteration 429][Wall Clock 41.67595686s] Trained 256 records in 0.078478928 seconds. Throughput is 3262.0222 records/second. Loss is 1.2135621. Sequentialfabab260's hyper parameters: Current learning rate is 0.009211495946941784. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 49920/60000][Iteration 430][Wall Clock 41.751208822s] Trained 256 records in 0.075251962 seconds. Throughput is 3401.9048 records/second. Loss is 1.1710712. Sequentialfabab260's hyper parameters: Current learning rate is 0.009209799226376865. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 50176/60000][Iteration 431][Wall Clock 41.823589708s] Trained 256 records in 0.072380886 seconds. Throughput is 3536.8455 records/second. Loss is 1.1990882. Sequentialfabab260's hyper parameters: Current learning rate is 0.009208103130755063. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 50432/60000][Iteration 432][Wall Clock 41.893592942s] Trained 256 records in 0.070003234 seconds. Throughput is 3656.9739 records/second. Loss is 1.1718154. Sequentialfabab260's hyper parameters: Current learning rate is 0.009206407659731172. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 50688/60000][Iteration 433][Wall Clock 41.972139418s] Trained 256 records in 0.078546476 seconds. Throughput is 3259.2168 records/second. Loss is 1.1876354. Sequentialfabab260's hyper parameters: Current learning rate is 0.009204712812960236. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:41 INFO  DistriOptimizer$:408 - [Epoch 2 50944/60000][Iteration 434][Wall Clock 42.053050567s] Trained 256 records in 0.080911149 seconds. Throughput is 3163.9644 records/second. Loss is 1.1805727. Sequentialfabab260's hyper parameters: Current learning rate is 0.009203018590097553. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 51200/60000][Iteration 435][Wall Clock 42.131897468s] Trained 256 records in 0.078846901 seconds. Throughput is 3246.7986 records/second. Loss is 1.2460811. Sequentialfabab260's hyper parameters: Current learning rate is 0.009201324990798676. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 51456/60000][Iteration 436][Wall Clock 42.207788322s] Trained 256 records in 0.075890854 seconds. Throughput is 3373.2656 records/second. Loss is 1.2068835. Sequentialfabab260's hyper parameters: Current learning rate is 0.009199632014719412. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 51712/60000][Iteration 437][Wall Clock 42.281886449s] Trained 256 records in 0.074098127 seconds. Throughput is 3454.8782 records/second. Loss is 1.219309. Sequentialfabab260's hyper parameters: Current learning rate is 0.009197939661515822. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 51968/60000][Iteration 438][Wall Clock 42.351469572s] Trained 256 records in 0.069583123 seconds. Throughput is 3679.053 records/second. Loss is 1.2048483. Sequentialfabab260's hyper parameters: Current learning rate is 0.009196247930844217. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 52224/60000][Iteration 439][Wall Clock 42.421919331s] Trained 256 records in 0.070449759 seconds. Throughput is 3633.7952 records/second. Loss is 1.1521243. Sequentialfabab260's hyper parameters: Current learning rate is 0.009194556822361163. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 52480/60000][Iteration 440][Wall Clock 42.500935692s] Trained 256 records in 0.079016361 seconds. Throughput is 3239.8354 records/second. Loss is 1.1341374. Sequentialfabab260's hyper parameters: Current learning rate is 0.009192866335723478. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 52736/60000][Iteration 441][Wall Clock 42.574575156s] Trained 256 records in 0.073639464 seconds. Throughput is 3476.3967 records/second. Loss is 1.1341921. Sequentialfabab260's hyper parameters: Current learning rate is 0.009191176470588236. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 52992/60000][Iteration 442][Wall Clock 42.639097918s] Trained 256 records in 0.064522762 seconds. Throughput is 3967.5918 records/second. Loss is 1.2021313. Sequentialfabab260's hyper parameters: Current learning rate is 0.009189487226612754. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 53248/60000][Iteration 443][Wall Clock 42.713265473s] Trained 256 records in 0.074167555 seconds. Throughput is 3451.644 records/second. Loss is 1.1542988. Sequentialfabab260's hyper parameters: Current learning rate is 0.009187798603454611. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 53504/60000][Iteration 444][Wall Clock 42.78697364s] Trained 256 records in 0.073708167 seconds. Throughput is 3473.1565 records/second. Loss is 1.1693287. Sequentialfabab260's hyper parameters: Current learning rate is 0.009186110600771633. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 53760/60000][Iteration 445][Wall Clock 42.869563406s] Trained 256 records in 0.082589766 seconds. Throughput is 3099.6575 records/second. Loss is 1.1066482. Sequentialfabab260's hyper parameters: Current learning rate is 0.009184423218221896. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 54016/60000][Iteration 446][Wall Clock 42.95252043s] Trained 256 records in 0.082957024 seconds. Throughput is 3085.9353 records/second. Loss is 1.1834424. Sequentialfabab260's hyper parameters: Current learning rate is 0.009182736455463728. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:42 INFO  DistriOptimizer$:408 - [Epoch 2 54272/60000][Iteration 447][Wall Clock 43.056862255s] Trained 256 records in 0.104341825 seconds. Throughput is 2453.4744 records/second. Loss is 1.1467345. Sequentialfabab260's hyper parameters: Current learning rate is 0.00918105031215571. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:43 INFO  DistriOptimizer$:408 - [Epoch 2 54528/60000][Iteration 448][Wall Clock 43.153927114s] Trained 256 records in 0.097064859 seconds. Throughput is 2637.4116 records/second. Loss is 1.2058965. Sequentialfabab260's hyper parameters: Current learning rate is 0.009179364787956674. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:43 INFO  DistriOptimizer$:408 - [Epoch 2 54784/60000][Iteration 449][Wall Clock 43.251477545s] Trained 256 records in 0.097550431 seconds. Throughput is 2624.2837 records/second. Loss is 1.1304917. Sequentialfabab260's hyper parameters: Current learning rate is 0.009177679882525698. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:43 INFO  DistriOptimizer$:408 - [Epoch 2 55040/60000][Iteration 450][Wall Clock 43.333122988s] Trained 256 records in 0.081645443 seconds. Throughput is 3135.5088 records/second. Loss is 1.1580638. Sequentialfabab260's hyper parameters: Current learning rate is 0.009175995595522114. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:43 INFO  DistriOptimizer$:408 - [Epoch 2 55296/60000][Iteration 451][Wall Clock 43.40914348s] Trained 256 records in 0.076020492 seconds. Throughput is 3367.513 records/second. Loss is 1.1570582. Sequentialfabab260's hyper parameters: Current learning rate is 0.009174311926605503. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:43 INFO  DistriOptimizer$:408 - [Epoch 2 55552/60000][Iteration 452][Wall Clock 43.488004851s] Trained 256 records in 0.078861371 seconds. Throughput is 3246.203 records/second. Loss is 1.1241692. Sequentialfabab260's hyper parameters: Current learning rate is 0.0091726288754357. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:43 INFO  DistriOptimizer$:408 - [Epoch 2 55808/60000][Iteration 453][Wall Clock 43.565952004s] Trained 256 records in 0.077947153 seconds. Throughput is 3284.2764 records/second. Loss is 1.141378. Sequentialfabab260's hyper parameters: Current learning rate is 0.009170946441672781. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:43 INFO  DistriOptimizer$:408 - [Epoch 2 56064/60000][Iteration 454][Wall Clock 43.649961641s] Trained 256 records in 0.084009637 seconds. Throughput is 3047.2693 records/second. Loss is 1.0630025. Sequentialfabab260's hyper parameters: Current learning rate is 0.009169264624977077. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:43 INFO  DistriOptimizer$:408 - [Epoch 2 56320/60000][Iteration 455][Wall Clock 43.734973014s] Trained 256 records in 0.085011373 seconds. Throughput is 3011.3618 records/second. Loss is 1.0885265. Sequentialfabab260's hyper parameters: Current learning rate is 0.009167583425009168. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:43 INFO  DistriOptimizer$:408 - [Epoch 2 56576/60000][Iteration 456][Wall Clock 43.800499s] Trained 256 records in 0.065525986 seconds. Throughput is 3906.847 records/second. Loss is 1.1522658. Sequentialfabab260's hyper parameters: Current learning rate is 0.00916590284142988. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:43 INFO  DistriOptimizer$:408 - [Epoch 2 56832/60000][Iteration 457][Wall Clock 43.875069489s] Trained 256 records in 0.074570489 seconds. Throughput is 3432.9934 records/second. Loss is 1.0896655. Sequentialfabab260's hyper parameters: Current learning rate is 0.009164222873900294. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:43 INFO  DistriOptimizer$:408 - [Epoch 2 57088/60000][Iteration 458][Wall Clock 43.951766362s] Trained 256 records in 0.076696873 seconds. Throughput is 3337.8154 records/second. Loss is 1.2005022. Sequentialfabab260's hyper parameters: Current learning rate is 0.00916254352208173. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:43 INFO  DistriOptimizer$:408 - [Epoch 2 57344/60000][Iteration 459][Wall Clock 44.028005219s] Trained 256 records in 0.076238857 seconds. Throughput is 3357.868 records/second. Loss is 1.1875727. Sequentialfabab260's hyper parameters: Current learning rate is 0.009160864785635764. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:44 INFO  DistriOptimizer$:408 - [Epoch 2 57600/60000][Iteration 460][Wall Clock 44.101002395s] Trained 256 records in 0.072997176 seconds. Throughput is 3506.985 records/second. Loss is 1.1288826. Sequentialfabab260's hyper parameters: Current learning rate is 0.009159186664224217. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:44 INFO  DistriOptimizer$:408 - [Epoch 2 57856/60000][Iteration 461][Wall Clock 44.175111737s] Trained 256 records in 0.074109342 seconds. Throughput is 3454.3552 records/second. Loss is 1.0983839. Sequentialfabab260's hyper parameters: Current learning rate is 0.009157509157509156. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:44 INFO  DistriOptimizer$:408 - [Epoch 2 58112/60000][Iteration 462][Wall Clock 44.24040997s] Trained 256 records in 0.065298233 seconds. Throughput is 3920.4739 records/second. Loss is 1.0398029. Sequentialfabab260's hyper parameters: Current learning rate is 0.009155832265152902. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:44 INFO  DistriOptimizer$:408 - [Epoch 2 58368/60000][Iteration 463][Wall Clock 44.325319506s] Trained 256 records in 0.084909536 seconds. Throughput is 3014.9734 records/second. Loss is 1.1153133. Sequentialfabab260's hyper parameters: Current learning rate is 0.009154155986818015. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:44 INFO  DistriOptimizer$:408 - [Epoch 2 58624/60000][Iteration 464][Wall Clock 44.402613466s] Trained 256 records in 0.07729396 seconds. Throughput is 3312.031 records/second. Loss is 1.0601932. Sequentialfabab260's hyper parameters: Current learning rate is 0.009152480322167308. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:44 INFO  DistriOptimizer$:408 - [Epoch 2 58880/60000][Iteration 465][Wall Clock 44.474559242s] Trained 256 records in 0.071945776 seconds. Throughput is 3558.235 records/second. Loss is 1.1433451. Sequentialfabab260's hyper parameters: Current learning rate is 0.009150805270863836. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:44 INFO  DistriOptimizer$:408 - [Epoch 2 59136/60000][Iteration 466][Wall Clock 44.549048143s] Trained 256 records in 0.074488901 seconds. Throughput is 3436.7537 records/second. Loss is 1.0887249. Sequentialfabab260's hyper parameters: Current learning rate is 0.009149130832570906. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:44 INFO  DistriOptimizer$:408 - [Epoch 2 59392/60000][Iteration 467][Wall Clock 44.623568488s] Trained 256 records in 0.074520345 seconds. Throughput is 3435.3037 records/second. Loss is 1.0639112. Sequentialfabab260's hyper parameters: Current learning rate is 0.009147457006952069. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:44 INFO  DistriOptimizer$:408 - [Epoch 2 59648/60000][Iteration 468][Wall Clock 44.701510963s] Trained 256 records in 0.077942475 seconds. Throughput is 3284.4736 records/second. Loss is 1.0826342. Sequentialfabab260's hyper parameters: Current learning rate is 0.009145783793671118. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:44 INFO  DistriOptimizer$:408 - [Epoch 2 59904/60000][Iteration 469][Wall Clock 44.767841436s] Trained 256 records in 0.066330473 seconds. Throughput is 3859.4631 records/second. Loss is 1.1638302. Sequentialfabab260's hyper parameters: Current learning rate is 0.0091441111923921. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:44 INFO  DistriOptimizer$:408 - [Epoch 2 60160/60000][Iteration 470][Wall Clock 44.831972398s] Trained 256 records in 0.064130962 seconds. Throughput is 3991.8315 records/second. Loss is 1.0861453. Sequentialfabab260's hyper parameters: Current learning rate is 0.0091424392027793. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:44 INFO  DistriOptimizer$:452 - [Epoch 2 60160/60000][Iteration 470][Wall Clock 44.831972398s] Epoch finished. Wall clock time is 45367.93116 ms
2019-10-17 12:21:44 INFO  DistriOptimizer$:111 - [Epoch 2 60160/60000][Iteration 470][Wall Clock 44.831972398s] Validate model...
2019-10-17 12:21:45 INFO  DistriOptimizer$:178 - [Epoch 2 60160/60000][Iteration 470][Wall Clock 44.831972398s] validate model throughput is 38105.56 records/second
2019-10-17 12:21:45 INFO  DistriOptimizer$:181 - [Epoch 2 60160/60000][Iteration 470][Wall Clock 44.831972398s] Top1Accuracy is Accuracy(correct: 7485, count: 10000, accuracy: 0.7485)
2019-10-17 12:21:45 INFO  DistriOptimizer$:221 - [Wall Clock 45.36793116s] Save model to /tmp/lenet5/20191017_122059
2019-10-17 12:21:45 INFO  DistriOptimizer$:226 - [Wall Clock 45.36793116s] Save optimMethod com.intel.analytics.bigdl.optim.SGD@9429a82 to /tmp/lenet5/20191017_122059
2019-10-17 12:21:45 INFO  DistriOptimizer$:408 - [Epoch 3 256/60000][Iteration 471][Wall Clock 45.476190183s] Trained 256 records in 0.108259023 seconds. Throughput is 2364.699 records/second. Loss is 1.0962298. Sequentialfabab260's hyper parameters: Current learning rate is 0.009140767824497258. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:45 INFO  DistriOptimizer$:408 - [Epoch 3 512/60000][Iteration 472][Wall Clock 45.568641967s] Trained 256 records in 0.092451784 seconds. Throughput is 2769.011 records/second. Loss is 1.0458864. Sequentialfabab260's hyper parameters: Current learning rate is 0.009139097057210747. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:45 INFO  DistriOptimizer$:408 - [Epoch 3 768/60000][Iteration 473][Wall Clock 45.663314539s] Trained 256 records in 0.094672572 seconds. Throughput is 2704.0566 records/second. Loss is 1.122916. Sequentialfabab260's hyper parameters: Current learning rate is 0.009137426900584795. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:45 INFO  DistriOptimizer$:408 - [Epoch 3 1024/60000][Iteration 474][Wall Clock 45.745072587s] Trained 256 records in 0.081758048 seconds. Throughput is 3131.1904 records/second. Loss is 1.1315132. Sequentialfabab260's hyper parameters: Current learning rate is 0.00913575735428467. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:45 INFO  DistriOptimizer$:408 - [Epoch 3 1280/60000][Iteration 475][Wall Clock 45.815696206s] Trained 256 records in 0.070623619 seconds. Throughput is 3624.8496 records/second. Loss is 1.1175431. Sequentialfabab260's hyper parameters: Current learning rate is 0.009134088417975887. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:45 INFO  DistriOptimizer$:408 - [Epoch 3 1536/60000][Iteration 476][Wall Clock 45.890658292s] Trained 256 records in 0.074962086 seconds. Throughput is 3415.0596 records/second. Loss is 1.0558951. Sequentialfabab260's hyper parameters: Current learning rate is 0.009132420091324202. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:45 INFO  DistriOptimizer$:408 - [Epoch 3 1792/60000][Iteration 477][Wall Clock 45.968531956s] Trained 256 records in 0.077873664 seconds. Throughput is 3287.376 records/second. Loss is 1.052136. Sequentialfabab260's hyper parameters: Current learning rate is 0.009130752373995618. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:45 INFO  DistriOptimizer$:408 - [Epoch 3 2048/60000][Iteration 478][Wall Clock 46.041098545s] Trained 256 records in 0.072566589 seconds. Throughput is 3527.7942 records/second. Loss is 1.0482581. Sequentialfabab260's hyper parameters: Current learning rate is 0.009129085265656383. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:45 INFO  DistriOptimizer$:408 - [Epoch 3 2304/60000][Iteration 479][Wall Clock 46.114044009s] Trained 256 records in 0.072945464 seconds. Throughput is 3509.4712 records/second. Loss is 1.1154377. Sequentialfabab260's hyper parameters: Current learning rate is 0.009127418765972983. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:45 INFO  DistriOptimizer$:408 - [Epoch 3 2560/60000][Iteration 480][Wall Clock 46.18929807s] Trained 256 records in 0.075254061 seconds. Throughput is 3401.8098 records/second. Loss is 1.0654848. Sequentialfabab260's hyper parameters: Current learning rate is 0.009125752874612154. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 2816/60000][Iteration 481][Wall Clock 46.273547716s] Trained 256 records in 0.084249646 seconds. Throughput is 3038.5884 records/second. Loss is 1.1207771. Sequentialfabab260's hyper parameters: Current learning rate is 0.009124087591240875. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 3072/60000][Iteration 482][Wall Clock 46.348535414s] Trained 256 records in 0.074987698 seconds. Throughput is 3413.8936 records/second. Loss is 1.0651493. Sequentialfabab260's hyper parameters: Current learning rate is 0.009122422915526363. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 3328/60000][Iteration 483][Wall Clock 46.423586649s] Trained 256 records in 0.075051235 seconds. Throughput is 3411.0032 records/second. Loss is 1.1004336. Sequentialfabab260's hyper parameters: Current learning rate is 0.009120758847136081. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 3584/60000][Iteration 484][Wall Clock 46.492554854s] Trained 256 records in 0.068968205 seconds. Throughput is 3711.8552 records/second. Loss is 1.1001567. Sequentialfabab260's hyper parameters: Current learning rate is 0.009119095385737734. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 3840/60000][Iteration 485][Wall Clock 46.57476806s] Trained 256 records in 0.082213206 seconds. Throughput is 3113.855 records/second. Loss is 1.167652. Sequentialfabab260's hyper parameters: Current learning rate is 0.00911743253099927. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 4096/60000][Iteration 486][Wall Clock 46.662914926s] Trained 256 records in 0.088146866 seconds. Throughput is 2904.244 records/second. Loss is 1.0319413. Sequentialfabab260's hyper parameters: Current learning rate is 0.00911577028258888. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 4352/60000][Iteration 487][Wall Clock 46.739653867s] Trained 256 records in 0.076738941 seconds. Throughput is 3335.9856 records/second. Loss is 1.0367. Sequentialfabab260's hyper parameters: Current learning rate is 0.009114108640174992. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 4608/60000][Iteration 488][Wall Clock 46.811409963s] Trained 256 records in 0.071756096 seconds. Throughput is 3567.641 records/second. Loss is 1.0408896. Sequentialfabab260's hyper parameters: Current learning rate is 0.00911244760342628. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 4864/60000][Iteration 489][Wall Clock 46.886164452s] Trained 256 records in 0.074754489 seconds. Throughput is 3424.5435 records/second. Loss is 1.0488822. Sequentialfabab260's hyper parameters: Current learning rate is 0.009110787172011662. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 5120/60000][Iteration 490][Wall Clock 46.959008065s] Trained 256 records in 0.072843613 seconds. Throughput is 3514.3782 records/second. Loss is 1.0292926. Sequentialfabab260's hyper parameters: Current learning rate is 0.009109127345600293. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 5376/60000][Iteration 491][Wall Clock 47.039860128s] Trained 256 records in 0.080852063 seconds. Throughput is 3166.2766 records/second. Loss is 1.0561681. Sequentialfabab260's hyper parameters: Current learning rate is 0.009107468123861566. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 5632/60000][Iteration 492][Wall Clock 47.105683746s] Trained 256 records in 0.065823618 seconds. Throughput is 3889.1816 records/second. Loss is 1.0603871. Sequentialfabab260's hyper parameters: Current learning rate is 0.009105809506465124. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:46 INFO  DistriOptimizer$:408 - [Epoch 3 5888/60000][Iteration 493][Wall Clock 47.180788948s] Trained 256 records in 0.075105202 seconds. Throughput is 3408.552 records/second. Loss is 1.084131. Sequentialfabab260's hyper parameters: Current learning rate is 0.009104151493080845. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:47 INFO  DistriOptimizer$:408 - [Epoch 3 6144/60000][Iteration 494][Wall Clock 47.251357522s] Trained 256 records in 0.070568574 seconds. Throughput is 3627.677 records/second. Loss is 0.963757. Sequentialfabab260's hyper parameters: Current learning rate is 0.009102494083378846. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:47 INFO  DistriOptimizer$:408 - [Epoch 3 6400/60000][Iteration 495][Wall Clock 47.329726604s] Trained 256 records in 0.078369082 seconds. Throughput is 3266.5945 records/second. Loss is 1.0769384. Sequentialfabab260's hyper parameters: Current learning rate is 0.009100837277029487. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:47 INFO  DistriOptimizer$:408 - [Epoch 3 6656/60000][Iteration 496][Wall Clock 47.412736761s] Trained 256 records in 0.083010157 seconds. Throughput is 3083.96 records/second. Loss is 1.07426. Sequentialfabab260's hyper parameters: Current learning rate is 0.009099181073703368. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:47 INFO  DistriOptimizer$:408 - [Epoch 3 6912/60000][Iteration 497][Wall Clock 47.513600048s] Trained 256 records in 0.100863287 seconds. Throughput is 2538.089 records/second. Loss is 0.94559133. Sequentialfabab260's hyper parameters: Current learning rate is 0.009097525473071326. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:47 INFO  DistriOptimizer$:408 - [Epoch 3 7168/60000][Iteration 498][Wall Clock 47.600088441s] Trained 256 records in 0.086488393 seconds. Throughput is 2959.9346 records/second. Loss is 1.027039. Sequentialfabab260's hyper parameters: Current learning rate is 0.00909587047480444. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:47 INFO  DistriOptimizer$:408 - [Epoch 3 7424/60000][Iteration 499][Wall Clock 47.697390079s] Trained 256 records in 0.097301638 seconds. Throughput is 2630.9937 records/second. Loss is 1.0059088. Sequentialfabab260's hyper parameters: Current learning rate is 0.009094216078574028. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:47 INFO  DistriOptimizer$:408 - [Epoch 3 7680/60000][Iteration 500][Wall Clock 47.784214335s] Trained 256 records in 0.086824256 seconds. Throughput is 2948.4849 records/second. Loss is 1.113368. Sequentialfabab260's hyper parameters: Current learning rate is 0.009092562284051645. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:47 INFO  DistriOptimizer$:408 - [Epoch 3 7936/60000][Iteration 501][Wall Clock 47.855072374s] Trained 256 records in 0.070858039 seconds. Throughput is 3612.8574 records/second. Loss is 1.0067946. Sequentialfabab260's hyper parameters: Current learning rate is 0.00909090909090909. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:47 INFO  DistriOptimizer$:408 - [Epoch 3 8192/60000][Iteration 502][Wall Clock 47.930884311s] Trained 256 records in 0.075811937 seconds. Throughput is 3376.7769 records/second. Loss is 0.98624915. Sequentialfabab260's hyper parameters: Current learning rate is 0.009089256498818397. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:47 INFO  DistriOptimizer$:408 - [Epoch 3 8448/60000][Iteration 503][Wall Clock 47.999901214s] Trained 256 records in 0.069016903 seconds. Throughput is 3709.2363 records/second. Loss is 1.0608402. Sequentialfabab260's hyper parameters: Current learning rate is 0.009087604507451835. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:47 INFO  DistriOptimizer$:408 - [Epoch 3 8704/60000][Iteration 504][Wall Clock 48.086271762s] Trained 256 records in 0.086370548 seconds. Throughput is 2963.9731 records/second. Loss is 1.0043043. Sequentialfabab260's hyper parameters: Current learning rate is 0.009085953116481919. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:47 INFO  DistriOptimizer$:408 - [Epoch 3 8960/60000][Iteration 505][Wall Clock 48.166740369s] Trained 256 records in 0.080468607 seconds. Throughput is 3181.3647 records/second. Loss is 1.0238998. Sequentialfabab260's hyper parameters: Current learning rate is 0.009084302325581396. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:48 INFO  DistriOptimizer$:408 - [Epoch 3 9216/60000][Iteration 506][Wall Clock 48.254766125s] Trained 256 records in 0.088025756 seconds. Throughput is 2908.2397 records/second. Loss is 1.0126994. Sequentialfabab260's hyper parameters: Current learning rate is 0.009082652134423252. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:48 INFO  DistriOptimizer$:408 - [Epoch 3 9472/60000][Iteration 507][Wall Clock 48.333257193s] Trained 256 records in 0.078491068 seconds. Throughput is 3261.5176 records/second. Loss is 1.0082232. Sequentialfabab260's hyper parameters: Current learning rate is 0.009081002542680712. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:48 INFO  DistriOptimizer$:408 - [Epoch 3 9728/60000][Iteration 508][Wall Clock 48.410515158s] Trained 256 records in 0.077257965 seconds. Throughput is 3313.574 records/second. Loss is 1.0212489. Sequentialfabab260's hyper parameters: Current learning rate is 0.009079353550027239. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:48 INFO  DistriOptimizer$:408 - [Epoch 3 9984/60000][Iteration 509][Wall Clock 48.48902969s] Trained 256 records in 0.078514532 seconds. Throughput is 3260.543 records/second. Loss is 0.97840106. Sequentialfabab260's hyper parameters: Current learning rate is 0.00907770515613653. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:48 INFO  DistriOptimizer$:408 - [Epoch 3 10240/60000][Iteration 510][Wall Clock 48.567127219s] Trained 256 records in 0.078097529 seconds. Throughput is 3277.9526 records/second. Loss is 1.0209033. Sequentialfabab260's hyper parameters: Current learning rate is 0.00907605736068252. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:48 INFO  DistriOptimizer$:408 - [Epoch 3 10496/60000][Iteration 511][Wall Clock 48.639392168s] Trained 256 records in 0.072264949 seconds. Throughput is 3542.5198 records/second. Loss is 1.002973. Sequentialfabab260's hyper parameters: Current learning rate is 0.009074410163339382. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:48 INFO  DistriOptimizer$:408 - [Epoch 3 10752/60000][Iteration 512][Wall Clock 48.7169515s] Trained 256 records in 0.077559332 seconds. Throughput is 3300.699 records/second. Loss is 0.97681326. Sequentialfabab260's hyper parameters: Current learning rate is 0.009072763563781528. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:48 INFO  DistriOptimizer$:408 - [Epoch 3 11008/60000][Iteration 513][Wall Clock 48.794779565s] Trained 256 records in 0.077828065 seconds. Throughput is 3289.302 records/second. Loss is 0.9806877. Sequentialfabab260's hyper parameters: Current learning rate is 0.009071117561683599. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:48 INFO  DistriOptimizer$:408 - [Epoch 3 11264/60000][Iteration 514][Wall Clock 48.875889245s] Trained 256 records in 0.08110968 seconds. Throughput is 3156.22 records/second. Loss is 1.0368725. Sequentialfabab260's hyper parameters: Current learning rate is 0.009069472156720479. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:48 INFO  DistriOptimizer$:408 - [Epoch 3 11520/60000][Iteration 515][Wall Clock 48.961983465s] Trained 256 records in 0.08609422 seconds. Throughput is 2973.4863 records/second. Loss is 1.0393587. Sequentialfabab260's hyper parameters: Current learning rate is 0.009067827348567283. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:48 INFO  DistriOptimizer$:408 - [Epoch 3 11776/60000][Iteration 516][Wall Clock 49.03673952s] Trained 256 records in 0.074756055 seconds. Throughput is 3424.4717 records/second. Loss is 0.98426163. Sequentialfabab260's hyper parameters: Current learning rate is 0.009066183136899365. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:48 INFO  DistriOptimizer$:408 - [Epoch 3 12032/60000][Iteration 517][Wall Clock 49.114623577s] Trained 256 records in 0.077884057 seconds. Throughput is 3286.9373 records/second. Loss is 0.9776677. Sequentialfabab260's hyper parameters: Current learning rate is 0.009064539521392313. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:49 INFO  DistriOptimizer$:408 - [Epoch 3 12288/60000][Iteration 518][Wall Clock 49.184751329s] Trained 256 records in 0.070127752 seconds. Throughput is 3650.4805 records/second. Loss is 1.0048347. Sequentialfabab260's hyper parameters: Current learning rate is 0.009062896501721951. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:49 INFO  DistriOptimizer$:408 - [Epoch 3 12544/60000][Iteration 519][Wall Clock 49.268736806s] Trained 256 records in 0.083985477 seconds. Throughput is 3048.146 records/second. Loss is 0.99899673. Sequentialfabab260's hyper parameters: Current learning rate is 0.009061254077564336. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:49 INFO  DistriOptimizer$:408 - [Epoch 3 12800/60000][Iteration 520][Wall Clock 49.360475513s] Trained 256 records in 0.091738707 seconds. Throughput is 2790.5342 records/second. Loss is 1.0511187. Sequentialfabab260's hyper parameters: Current learning rate is 0.00905961224859576. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:49 INFO  DistriOptimizer$:408 - [Epoch 3 13056/60000][Iteration 521][Wall Clock 49.451921325s] Trained 256 records in 0.091445812 seconds. Throughput is 2799.4722 records/second. Loss is 0.9198914. Sequentialfabab260's hyper parameters: Current learning rate is 0.009057971014492752. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:49 INFO  DistriOptimizer$:408 - [Epoch 3 13312/60000][Iteration 522][Wall Clock 49.543343459s] Trained 256 records in 0.091422134 seconds. Throughput is 2800.1973 records/second. Loss is 1.0228168. Sequentialfabab260's hyper parameters: Current learning rate is 0.009056330374932076. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:49 INFO  DistriOptimizer$:408 - [Epoch 3 13568/60000][Iteration 523][Wall Clock 49.634915397s] Trained 256 records in 0.091571938 seconds. Throughput is 2795.6165 records/second. Loss is 1.0062842. Sequentialfabab260's hyper parameters: Current learning rate is 0.009054690329590729. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:49 INFO  DistriOptimizer$:408 - [Epoch 3 13824/60000][Iteration 524][Wall Clock 49.732158493s] Trained 256 records in 0.097243096 seconds. Throughput is 2632.5776 records/second. Loss is 0.90142286. Sequentialfabab260's hyper parameters: Current learning rate is 0.009053050878145934. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:49 INFO  DistriOptimizer$:408 - [Epoch 3 14080/60000][Iteration 525][Wall Clock 49.806046437s] Trained 256 records in 0.073887944 seconds. Throughput is 3464.706 records/second. Loss is 0.97269094. Sequentialfabab260's hyper parameters: Current learning rate is 0.009051412020275163. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:49 INFO  DistriOptimizer$:408 - [Epoch 3 14336/60000][Iteration 526][Wall Clock 49.873167101s] Trained 256 records in 0.067120664 seconds. Throughput is 3814.0266 records/second. Loss is 0.9482418. Sequentialfabab260's hyper parameters: Current learning rate is 0.00904977375565611. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:49 INFO  DistriOptimizer$:408 - [Epoch 3 14592/60000][Iteration 527][Wall Clock 49.955772998s] Trained 256 records in 0.082605897 seconds. Throughput is 3099.0522 records/second. Loss is 0.944786. Sequentialfabab260's hyper parameters: Current learning rate is 0.009048136083966703. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:49 INFO  DistriOptimizer$:408 - [Epoch 3 14848/60000][Iteration 528][Wall Clock 50.036274968s] Trained 256 records in 0.08050197 seconds. Throughput is 3180.0461 records/second. Loss is 0.9847702. Sequentialfabab260's hyper parameters: Current learning rate is 0.009046499004885111. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:49 INFO  DistriOptimizer$:408 - [Epoch 3 15104/60000][Iteration 529][Wall Clock 50.114228399s] Trained 256 records in 0.077953431 seconds. Throughput is 3284.0122 records/second. Loss is 1.0354908. Sequentialfabab260's hyper parameters: Current learning rate is 0.009044862518089726. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 15360/60000][Iteration 530][Wall Clock 50.195723962s] Trained 256 records in 0.081495563 seconds. Throughput is 3141.2754 records/second. Loss is 1.0515168. Sequentialfabab260's hyper parameters: Current learning rate is 0.00904322662325918. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 15616/60000][Iteration 531][Wall Clock 50.267092646s] Trained 256 records in 0.071368684 seconds. Throughput is 3587.0073 records/second. Loss is 0.91244966. Sequentialfabab260's hyper parameters: Current learning rate is 0.009041591320072331. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 15872/60000][Iteration 532][Wall Clock 50.340311716s] Trained 256 records in 0.07321907 seconds. Throughput is 3496.357 records/second. Loss is 0.9493032. Sequentialfabab260's hyper parameters: Current learning rate is 0.00903995660820828. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 16128/60000][Iteration 533][Wall Clock 50.418101024s] Trained 256 records in 0.077789308 seconds. Throughput is 3290.9407 records/second. Loss is 1.0598985. Sequentialfabab260's hyper parameters: Current learning rate is 0.009038322487346349. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 16384/60000][Iteration 534][Wall Clock 50.497024276s] Trained 256 records in 0.078923252 seconds. Throughput is 3243.6575 records/second. Loss is 1.0082139. Sequentialfabab260's hyper parameters: Current learning rate is 0.009036688957166094. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 16640/60000][Iteration 535][Wall Clock 50.573928726s] Trained 256 records in 0.07690445 seconds. Throughput is 3328.806 records/second. Loss is 0.987483. Sequentialfabab260's hyper parameters: Current learning rate is 0.009035056017347307. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 16896/60000][Iteration 536][Wall Clock 50.653238522s] Trained 256 records in 0.079309796 seconds. Throughput is 3227.8484 records/second. Loss is 0.9447853. Sequentialfabab260's hyper parameters: Current learning rate is 0.00903342366757001. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 17152/60000][Iteration 537][Wall Clock 50.73134245s] Trained 256 records in 0.078103928 seconds. Throughput is 3277.684 records/second. Loss is 0.9397789. Sequentialfabab260's hyper parameters: Current learning rate is 0.009031791907514452. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 17408/60000][Iteration 538][Wall Clock 50.813954633s] Trained 256 records in 0.082612183 seconds. Throughput is 3098.8164 records/second. Loss is 0.93596107. Sequentialfabab260's hyper parameters: Current learning rate is 0.009030160736861116. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 17664/60000][Iteration 539][Wall Clock 50.890271551s] Trained 256 records in 0.076316918 seconds. Throughput is 3354.4333 records/second. Loss is 1.0151335. Sequentialfabab260's hyper parameters: Current learning rate is 0.00902853015529072. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 17920/60000][Iteration 540][Wall Clock 50.971417436s] Trained 256 records in 0.081145885 seconds. Throughput is 3154.812 records/second. Loss is 0.9345035. Sequentialfabab260's hyper parameters: Current learning rate is 0.009026900162484202. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 18176/60000][Iteration 541][Wall Clock 51.04060567s] Trained 256 records in 0.069188234 seconds. Throughput is 3700.051 records/second. Loss is 1.0102901. Sequentialfabab260's hyper parameters: Current learning rate is 0.009025270758122744. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:50 INFO  DistriOptimizer$:408 - [Epoch 3 18432/60000][Iteration 542][Wall Clock 51.111811907s] Trained 256 records in 0.071206237 seconds. Throughput is 3595.191 records/second. Loss is 0.92524886. Sequentialfabab260's hyper parameters: Current learning rate is 0.009023641941887746. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 18688/60000][Iteration 543][Wall Clock 51.189141513s] Trained 256 records in 0.077329606 seconds. Throughput is 3310.5044 records/second. Loss is 0.9680442. Sequentialfabab260's hyper parameters: Current learning rate is 0.009022013713460845. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 18944/60000][Iteration 544][Wall Clock 51.265881599s] Trained 256 records in 0.076740086 seconds. Throughput is 3335.9358 records/second. Loss is 0.9467826. Sequentialfabab260's hyper parameters: Current learning rate is 0.009020386072523904. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 19200/60000][Iteration 545][Wall Clock 51.357415143s] Trained 256 records in 0.091533544 seconds. Throughput is 2796.789 records/second. Loss is 0.90811396. Sequentialfabab260's hyper parameters: Current learning rate is 0.009018759018759018. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 19456/60000][Iteration 546][Wall Clock 51.439593297s] Trained 256 records in 0.082178154 seconds. Throughput is 3115.183 records/second. Loss is 0.9468841. Sequentialfabab260's hyper parameters: Current learning rate is 0.009017132551848512. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 19712/60000][Iteration 547][Wall Clock 51.540294891s] Trained 256 records in 0.100701594 seconds. Throughput is 2542.1643 records/second. Loss is 0.9520832. Sequentialfabab260's hyper parameters: Current learning rate is 0.009015506671474938. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 19968/60000][Iteration 548][Wall Clock 51.623174485s] Trained 256 records in 0.082879594 seconds. Throughput is 3088.818 records/second. Loss is 0.8951718. Sequentialfabab260's hyper parameters: Current learning rate is 0.009013881377321075. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 20224/60000][Iteration 549][Wall Clock 51.699276679s] Trained 256 records in 0.076102194 seconds. Throughput is 3363.8977 records/second. Loss is 0.9484002. Sequentialfabab260's hyper parameters: Current learning rate is 0.009012256669069936. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 20480/60000][Iteration 550][Wall Clock 51.768771724s] Trained 256 records in 0.069495045 seconds. Throughput is 3683.7158 records/second. Loss is 0.9514862. Sequentialfabab260's hyper parameters: Current learning rate is 0.00901063254640476. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 20736/60000][Iteration 551][Wall Clock 51.843348237s] Trained 256 records in 0.074576513 seconds. Throughput is 3432.7163 records/second. Loss is 0.91900796. Sequentialfabab260's hyper parameters: Current learning rate is 0.009009009009009009. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 20992/60000][Iteration 552][Wall Clock 51.922347463s] Trained 256 records in 0.078999226 seconds. Throughput is 3240.538 records/second. Loss is 0.9102297. Sequentialfabab260's hyper parameters: Current learning rate is 0.009007386056566384. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 21248/60000][Iteration 553][Wall Clock 51.995058822s] Trained 256 records in 0.072711359 seconds. Throughput is 3520.7705 records/second. Loss is 0.98510206. Sequentialfabab260's hyper parameters: Current learning rate is 0.009005763688760807. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 21504/60000][Iteration 554][Wall Clock 52.066596389s] Trained 256 records in 0.071537567 seconds. Throughput is 3578.5393 records/second. Loss is 0.8607517. Sequentialfabab260's hyper parameters: Current learning rate is 0.009004141905276427. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:51 INFO  DistriOptimizer$:408 - [Epoch 3 21760/60000][Iteration 555][Wall Clock 52.139608513s] Trained 256 records in 0.073012124 seconds. Throughput is 3506.267 records/second. Loss is 0.92579216. Sequentialfabab260's hyper parameters: Current learning rate is 0.009002520705797623. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 22016/60000][Iteration 556][Wall Clock 52.214939818s] Trained 256 records in 0.075331305 seconds. Throughput is 3398.3215 records/second. Loss is 0.9042146. Sequentialfabab260's hyper parameters: Current learning rate is 0.009000900090009001. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 22272/60000][Iteration 557][Wall Clock 52.292242134s] Trained 256 records in 0.077302316 seconds. Throughput is 3311.673 records/second. Loss is 0.85054314. Sequentialfabab260's hyper parameters: Current learning rate is 0.008999280057595392. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 22528/60000][Iteration 558][Wall Clock 52.36899992s] Trained 256 records in 0.076757786 seconds. Throughput is 3335.1665 records/second. Loss is 0.90284437. Sequentialfabab260's hyper parameters: Current learning rate is 0.008997660608241857. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 22784/60000][Iteration 559][Wall Clock 52.435305735s] Trained 256 records in 0.066305815 seconds. Throughput is 3860.8982 records/second. Loss is 0.946072. Sequentialfabab260's hyper parameters: Current learning rate is 0.008996041741633681. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 23040/60000][Iteration 560][Wall Clock 52.505883205s] Trained 256 records in 0.07057747 seconds. Throughput is 3627.2197 records/second. Loss is 0.85955286. Sequentialfabab260's hyper parameters: Current learning rate is 0.008994423457456376. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 23296/60000][Iteration 561][Wall Clock 52.577716112s] Trained 256 records in 0.071832907 seconds. Throughput is 3563.8262 records/second. Loss is 0.9319813. Sequentialfabab260's hyper parameters: Current learning rate is 0.008992805755395683. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 23552/60000][Iteration 562][Wall Clock 52.65327235s] Trained 256 records in 0.075556238 seconds. Throughput is 3388.2046 records/second. Loss is 0.87467605. Sequentialfabab260's hyper parameters: Current learning rate is 0.008991188635137565. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 23808/60000][Iteration 563][Wall Clock 52.722839855s] Trained 256 records in 0.069567505 seconds. Throughput is 3679.8792 records/second. Loss is 0.91240287. Sequentialfabab260's hyper parameters: Current learning rate is 0.008989572096368213. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 24064/60000][Iteration 564][Wall Clock 52.798312246s] Trained 256 records in 0.075472391 seconds. Throughput is 3391.9688 records/second. Loss is 0.94776076. Sequentialfabab260's hyper parameters: Current learning rate is 0.008987956138774043. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 24320/60000][Iteration 565][Wall Clock 52.869570847s] Trained 256 records in 0.071258601 seconds. Throughput is 3592.5486 records/second. Loss is 0.8545631. Sequentialfabab260's hyper parameters: Current learning rate is 0.008986340762041696. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 24576/60000][Iteration 566][Wall Clock 52.953970132s] Trained 256 records in 0.084399285 seconds. Throughput is 3033.2012 records/second. Loss is 0.85325307. Sequentialfabab260's hyper parameters: Current learning rate is 0.008984725965858042. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 24832/60000][Iteration 567][Wall Clock 53.042436693s] Trained 256 records in 0.088466561 seconds. Throughput is 2893.7488 records/second. Loss is 0.89978796. Sequentialfabab260's hyper parameters: Current learning rate is 0.008983111749910169. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:52 INFO  DistriOptimizer$:408 - [Epoch 3 25088/60000][Iteration 568][Wall Clock 53.123771761s] Trained 256 records in 0.081335068 seconds. Throughput is 3147.4739 records/second. Loss is 0.88041437. Sequentialfabab260's hyper parameters: Current learning rate is 0.008981498113885397. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 25344/60000][Iteration 569][Wall Clock 53.191826998s] Trained 256 records in 0.068055237 seconds. Throughput is 3761.6504 records/second. Loss is 0.9284997. Sequentialfabab260's hyper parameters: Current learning rate is 0.008979885057471266. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 25600/60000][Iteration 570][Wall Clock 53.26959316s] Trained 256 records in 0.077766162 seconds. Throughput is 3291.9202 records/second. Loss is 0.92978173. Sequentialfabab260's hyper parameters: Current learning rate is 0.00897827258035554. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 25856/60000][Iteration 571][Wall Clock 53.352214926s] Trained 256 records in 0.082621766 seconds. Throughput is 3098.457 records/second. Loss is 0.9343691. Sequentialfabab260's hyper parameters: Current learning rate is 0.00897666068222621. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 26112/60000][Iteration 572][Wall Clock 53.439935833s] Trained 256 records in 0.087720907 seconds. Throughput is 2918.3464 records/second. Loss is 0.8609945. Sequentialfabab260's hyper parameters: Current learning rate is 0.008975049362771494. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 26368/60000][Iteration 573][Wall Clock 53.522424285s] Trained 256 records in 0.082488452 seconds. Throughput is 3103.4646 records/second. Loss is 0.87752783. Sequentialfabab260's hyper parameters: Current learning rate is 0.008973438621679828. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 26624/60000][Iteration 574][Wall Clock 53.607834201s] Trained 256 records in 0.085409916 seconds. Throughput is 2997.31 records/second. Loss is 0.89074993. Sequentialfabab260's hyper parameters: Current learning rate is 0.00897182845863987. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 26880/60000][Iteration 575][Wall Clock 53.694366138s] Trained 256 records in 0.086531937 seconds. Throughput is 2958.4453 records/second. Loss is 0.9341476. Sequentialfabab260's hyper parameters: Current learning rate is 0.00897021887334051. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 27136/60000][Iteration 576][Wall Clock 53.774304694s] Trained 256 records in 0.079938556 seconds. Throughput is 3202.4597 records/second. Loss is 0.8874629. Sequentialfabab260's hyper parameters: Current learning rate is 0.008968609865470852. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 27392/60000][Iteration 577][Wall Clock 53.840668211s] Trained 256 records in 0.066363517 seconds. Throughput is 3857.5415 records/second. Loss is 0.8131061. Sequentialfabab260's hyper parameters: Current learning rate is 0.00896700143472023. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 27648/60000][Iteration 578][Wall Clock 53.915340306s] Trained 256 records in 0.074672095 seconds. Throughput is 3428.3223 records/second. Loss is 0.9129692. Sequentialfabab260's hyper parameters: Current learning rate is 0.008965393580778197. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 27904/60000][Iteration 579][Wall Clock 53.99180877s] Trained 256 records in 0.076468464 seconds. Throughput is 3347.7852 records/second. Loss is 0.867631. Sequentialfabab260's hyper parameters: Current learning rate is 0.008963786303334529. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 28160/60000][Iteration 580][Wall Clock 54.07113767s] Trained 256 records in 0.0793289 seconds. Throughput is 3227.071 records/second. Loss is 0.7730137. Sequentialfabab260's hyper parameters: Current learning rate is 0.008962179602079227. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:53 INFO  DistriOptimizer$:408 - [Epoch 3 28416/60000][Iteration 581][Wall Clock 54.152246073s] Trained 256 records in 0.081108403 seconds. Throughput is 3156.2695 records/second. Loss is 0.89704764. Sequentialfabab260's hyper parameters: Current learning rate is 0.008960573476702509. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:54 INFO  DistriOptimizer$:408 - [Epoch 3 28672/60000][Iteration 582][Wall Clock 54.229089985s] Trained 256 records in 0.076843912 seconds. Throughput is 3331.4287 records/second. Loss is 0.80909574. Sequentialfabab260's hyper parameters: Current learning rate is 0.008958967926894821. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:54 INFO  DistriOptimizer$:408 - [Epoch 3 28928/60000][Iteration 583][Wall Clock 54.312889585s] Trained 256 records in 0.0837996 seconds. Throughput is 3054.9072 records/second. Loss is 0.94280845. Sequentialfabab260's hyper parameters: Current learning rate is 0.008957362952346828. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:54 INFO  DistriOptimizer$:408 - [Epoch 3 29184/60000][Iteration 584][Wall Clock 54.396995866s] Trained 256 records in 0.084106281 seconds. Throughput is 3043.7678 records/second. Loss is 0.8978766. Sequentialfabab260's hyper parameters: Current learning rate is 0.008955758552749419. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:54 INFO  DistriOptimizer$:408 - [Epoch 3 29440/60000][Iteration 585][Wall Clock 54.467492727s] Trained 256 records in 0.070496861 seconds. Throughput is 3631.3672 records/second. Loss is 0.9209735. Sequentialfabab260's hyper parameters: Current learning rate is 0.008954154727793696. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:54 INFO  DistriOptimizer$:408 - [Epoch 3 29696/60000][Iteration 586][Wall Clock 54.545806229s] Trained 256 records in 0.078313502 seconds. Throughput is 3268.9128 records/second. Loss is 0.8642283. Sequentialfabab260's hyper parameters: Current learning rate is 0.008952551477170993. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:54 INFO  DistriOptimizer$:408 - [Epoch 3 29952/60000][Iteration 587][Wall Clock 54.621442904s] Trained 256 records in 0.075636675 seconds. Throughput is 3384.6013 records/second. Loss is 0.8496009. Sequentialfabab260's hyper parameters: Current learning rate is 0.008950948800572862. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:54 INFO  DistriOptimizer$:408 - [Epoch 3 30208/60000][Iteration 588][Wall Clock 54.703998955s] Trained 256 records in 0.082556051 seconds. Throughput is 3100.9233 records/second. Loss is 0.83149576. Sequentialfabab260's hyper parameters: Current learning rate is 0.008949346697691068. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:54 INFO  DistriOptimizer$:408 - [Epoch 3 30464/60000][Iteration 589][Wall Clock 54.785807566s] Trained 256 records in 0.081808611 seconds. Throughput is 3129.255 records/second. Loss is 0.79219186. Sequentialfabab260's hyper parameters: Current learning rate is 0.00894774516821761. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:54 INFO  DistriOptimizer$:408 - [Epoch 3 30720/60000][Iteration 590][Wall Clock 54.869241827s] Trained 256 records in 0.083434261 seconds. Throughput is 3068.284 records/second. Loss is 0.85777575. Sequentialfabab260's hyper parameters: Current learning rate is 0.008946144211844696. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:54 INFO  DistriOptimizer$:408 - [Epoch 3 30976/60000][Iteration 591][Wall Clock 54.957683805s] Trained 256 records in 0.088441978 seconds. Throughput is 2894.5532 records/second. Loss is 0.8289037. Sequentialfabab260's hyper parameters: Current learning rate is 0.008944543828264758. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:54 INFO  DistriOptimizer$:408 - [Epoch 3 31232/60000][Iteration 592][Wall Clock 55.035178328s] Trained 256 records in 0.077494523 seconds. Throughput is 3303.4592 records/second. Loss is 0.83626676. Sequentialfabab260's hyper parameters: Current learning rate is 0.008942944017170452. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:54 INFO  DistriOptimizer$:408 - [Epoch 3 31488/60000][Iteration 593][Wall Clock 55.113295282s] Trained 256 records in 0.078116954 seconds. Throughput is 3277.1375 records/second. Loss is 0.9242399. Sequentialfabab260's hyper parameters: Current learning rate is 0.008941344778254649. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 31744/60000][Iteration 594][Wall Clock 55.188517884s] Trained 256 records in 0.075222602 seconds. Throughput is 3403.2324 records/second. Loss is 0.859401. Sequentialfabab260's hyper parameters: Current learning rate is 0.008939746111210442. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 32000/60000][Iteration 595][Wall Clock 55.263409598s] Trained 256 records in 0.074891714 seconds. Throughput is 3418.2686 records/second. Loss is 0.920801. Sequentialfabab260's hyper parameters: Current learning rate is 0.00893814801573114. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 32256/60000][Iteration 596][Wall Clock 55.349210617s] Trained 256 records in 0.085801019 seconds. Throughput is 2983.6475 records/second. Loss is 0.94814754. Sequentialfabab260's hyper parameters: Current learning rate is 0.008936550491510277. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 32512/60000][Iteration 597][Wall Clock 55.438005767s] Trained 256 records in 0.08879515 seconds. Throughput is 2883.0405 records/second. Loss is 0.90365916. Sequentialfabab260's hyper parameters: Current learning rate is 0.008934953538241601. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 32768/60000][Iteration 598][Wall Clock 55.527010006s] Trained 256 records in 0.089004239 seconds. Throughput is 2876.2673 records/second. Loss is 0.874032. Sequentialfabab260's hyper parameters: Current learning rate is 0.008933357155619083. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 33024/60000][Iteration 599][Wall Clock 55.605833463s] Trained 256 records in 0.078823457 seconds. Throughput is 3247.7642 records/second. Loss is 0.8296065. Sequentialfabab260's hyper parameters: Current learning rate is 0.008931761343336907. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 33280/60000][Iteration 600][Wall Clock 55.704073616s] Trained 256 records in 0.098240153 seconds. Throughput is 2605.8591 records/second. Loss is 0.95476687. Sequentialfabab260's hyper parameters: Current learning rate is 0.00893016610108948. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 33536/60000][Iteration 601][Wall Clock 55.78292358s] Trained 256 records in 0.078849964 seconds. Throughput is 3246.6724 records/second. Loss is 0.88412476. Sequentialfabab260's hyper parameters: Current learning rate is 0.008928571428571428. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 33792/60000][Iteration 602][Wall Clock 55.849771886s] Trained 256 records in 0.066848306 seconds. Throughput is 3829.566 records/second. Loss is 0.80755186. Sequentialfabab260's hyper parameters: Current learning rate is 0.008926977325477594. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 34048/60000][Iteration 603][Wall Clock 55.917808867s] Trained 256 records in 0.068036981 seconds. Throughput is 3762.6597 records/second. Loss is 0.86643606. Sequentialfabab260's hyper parameters: Current learning rate is 0.008925383791503034. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 34304/60000][Iteration 604][Wall Clock 55.985462545s] Trained 256 records in 0.067653678 seconds. Throughput is 3783.9775 records/second. Loss is 0.90438515. Sequentialfabab260's hyper parameters: Current learning rate is 0.00892379082634303. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 34560/60000][Iteration 605][Wall Clock 56.062684957s] Trained 256 records in 0.077222412 seconds. Throughput is 3315.0996 records/second. Loss is 0.81775784. Sequentialfabab260's hyper parameters: Current learning rate is 0.008922198429693077. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:55 INFO  DistriOptimizer$:408 - [Epoch 3 34816/60000][Iteration 606][Wall Clock 56.142505133s] Trained 256 records in 0.079820176 seconds. Throughput is 3207.209 records/second. Loss is 0.92008346. Sequentialfabab260's hyper parameters: Current learning rate is 0.008920606601248885. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 35072/60000][Iteration 607][Wall Clock 56.210763657s] Trained 256 records in 0.068258524 seconds. Throughput is 3750.4473 records/second. Loss is 0.81440455. Sequentialfabab260's hyper parameters: Current learning rate is 0.008919015340706386. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 35328/60000][Iteration 608][Wall Clock 56.285854181s] Trained 256 records in 0.075090524 seconds. Throughput is 3409.2183 records/second. Loss is 0.82781607. Sequentialfabab260's hyper parameters: Current learning rate is 0.008917424647761726. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 35584/60000][Iteration 609][Wall Clock 56.356986813s] Trained 256 records in 0.071132632 seconds. Throughput is 3598.911 records/second. Loss is 0.905394. Sequentialfabab260's hyper parameters: Current learning rate is 0.00891583452211127. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 35840/60000][Iteration 610][Wall Clock 56.44576881s] Trained 256 records in 0.088781997 seconds. Throughput is 2883.4675 records/second. Loss is 0.889213. Sequentialfabab260's hyper parameters: Current learning rate is 0.008914244963451596. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 36096/60000][Iteration 611][Wall Clock 56.521453915s] Trained 256 records in 0.075685105 seconds. Throughput is 3382.4355 records/second. Loss is 0.83186144. Sequentialfabab260's hyper parameters: Current learning rate is 0.0089126559714795. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 36352/60000][Iteration 612][Wall Clock 56.598652145s] Trained 256 records in 0.07719823 seconds. Throughput is 3316.1382 records/second. Loss is 0.83643824. Sequentialfabab260's hyper parameters: Current learning rate is 0.008911067545891998. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 36608/60000][Iteration 613][Wall Clock 56.677100853s] Trained 256 records in 0.078448708 seconds. Throughput is 3263.2788 records/second. Loss is 0.81919616. Sequentialfabab260's hyper parameters: Current learning rate is 0.008909479686386316. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 36864/60000][Iteration 614][Wall Clock 56.756816255s] Trained 256 records in 0.079715402 seconds. Throughput is 3211.4246 records/second. Loss is 0.7866249. Sequentialfabab260's hyper parameters: Current learning rate is 0.008907892392659897. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 37120/60000][Iteration 615][Wall Clock 56.847721122s] Trained 256 records in 0.090904867 seconds. Throughput is 2816.1309 records/second. Loss is 0.85974056. Sequentialfabab260's hyper parameters: Current learning rate is 0.008906305664410403. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 37376/60000][Iteration 616][Wall Clock 56.923654866s] Trained 256 records in 0.075933744 seconds. Throughput is 3371.36 records/second. Loss is 0.88119096. Sequentialfabab260's hyper parameters: Current learning rate is 0.008904719501335707. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 37632/60000][Iteration 617][Wall Clock 56.99932967s] Trained 256 records in 0.075674804 seconds. Throughput is 3382.8962 records/second. Loss is 0.82021856. Sequentialfabab260's hyper parameters: Current learning rate is 0.008903133903133903. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 37888/60000][Iteration 618][Wall Clock 57.07195156s] Trained 256 records in 0.07262189 seconds. Throughput is 3525.108 records/second. Loss is 0.84722596. Sequentialfabab260's hyper parameters: Current learning rate is 0.008901548869503294. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:56 INFO  DistriOptimizer$:408 - [Epoch 3 38144/60000][Iteration 619][Wall Clock 57.143549224s] Trained 256 records in 0.071597664 seconds. Throughput is 3575.5356 records/second. Loss is 0.86669064. Sequentialfabab260's hyper parameters: Current learning rate is 0.0088999644001424. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 38400/60000][Iteration 620][Wall Clock 57.232323463s] Trained 256 records in 0.088774239 seconds. Throughput is 2883.7195 records/second. Loss is 0.8665125. Sequentialfabab260's hyper parameters: Current learning rate is 0.008898380494749957. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 38656/60000][Iteration 621][Wall Clock 57.31140348s] Trained 256 records in 0.079080017 seconds. Throughput is 3237.2275 records/second. Loss is 0.87265295. Sequentialfabab260's hyper parameters: Current learning rate is 0.00889679715302491. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 38912/60000][Iteration 622][Wall Clock 57.394614703s] Trained 256 records in 0.083211223 seconds. Throughput is 3076.508 records/second. Loss is 0.88713664. Sequentialfabab260's hyper parameters: Current learning rate is 0.008895214374666428. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 39168/60000][Iteration 623][Wall Clock 57.468578089s] Trained 256 records in 0.073963386 seconds. Throughput is 3461.1719 records/second. Loss is 0.79324216. Sequentialfabab260's hyper parameters: Current learning rate is 0.008893632159373888. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 39424/60000][Iteration 624][Wall Clock 57.540767272s] Trained 256 records in 0.072189183 seconds. Throughput is 3546.2378 records/second. Loss is 0.78328156. Sequentialfabab260's hyper parameters: Current learning rate is 0.008892050506846879. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 39680/60000][Iteration 625][Wall Clock 57.619567242s] Trained 256 records in 0.07879997 seconds. Throughput is 3248.7322 records/second. Loss is 0.86251014. Sequentialfabab260's hyper parameters: Current learning rate is 0.008890469416785207. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 39936/60000][Iteration 626][Wall Clock 57.689497568s] Trained 256 records in 0.069930326 seconds. Throughput is 3660.7869 records/second. Loss is 0.8024499. Sequentialfabab260's hyper parameters: Current learning rate is 0.008888888888888889. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 40192/60000][Iteration 627][Wall Clock 57.760027081s] Trained 256 records in 0.070529513 seconds. Throughput is 3629.6863 records/second. Loss is 0.9063779. Sequentialfabab260's hyper parameters: Current learning rate is 0.00888730892285816. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 40448/60000][Iteration 628][Wall Clock 57.835370079s] Trained 256 records in 0.075342998 seconds. Throughput is 3397.7942 records/second. Loss is 0.8702599. Sequentialfabab260's hyper parameters: Current learning rate is 0.00888572951839346. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 40704/60000][Iteration 629][Wall Clock 57.913131814s] Trained 256 records in 0.077761735 seconds. Throughput is 3292.1077 records/second. Loss is 0.8533775. Sequentialfabab260's hyper parameters: Current learning rate is 0.008884150675195452. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 40960/60000][Iteration 630][Wall Clock 57.98283406s] Trained 256 records in 0.069702246 seconds. Throughput is 3672.7654 records/second. Loss is 0.78983057. Sequentialfabab260's hyper parameters: Current learning rate is 0.008882572392965004. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 41216/60000][Iteration 631][Wall Clock 58.05232554s] Trained 256 records in 0.06949148 seconds. Throughput is 3683.9048 records/second. Loss is 0.83481145. Sequentialfabab260's hyper parameters: Current learning rate is 0.008880994671403198. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:57 INFO  DistriOptimizer$:408 - [Epoch 3 41472/60000][Iteration 632][Wall Clock 58.130418066s] Trained 256 records in 0.078092526 seconds. Throughput is 3278.1628 records/second. Loss is 0.8121297. Sequentialfabab260's hyper parameters: Current learning rate is 0.00887941751021133. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 41728/60000][Iteration 633][Wall Clock 58.201122385s] Trained 256 records in 0.070704319 seconds. Throughput is 3620.7124 records/second. Loss is 0.85072166. Sequentialfabab260's hyper parameters: Current learning rate is 0.008877840909090908. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 41984/60000][Iteration 634][Wall Clock 58.280774918s] Trained 256 records in 0.079652533 seconds. Throughput is 3213.9592 records/second. Loss is 0.7919775. Sequentialfabab260's hyper parameters: Current learning rate is 0.008876264867743653. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 42240/60000][Iteration 635][Wall Clock 58.361162749s] Trained 256 records in 0.080387831 seconds. Throughput is 3184.5615 records/second. Loss is 0.82012093. Sequentialfabab260's hyper parameters: Current learning rate is 0.008874689385871494. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 42496/60000][Iteration 636][Wall Clock 58.440318098s] Trained 256 records in 0.079155349 seconds. Throughput is 3234.1465 records/second. Loss is 0.8719075. Sequentialfabab260's hyper parameters: Current learning rate is 0.008873114463176575. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 42752/60000][Iteration 637][Wall Clock 58.516137211s] Trained 256 records in 0.075819113 seconds. Throughput is 3376.4573 records/second. Loss is 0.7933904. Sequentialfabab260's hyper parameters: Current learning rate is 0.008871540099361249. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 43008/60000][Iteration 638][Wall Clock 58.590550046s] Trained 256 records in 0.074412835 seconds. Throughput is 3440.2666 records/second. Loss is 0.83295184. Sequentialfabab260's hyper parameters: Current learning rate is 0.008869966294128083. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 43264/60000][Iteration 639][Wall Clock 58.671028819s] Trained 256 records in 0.080478773 seconds. Throughput is 3180.9631 records/second. Loss is 0.82440865. Sequentialfabab260's hyper parameters: Current learning rate is 0.008868393047179852. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 43520/60000][Iteration 640][Wall Clock 58.744059992s] Trained 256 records in 0.073031173 seconds. Throughput is 3505.3525 records/second. Loss is 0.7824891. Sequentialfabab260's hyper parameters: Current learning rate is 0.008866820358219544. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 43776/60000][Iteration 641][Wall Clock 58.814058179s] Trained 256 records in 0.069998187 seconds. Throughput is 3657.2375 records/second. Loss is 0.7819081. Sequentialfabab260's hyper parameters: Current learning rate is 0.008865248226950354. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 44032/60000][Iteration 642][Wall Clock 58.88463925s] Trained 256 records in 0.070581071 seconds. Throughput is 3627.0347 records/second. Loss is 0.76460814. Sequentialfabab260's hyper parameters: Current learning rate is 0.008863676653075695. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 44288/60000][Iteration 643][Wall Clock 58.957073633s] Trained 256 records in 0.072434383 seconds. Throughput is 3534.2334 records/second. Loss is 0.82997525. Sequentialfabab260's hyper parameters: Current learning rate is 0.008862105636299184. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 44544/60000][Iteration 644][Wall Clock 59.041878338s] Trained 256 records in 0.084804705 seconds. Throughput is 3018.7004 records/second. Loss is 0.79301214. Sequentialfabab260's hyper parameters: Current learning rate is 0.00886053517632465. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:58 INFO  DistriOptimizer$:408 - [Epoch 3 44800/60000][Iteration 645][Wall Clock 59.12852792s] Trained 256 records in 0.086649582 seconds. Throughput is 2954.4285 records/second. Loss is 0.8292842. Sequentialfabab260's hyper parameters: Current learning rate is 0.00885896527285613. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:59 INFO  DistriOptimizer$:408 - [Epoch 3 45056/60000][Iteration 646][Wall Clock 59.225247942s] Trained 256 records in 0.096720022 seconds. Throughput is 2646.815 records/second. Loss is 0.87330216. Sequentialfabab260's hyper parameters: Current learning rate is 0.008857395925597875. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:59 INFO  DistriOptimizer$:408 - [Epoch 3 45312/60000][Iteration 647][Wall Clock 59.331823627s] Trained 256 records in 0.106575685 seconds. Throughput is 2402.0488 records/second. Loss is 0.78583235. Sequentialfabab260's hyper parameters: Current learning rate is 0.00885582713425434. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:59 INFO  DistriOptimizer$:408 - [Epoch 3 45568/60000][Iteration 648][Wall Clock 59.429090753s] Trained 256 records in 0.097267126 seconds. Throughput is 2631.9272 records/second. Loss is 0.8249519. Sequentialfabab260's hyper parameters: Current learning rate is 0.008854258898530193. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:59 INFO  DistriOptimizer$:408 - [Epoch 3 45824/60000][Iteration 649][Wall Clock 59.521537908s] Trained 256 records in 0.092447155 seconds. Throughput is 2769.1497 records/second. Loss is 0.7573416. Sequentialfabab260's hyper parameters: Current learning rate is 0.008852691218130312. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:59 INFO  DistriOptimizer$:408 - [Epoch 3 46080/60000][Iteration 650][Wall Clock 59.62256998s] Trained 256 records in 0.101032072 seconds. Throughput is 2533.8489 records/second. Loss is 0.8392753. Sequentialfabab260's hyper parameters: Current learning rate is 0.008851124092759781. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:59 INFO  DistriOptimizer$:408 - [Epoch 3 46336/60000][Iteration 651][Wall Clock 59.704577933s] Trained 256 records in 0.082007953 seconds. Throughput is 3121.6484 records/second. Loss is 0.8454208. Sequentialfabab260's hyper parameters: Current learning rate is 0.008849557522123895. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:59 INFO  DistriOptimizer$:408 - [Epoch 3 46592/60000][Iteration 652][Wall Clock 59.79182609s] Trained 256 records in 0.087248157 seconds. Throughput is 2934.1594 records/second. Loss is 0.83024603. Sequentialfabab260's hyper parameters: Current learning rate is 0.008847991505928153. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:59 INFO  DistriOptimizer$:408 - [Epoch 3 46848/60000][Iteration 653][Wall Clock 59.870500608s] Trained 256 records in 0.078674518 seconds. Throughput is 3253.9126 records/second. Loss is 0.7528225. Sequentialfabab260's hyper parameters: Current learning rate is 0.008846426043878274. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:59 INFO  DistriOptimizer$:408 - [Epoch 3 47104/60000][Iteration 654][Wall Clock 59.938074044s] Trained 256 records in 0.067573436 seconds. Throughput is 3788.471 records/second. Loss is 0.79742694. Sequentialfabab260's hyper parameters: Current learning rate is 0.00884486113568017. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:59 INFO  DistriOptimizer$:408 - [Epoch 3 47360/60000][Iteration 655][Wall Clock 60.027075306s] Trained 256 records in 0.089001262 seconds. Throughput is 2876.3638 records/second. Loss is 0.7689353. Sequentialfabab260's hyper parameters: Current learning rate is 0.008843296781039971. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:21:59 INFO  DistriOptimizer$:408 - [Epoch 3 47616/60000][Iteration 656][Wall Clock 60.105990333s] Trained 256 records in 0.078915027 seconds. Throughput is 3243.9954 records/second. Loss is 0.7936754. Sequentialfabab260's hyper parameters: Current learning rate is 0.008841732979664015. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:00 INFO  DistriOptimizer$:408 - [Epoch 3 47872/60000][Iteration 657][Wall Clock 60.18872563s] Trained 256 records in 0.082735297 seconds. Throughput is 3094.2053 records/second. Loss is 0.7314215. Sequentialfabab260's hyper parameters: Current learning rate is 0.00884016973125884. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:00 INFO  DistriOptimizer$:408 - [Epoch 3 48128/60000][Iteration 658][Wall Clock 60.283489922s] Trained 256 records in 0.094764292 seconds. Throughput is 2701.4395 records/second. Loss is 0.7861107. Sequentialfabab260's hyper parameters: Current learning rate is 0.008838607035531201. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:00 INFO  DistriOptimizer$:408 - [Epoch 3 48384/60000][Iteration 659][Wall Clock 60.36813619s] Trained 256 records in 0.084646268 seconds. Throughput is 3024.3506 records/second. Loss is 0.7406382. Sequentialfabab260's hyper parameters: Current learning rate is 0.008837044892188053. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:00 INFO  DistriOptimizer$:408 - [Epoch 3 48640/60000][Iteration 660][Wall Clock 60.492878518s] Trained 256 records in 0.124742328 seconds. Throughput is 2052.2305 records/second. Loss is 0.82093483. Sequentialfabab260's hyper parameters: Current learning rate is 0.008835483300936562. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:00 INFO  DistriOptimizer$:408 - [Epoch 3 48896/60000][Iteration 661][Wall Clock 60.571079482s] Trained 256 records in 0.078200964 seconds. Throughput is 3273.617 records/second. Loss is 0.8183639. Sequentialfabab260's hyper parameters: Current learning rate is 0.008833922261484098. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:00 INFO  DistriOptimizer$:408 - [Epoch 3 49152/60000][Iteration 662][Wall Clock 60.654730028s] Trained 256 records in 0.083650546 seconds. Throughput is 3060.3506 records/second. Loss is 0.76793313. Sequentialfabab260's hyper parameters: Current learning rate is 0.008832361773538244. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:00 INFO  DistriOptimizer$:408 - [Epoch 3 49408/60000][Iteration 663][Wall Clock 60.738944408s] Trained 256 records in 0.08421438 seconds. Throughput is 3039.8608 records/second. Loss is 0.8469234. Sequentialfabab260's hyper parameters: Current learning rate is 0.008830801836806781. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:00 INFO  DistriOptimizer$:408 - [Epoch 3 49664/60000][Iteration 664][Wall Clock 60.812174796s] Trained 256 records in 0.073230388 seconds. Throughput is 3495.8167 records/second. Loss is 0.8248712. Sequentialfabab260's hyper parameters: Current learning rate is 0.008829242450997704. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:00 INFO  DistriOptimizer$:408 - [Epoch 3 49920/60000][Iteration 665][Wall Clock 60.902231907s] Trained 256 records in 0.090057111 seconds. Throughput is 2842.6406 records/second. Loss is 0.73603237. Sequentialfabab260's hyper parameters: Current learning rate is 0.00882768361581921. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:00 INFO  DistriOptimizer$:408 - [Epoch 3 50176/60000][Iteration 666][Wall Clock 60.974654632s] Trained 256 records in 0.072422725 seconds. Throughput is 3534.802 records/second. Loss is 0.7652321. Sequentialfabab260's hyper parameters: Current learning rate is 0.008826125330979701. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:00 INFO  DistriOptimizer$:408 - [Epoch 3 50432/60000][Iteration 667][Wall Clock 61.044535965s] Trained 256 records in 0.069881333 seconds. Throughput is 3663.353 records/second. Loss is 0.7584902. Sequentialfabab260's hyper parameters: Current learning rate is 0.008824567596187787. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:00 INFO  DistriOptimizer$:408 - [Epoch 3 50688/60000][Iteration 668][Wall Clock 61.125578992s] Trained 256 records in 0.081043027 seconds. Throughput is 3158.816 records/second. Loss is 0.759586. Sequentialfabab260's hyper parameters: Current learning rate is 0.008823010411152285. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:01 INFO  DistriOptimizer$:408 - [Epoch 3 50944/60000][Iteration 669][Wall Clock 61.199276753s] Trained 256 records in 0.073697761 seconds. Throughput is 3473.647 records/second. Loss is 0.7548119. Sequentialfabab260's hyper parameters: Current learning rate is 0.008821453775582216. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:01 INFO  DistriOptimizer$:408 - [Epoch 3 51200/60000][Iteration 670][Wall Clock 61.296842438s] Trained 256 records in 0.097565685 seconds. Throughput is 2623.8733 records/second. Loss is 0.76236284. Sequentialfabab260's hyper parameters: Current learning rate is 0.008819897689186807. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:01 INFO  DistriOptimizer$:408 - [Epoch 3 51456/60000][Iteration 671][Wall Clock 61.377564318s] Trained 256 records in 0.08072188 seconds. Throughput is 3171.383 records/second. Loss is 0.78958684. Sequentialfabab260's hyper parameters: Current learning rate is 0.008818342151675486. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:01 INFO  DistriOptimizer$:408 - [Epoch 3 51712/60000][Iteration 672][Wall Clock 61.482529045s] Trained 256 records in 0.104964727 seconds. Throughput is 2438.9146 records/second. Loss is 0.85149145. Sequentialfabab260's hyper parameters: Current learning rate is 0.00881678716275789. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:01 INFO  DistriOptimizer$:408 - [Epoch 3 51968/60000][Iteration 673][Wall Clock 61.57052124s] Trained 256 records in 0.087992195 seconds. Throughput is 2909.349 records/second. Loss is 0.7578625. Sequentialfabab260's hyper parameters: Current learning rate is 0.008815232722143865. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:01 INFO  DistriOptimizer$:408 - [Epoch 3 52224/60000][Iteration 674][Wall Clock 61.649137337s] Trained 256 records in 0.078616097 seconds. Throughput is 3256.3306 records/second. Loss is 0.8085406. Sequentialfabab260's hyper parameters: Current learning rate is 0.008813678829543451. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:01 INFO  DistriOptimizer$:408 - [Epoch 3 52480/60000][Iteration 675][Wall Clock 61.726891187s] Trained 256 records in 0.07775385 seconds. Throughput is 3292.4414 records/second. Loss is 0.8182682. Sequentialfabab260's hyper parameters: Current learning rate is 0.008812125484666901. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:01 INFO  DistriOptimizer$:408 - [Epoch 3 52736/60000][Iteration 676][Wall Clock 61.798524929s] Trained 256 records in 0.071633742 seconds. Throughput is 3573.7349 records/second. Loss is 0.75344145. Sequentialfabab260's hyper parameters: Current learning rate is 0.00881057268722467. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:01 INFO  DistriOptimizer$:408 - [Epoch 3 52992/60000][Iteration 677][Wall Clock 61.886921056s] Trained 256 records in 0.088396127 seconds. Throughput is 2896.0544 records/second. Loss is 0.7676114. Sequentialfabab260's hyper parameters: Current learning rate is 0.008809020436927413. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:01 INFO  DistriOptimizer$:408 - [Epoch 3 53248/60000][Iteration 678][Wall Clock 61.964397405s] Trained 256 records in 0.077476349 seconds. Throughput is 3304.2341 records/second. Loss is 0.80618036. Sequentialfabab260's hyper parameters: Current learning rate is 0.008807468733485996. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:01 INFO  DistriOptimizer$:408 - [Epoch 3 53504/60000][Iteration 679][Wall Clock 62.048092216s] Trained 256 records in 0.083694811 seconds. Throughput is 3058.7322 records/second. Loss is 0.7828283. Sequentialfabab260's hyper parameters: Current learning rate is 0.008805917576611484. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:01 INFO  DistriOptimizer$:408 - [Epoch 3 53760/60000][Iteration 680][Wall Clock 62.114352351s] Trained 256 records in 0.066260135 seconds. Throughput is 3863.5598 records/second. Loss is 0.761258. Sequentialfabab260's hyper parameters: Current learning rate is 0.008804366966015144. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 54016/60000][Iteration 681][Wall Clock 62.194546805s] Trained 256 records in 0.080194454 seconds. Throughput is 3192.2407 records/second. Loss is 0.8014848. Sequentialfabab260's hyper parameters: Current learning rate is 0.00880281690140845. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 54272/60000][Iteration 682][Wall Clock 62.261277537s] Trained 256 records in 0.066730732 seconds. Throughput is 3836.3135 records/second. Loss is 0.7025769. Sequentialfabab260's hyper parameters: Current learning rate is 0.00880126738250308. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 54528/60000][Iteration 683][Wall Clock 62.33357748s] Trained 256 records in 0.072299943 seconds. Throughput is 3540.805 records/second. Loss is 0.74161696. Sequentialfabab260's hyper parameters: Current learning rate is 0.008799718409010912. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 54784/60000][Iteration 684][Wall Clock 62.401198937s] Trained 256 records in 0.067621457 seconds. Throughput is 3785.7808 records/second. Loss is 0.7060088. Sequentialfabab260's hyper parameters: Current learning rate is 0.008798169980644026. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 55040/60000][Iteration 685][Wall Clock 62.472992769s] Trained 256 records in 0.071793832 seconds. Throughput is 3565.7659 records/second. Loss is 0.8070386. Sequentialfabab260's hyper parameters: Current learning rate is 0.008796622097114707. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 55296/60000][Iteration 686][Wall Clock 62.545691223s] Trained 256 records in 0.072698454 seconds. Throughput is 3521.3955 records/second. Loss is 0.73946023. Sequentialfabab260's hyper parameters: Current learning rate is 0.008795074758135445. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 55552/60000][Iteration 687][Wall Clock 62.618939253s] Trained 256 records in 0.07324803 seconds. Throughput is 3494.9746 records/second. Loss is 0.7825239. Sequentialfabab260's hyper parameters: Current learning rate is 0.008793527963418923. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 55808/60000][Iteration 688][Wall Clock 62.694082521s] Trained 256 records in 0.075143268 seconds. Throughput is 3406.8254 records/second. Loss is 0.71600485. Sequentialfabab260's hyper parameters: Current learning rate is 0.008791981712678039. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 56064/60000][Iteration 689][Wall Clock 62.768025413s] Trained 256 records in 0.073942892 seconds. Throughput is 3462.1313 records/second. Loss is 0.7401473. Sequentialfabab260's hyper parameters: Current learning rate is 0.00879043600562588. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 56320/60000][Iteration 690][Wall Clock 62.836479822s] Trained 256 records in 0.068454409 seconds. Throughput is 3739.7153 records/second. Loss is 0.87700206. Sequentialfabab260's hyper parameters: Current learning rate is 0.008788890841975743. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 56576/60000][Iteration 691][Wall Clock 62.908417161s] Trained 256 records in 0.071937339 seconds. Throughput is 3558.6526 records/second. Loss is 0.7662201. Sequentialfabab260's hyper parameters: Current learning rate is 0.008787346221441126. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 56832/60000][Iteration 692][Wall Clock 62.972914011s] Trained 256 records in 0.06449685 seconds. Throughput is 3969.186 records/second. Loss is 0.7610488. Sequentialfabab260's hyper parameters: Current learning rate is 0.008785802143735722. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 57088/60000][Iteration 693][Wall Clock 63.05113683s] Trained 256 records in 0.078222819 seconds. Throughput is 3272.7024 records/second. Loss is 0.8461479. Sequentialfabab260's hyper parameters: Current learning rate is 0.008784258608573436. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:02 INFO  DistriOptimizer$:408 - [Epoch 3 57344/60000][Iteration 694][Wall Clock 63.123507885s] Trained 256 records in 0.072371055 seconds. Throughput is 3537.3257 records/second. Loss is 0.78659374. Sequentialfabab260's hyper parameters: Current learning rate is 0.008782715615668365. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:03 INFO  DistriOptimizer$:408 - [Epoch 3 57600/60000][Iteration 695][Wall Clock 63.210477978s] Trained 256 records in 0.086970093 seconds. Throughput is 2943.5408 records/second. Loss is 0.76981115. Sequentialfabab260's hyper parameters: Current learning rate is 0.008781173164734809. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:03 INFO  DistriOptimizer$:408 - [Epoch 3 57856/60000][Iteration 696][Wall Clock 63.29278151s] Trained 256 records in 0.082303532 seconds. Throughput is 3110.4375 records/second. Loss is 0.77945507. Sequentialfabab260's hyper parameters: Current learning rate is 0.00877963125548727. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:03 INFO  DistriOptimizer$:408 - [Epoch 3 58112/60000][Iteration 697][Wall Clock 63.394279711s] Trained 256 records in 0.101498201 seconds. Throughput is 2522.2122 records/second. Loss is 0.8127144. Sequentialfabab260's hyper parameters: Current learning rate is 0.00877808988764045. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:03 INFO  DistriOptimizer$:408 - [Epoch 3 58368/60000][Iteration 698][Wall Clock 63.477594622s] Trained 256 records in 0.083314911 seconds. Throughput is 3072.6792 records/second. Loss is 0.7491969. Sequentialfabab260's hyper parameters: Current learning rate is 0.008776549060909251. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:03 INFO  DistriOptimizer$:408 - [Epoch 3 58624/60000][Iteration 699][Wall Clock 63.562927549s] Trained 256 records in 0.085332927 seconds. Throughput is 3000.0142 records/second. Loss is 0.710454. Sequentialfabab260's hyper parameters: Current learning rate is 0.008775008775008775. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:03 INFO  DistriOptimizer$:408 - [Epoch 3 58880/60000][Iteration 700][Wall Clock 63.64517168s] Trained 256 records in 0.082244131 seconds. Throughput is 3112.6843 records/second. Loss is 0.8087748. Sequentialfabab260's hyper parameters: Current learning rate is 0.008773469029654327. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:03 INFO  DistriOptimizer$:408 - [Epoch 3 59136/60000][Iteration 701][Wall Clock 63.721816323s] Trained 256 records in 0.076644643 seconds. Throughput is 3340.0898 records/second. Loss is 0.68908423. Sequentialfabab260's hyper parameters: Current learning rate is 0.008771929824561403. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:03 INFO  DistriOptimizer$:408 - [Epoch 3 59392/60000][Iteration 702][Wall Clock 63.797643602s] Trained 256 records in 0.075827279 seconds. Throughput is 3376.0938 records/second. Loss is 0.7166154. Sequentialfabab260's hyper parameters: Current learning rate is 0.00877039115944571. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:03 INFO  DistriOptimizer$:408 - [Epoch 3 59648/60000][Iteration 703][Wall Clock 63.877196345s] Trained 256 records in 0.079552743 seconds. Throughput is 3217.991 records/second. Loss is 0.8088037. Sequentialfabab260's hyper parameters: Current learning rate is 0.008768853034023149. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:03 INFO  DistriOptimizer$:408 - [Epoch 3 59904/60000][Iteration 704][Wall Clock 63.943066723s] Trained 256 records in 0.065870378 seconds. Throughput is 3886.421 records/second. Loss is 0.7169901. Sequentialfabab260's hyper parameters: Current learning rate is 0.00876731544800982. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:03 INFO  DistriOptimizer$:408 - [Epoch 3 60160/60000][Iteration 705][Wall Clock 64.030582019s] Trained 256 records in 0.087515296 seconds. Throughput is 2925.203 records/second. Loss is 0.735619. Sequentialfabab260's hyper parameters: Current learning rate is 0.00876577840112202. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:03 INFO  DistriOptimizer$:452 - [Epoch 3 60160/60000][Iteration 705][Wall Clock 64.030582019s] Epoch finished. Wall clock time is 64525.841432 ms
2019-10-17 12:22:03 INFO  DistriOptimizer$:111 - [Epoch 3 60160/60000][Iteration 705][Wall Clock 64.030582019s] Validate model...
2019-10-17 12:22:04 INFO  DistriOptimizer$:178 - [Epoch 3 60160/60000][Iteration 705][Wall Clock 64.030582019s] validate model throughput is 37755.03 records/second
2019-10-17 12:22:04 INFO  DistriOptimizer$:181 - [Epoch 3 60160/60000][Iteration 705][Wall Clock 64.030582019s] Top1Accuracy is Accuracy(correct: 8269, count: 10000, accuracy: 0.8269)
2019-10-17 12:22:04 INFO  DistriOptimizer$:221 - [Wall Clock 64.525841432s] Save model to /tmp/lenet5/20191017_122059
2019-10-17 12:22:04 INFO  DistriOptimizer$:226 - [Wall Clock 64.525841432s] Save optimMethod com.intel.analytics.bigdl.optim.SGD@9429a82 to /tmp/lenet5/20191017_122059
2019-10-17 12:22:04 INFO  DistriOptimizer$:408 - [Epoch 4 256/60000][Iteration 706][Wall Clock 64.604383852s] Trained 256 records in 0.07854242 seconds. Throughput is 3259.3853 records/second. Loss is 0.7315644. Sequentialfabab260's hyper parameters: Current learning rate is 0.008764241893076249. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:04 INFO  DistriOptimizer$:408 - [Epoch 4 512/60000][Iteration 707][Wall Clock 64.679080545s] Trained 256 records in 0.074696693 seconds. Throughput is 3427.1934 records/second. Loss is 0.72393894. Sequentialfabab260's hyper parameters: Current learning rate is 0.008762705923589204. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:04 INFO  DistriOptimizer$:408 - [Epoch 4 768/60000][Iteration 708][Wall Clock 64.76098146s] Trained 256 records in 0.081900915 seconds. Throughput is 3125.728 records/second. Loss is 0.6786555. Sequentialfabab260's hyper parameters: Current learning rate is 0.008761170492377781. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:04 INFO  DistriOptimizer$:408 - [Epoch 4 1024/60000][Iteration 709][Wall Clock 64.851127012s] Trained 256 records in 0.090145552 seconds. Throughput is 2839.8518 records/second. Loss is 0.7518822. Sequentialfabab260's hyper parameters: Current learning rate is 0.008759635599159075. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:04 INFO  DistriOptimizer$:408 - [Epoch 4 1280/60000][Iteration 710][Wall Clock 64.930077814s] Trained 256 records in 0.078950802 seconds. Throughput is 3242.5256 records/second. Loss is 0.7532071. Sequentialfabab260's hyper parameters: Current learning rate is 0.008758101243650377. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:04 INFO  DistriOptimizer$:408 - [Epoch 4 1536/60000][Iteration 711][Wall Clock 65.005500398s] Trained 256 records in 0.075422584 seconds. Throughput is 3394.2087 records/second. Loss is 0.76106167. Sequentialfabab260's hyper parameters: Current learning rate is 0.008756567425569177. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:04 INFO  DistriOptimizer$:408 - [Epoch 4 1792/60000][Iteration 712][Wall Clock 65.082795973s] Trained 256 records in 0.077295575 seconds. Throughput is 3311.9622 records/second. Loss is 0.7292557. Sequentialfabab260's hyper parameters: Current learning rate is 0.008755034144633165. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:04 INFO  DistriOptimizer$:408 - [Epoch 4 2048/60000][Iteration 713][Wall Clock 65.160852042s] Trained 256 records in 0.078056069 seconds. Throughput is 3279.6938 records/second. Loss is 0.7216017. Sequentialfabab260's hyper parameters: Current learning rate is 0.008753501400560224. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:04 INFO  DistriOptimizer$:408 - [Epoch 4 2304/60000][Iteration 714][Wall Clock 65.244412088s] Trained 256 records in 0.083560046 seconds. Throughput is 3063.665 records/second. Loss is 0.7129258. Sequentialfabab260's hyper parameters: Current learning rate is 0.00875196919306844. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 2560/60000][Iteration 715][Wall Clock 65.323732951s] Trained 256 records in 0.079320863 seconds. Throughput is 3227.3982 records/second. Loss is 0.7368182. Sequentialfabab260's hyper parameters: Current learning rate is 0.008750437521876094. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 2816/60000][Iteration 716][Wall Clock 65.399114653s] Trained 256 records in 0.075381702 seconds. Throughput is 3396.0496 records/second. Loss is 0.7429714. Sequentialfabab260's hyper parameters: Current learning rate is 0.008748906386701663. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 3072/60000][Iteration 717][Wall Clock 65.47210901s] Trained 256 records in 0.072994357 seconds. Throughput is 3507.1204 records/second. Loss is 0.66976476. Sequentialfabab260's hyper parameters: Current learning rate is 0.008747375787263822. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 3328/60000][Iteration 718][Wall Clock 65.554506749s] Trained 256 records in 0.082397739 seconds. Throughput is 3106.8813 records/second. Loss is 0.6807964. Sequentialfabab260's hyper parameters: Current learning rate is 0.008745845723281442. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 3584/60000][Iteration 719][Wall Clock 65.636670326s] Trained 256 records in 0.082163577 seconds. Throughput is 3115.7356 records/second. Loss is 0.6988429. Sequentialfabab260's hyper parameters: Current learning rate is 0.008744316194473592. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 3840/60000][Iteration 720][Wall Clock 65.715107147s] Trained 256 records in 0.078436821 seconds. Throughput is 3263.7732 records/second. Loss is 0.7368386. Sequentialfabab260's hyper parameters: Current learning rate is 0.00874278720055954. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 4096/60000][Iteration 721][Wall Clock 65.793885056s] Trained 256 records in 0.078777909 seconds. Throughput is 3249.6418 records/second. Loss is 0.75334764. Sequentialfabab260's hyper parameters: Current learning rate is 0.00874125874125874. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 4352/60000][Iteration 722][Wall Clock 65.867444071s] Trained 256 records in 0.073559015 seconds. Throughput is 3480.1987 records/second. Loss is 0.7816638. Sequentialfabab260's hyper parameters: Current learning rate is 0.008739730816290857. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 4608/60000][Iteration 723][Wall Clock 65.938021681s] Trained 256 records in 0.07057761 seconds. Throughput is 3627.213 records/second. Loss is 0.7392864. Sequentialfabab260's hyper parameters: Current learning rate is 0.008738203425375742. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 4864/60000][Iteration 724][Wall Clock 66.009410322s] Trained 256 records in 0.071388641 seconds. Throughput is 3586.0046 records/second. Loss is 0.71042174. Sequentialfabab260's hyper parameters: Current learning rate is 0.008736676568233443. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 5120/60000][Iteration 725][Wall Clock 66.087460123s] Trained 256 records in 0.078049801 seconds. Throughput is 3279.957 records/second. Loss is 0.72884977. Sequentialfabab260's hyper parameters: Current learning rate is 0.008735150244584206. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 5376/60000][Iteration 726][Wall Clock 66.163828627s] Trained 256 records in 0.076368504 seconds. Throughput is 3352.1672 records/second. Loss is 0.61520875. Sequentialfabab260's hyper parameters: Current learning rate is 0.008733624454148471. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:05 INFO  DistriOptimizer$:408 - [Epoch 4 5632/60000][Iteration 727][Wall Clock 66.237492836s] Trained 256 records in 0.073664209 seconds. Throughput is 3475.229 records/second. Loss is 0.7567966. Sequentialfabab260's hyper parameters: Current learning rate is 0.008732099196646874. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 5888/60000][Iteration 728][Wall Clock 66.313450278s] Trained 256 records in 0.075957442 seconds. Throughput is 3370.3083 records/second. Loss is 0.7997457. Sequentialfabab260's hyper parameters: Current learning rate is 0.008730574471800244. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 6144/60000][Iteration 729][Wall Clock 66.386263045s] Trained 256 records in 0.072812767 seconds. Throughput is 3515.867 records/second. Loss is 0.69870746. Sequentialfabab260's hyper parameters: Current learning rate is 0.00872905027932961. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 6400/60000][Iteration 730][Wall Clock 66.451978124s] Trained 256 records in 0.065715079 seconds. Throughput is 3895.605 records/second. Loss is 0.726271. Sequentialfabab260's hyper parameters: Current learning rate is 0.008727526618956188. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 6656/60000][Iteration 731][Wall Clock 66.521169655s] Trained 256 records in 0.069191531 seconds. Throughput is 3699.8748 records/second. Loss is 0.71334994. Sequentialfabab260's hyper parameters: Current learning rate is 0.008726003490401398. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 6912/60000][Iteration 732][Wall Clock 66.595466934s] Trained 256 records in 0.074297279 seconds. Throughput is 3445.6174 records/second. Loss is 0.71527785. Sequentialfabab260's hyper parameters: Current learning rate is 0.008724480893386845. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 7168/60000][Iteration 733][Wall Clock 66.689571601s] Trained 256 records in 0.094104667 seconds. Throughput is 2720.375 records/second. Loss is 0.72690254. Sequentialfabab260's hyper parameters: Current learning rate is 0.008722958827634334. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 7424/60000][Iteration 734][Wall Clock 66.754094489s] Trained 256 records in 0.064522888 seconds. Throughput is 3967.5845 records/second. Loss is 0.68520415. Sequentialfabab260's hyper parameters: Current learning rate is 0.008721437292865864. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 7680/60000][Iteration 735][Wall Clock 66.833689649s] Trained 256 records in 0.07959516 seconds. Throughput is 3216.276 records/second. Loss is 0.7081996. Sequentialfabab260's hyper parameters: Current learning rate is 0.008719916288803628. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 7936/60000][Iteration 736][Wall Clock 66.91381168s] Trained 256 records in 0.080122031 seconds. Throughput is 3195.1262 records/second. Loss is 0.6582855. Sequentialfabab260's hyper parameters: Current learning rate is 0.008718395815170008. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 8192/60000][Iteration 737][Wall Clock 66.993003255s] Trained 256 records in 0.079191575 seconds. Throughput is 3232.6672 records/second. Loss is 0.6879666. Sequentialfabab260's hyper parameters: Current learning rate is 0.008716875871687587. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 8448/60000][Iteration 738][Wall Clock 67.066319657s] Trained 256 records in 0.073316402 seconds. Throughput is 3491.7153 records/second. Loss is 0.73530596. Sequentialfabab260's hyper parameters: Current learning rate is 0.008715356458079136. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 8704/60000][Iteration 739][Wall Clock 67.133926522s] Trained 256 records in 0.067606865 seconds. Throughput is 3786.5977 records/second. Loss is 0.65612996. Sequentialfabab260's hyper parameters: Current learning rate is 0.00871383757406762. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 8960/60000][Iteration 740][Wall Clock 67.199701489s] Trained 256 records in 0.065774967 seconds. Throughput is 3892.058 records/second. Loss is 0.6741093. Sequentialfabab260's hyper parameters: Current learning rate is 0.008712319219376199. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:06 INFO  DistriOptimizer$:408 - [Epoch 4 9216/60000][Iteration 741][Wall Clock 67.267827684s] Trained 256 records in 0.068126195 seconds. Throughput is 3757.7322 records/second. Loss is 0.6699641. Sequentialfabab260's hyper parameters: Current learning rate is 0.008710801393728223. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:07 INFO  DistriOptimizer$:408 - [Epoch 4 9472/60000][Iteration 742][Wall Clock 67.332911815s] Trained 256 records in 0.065084131 seconds. Throughput is 3933.3706 records/second. Loss is 0.6417192. Sequentialfabab260's hyper parameters: Current learning rate is 0.008709284096847238. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:07 INFO  DistriOptimizer$:408 - [Epoch 4 9728/60000][Iteration 743][Wall Clock 67.412325503s] Trained 256 records in 0.079413688 seconds. Throughput is 3223.6255 records/second. Loss is 0.67708826. Sequentialfabab260's hyper parameters: Current learning rate is 0.008707767328456984. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:07 INFO  DistriOptimizer$:408 - [Epoch 4 9984/60000][Iteration 744][Wall Clock 67.501105894s] Trained 256 records in 0.088780391 seconds. Throughput is 2883.5198 records/second. Loss is 0.6942605. Sequentialfabab260's hyper parameters: Current learning rate is 0.008706251088281386. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:07 INFO  DistriOptimizer$:408 - [Epoch 4 10240/60000][Iteration 745][Wall Clock 67.580993438s] Trained 256 records in 0.079887544 seconds. Throughput is 3204.5044 records/second. Loss is 0.7194532. Sequentialfabab260's hyper parameters: Current learning rate is 0.008704735376044569. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:07 INFO  DistriOptimizer$:408 - [Epoch 4 10496/60000][Iteration 746][Wall Clock 67.667006101s] Trained 256 records in 0.086012663 seconds. Throughput is 2976.306 records/second. Loss is 0.7023769. Sequentialfabab260's hyper parameters: Current learning rate is 0.008703220191470844. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:07 INFO  DistriOptimizer$:408 - [Epoch 4 10752/60000][Iteration 747][Wall Clock 67.762772031s] Trained 256 records in 0.09576593 seconds. Throughput is 2673.1843 records/second. Loss is 0.62082046. Sequentialfabab260's hyper parameters: Current learning rate is 0.00870170553428472. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:07 INFO  DistriOptimizer$:408 - [Epoch 4 11008/60000][Iteration 748][Wall Clock 67.835154452s] Trained 256 records in 0.072382421 seconds. Throughput is 3536.7703 records/second. Loss is 0.66446865. Sequentialfabab260's hyper parameters: Current learning rate is 0.008700191404210893. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:07 INFO  DistriOptimizer$:408 - [Epoch 4 11264/60000][Iteration 749][Wall Clock 67.909203102s] Trained 256 records in 0.07404865 seconds. Throughput is 3457.1865 records/second. Loss is 0.73321664. Sequentialfabab260's hyper parameters: Current learning rate is 0.008698677800974252. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:07 INFO  DistriOptimizer$:408 - [Epoch 4 11520/60000][Iteration 750][Wall Clock 67.979111826s] Trained 256 records in 0.069908724 seconds. Throughput is 3661.9177 records/second. Loss is 0.6947763. Sequentialfabab260's hyper parameters: Current learning rate is 0.008697164724299879. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:07 INFO  DistriOptimizer$:408 - [Epoch 4 11776/60000][Iteration 751][Wall Clock 68.054178022s] Trained 256 records in 0.075066196 seconds. Throughput is 3410.3235 records/second. Loss is 0.71031857. Sequentialfabab260's hyper parameters: Current learning rate is 0.008695652173913044. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:07 INFO  DistriOptimizer$:408 - [Epoch 4 12032/60000][Iteration 752][Wall Clock 68.132869445s] Trained 256 records in 0.078691423 seconds. Throughput is 3253.2136 records/second. Loss is 0.6946856. Sequentialfabab260's hyper parameters: Current learning rate is 0.008694140149539212. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:07 INFO  DistriOptimizer$:408 - [Epoch 4 12288/60000][Iteration 753][Wall Clock 68.205759226s] Trained 256 records in 0.072889781 seconds. Throughput is 3512.152 records/second. Loss is 0.63068855. Sequentialfabab260's hyper parameters: Current learning rate is 0.008692628650904033. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 12544/60000][Iteration 754][Wall Clock 68.271075011s] Trained 256 records in 0.065315785 seconds. Throughput is 3919.4202 records/second. Loss is 0.6399713. Sequentialfabab260's hyper parameters: Current learning rate is 0.008691117677733355. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 12800/60000][Iteration 755][Wall Clock 68.33572413s] Trained 256 records in 0.064649119 seconds. Throughput is 3959.8374 records/second. Loss is 0.714949. Sequentialfabab260's hyper parameters: Current learning rate is 0.008689607229753215. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 13056/60000][Iteration 756][Wall Clock 68.402942093s] Trained 256 records in 0.067217963 seconds. Throughput is 3808.5059 records/second. Loss is 0.6426012. Sequentialfabab260's hyper parameters: Current learning rate is 0.008688097306689836. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 13312/60000][Iteration 757][Wall Clock 68.487071808s] Trained 256 records in 0.084129715 seconds. Throughput is 3042.9202 records/second. Loss is 0.65906197. Sequentialfabab260's hyper parameters: Current learning rate is 0.008686587908269632. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 13568/60000][Iteration 758][Wall Clock 68.556363837s] Trained 256 records in 0.069292029 seconds. Throughput is 3694.5085 records/second. Loss is 0.6977819. Sequentialfabab260's hyper parameters: Current learning rate is 0.008685079034219213. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 13824/60000][Iteration 759][Wall Clock 68.633962316s] Trained 256 records in 0.077598479 seconds. Throughput is 3299.0337 records/second. Loss is 0.7125666. Sequentialfabab260's hyper parameters: Current learning rate is 0.00868357068426537. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 14080/60000][Iteration 760][Wall Clock 68.709440321s] Trained 256 records in 0.075478005 seconds. Throughput is 3391.7168 records/second. Loss is 0.6982204. Sequentialfabab260's hyper parameters: Current learning rate is 0.008682062858135093. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 14336/60000][Iteration 761][Wall Clock 68.781552842s] Trained 256 records in 0.072112521 seconds. Throughput is 3550.0076 records/second. Loss is 0.7401565. Sequentialfabab260's hyper parameters: Current learning rate is 0.008680555555555556. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 14592/60000][Iteration 762][Wall Clock 68.863935792s] Trained 256 records in 0.08238295 seconds. Throughput is 3107.4392 records/second. Loss is 0.65976214. Sequentialfabab260's hyper parameters: Current learning rate is 0.008679048776254122. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 14848/60000][Iteration 763][Wall Clock 68.935848016s] Trained 256 records in 0.071912224 seconds. Throughput is 3559.8955 records/second. Loss is 0.69282055. Sequentialfabab260's hyper parameters: Current learning rate is 0.008677542519958347. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 15104/60000][Iteration 764][Wall Clock 69.005593108s] Trained 256 records in 0.069745092 seconds. Throughput is 3670.509 records/second. Loss is 0.63726145. Sequentialfabab260's hyper parameters: Current learning rate is 0.008676036786395974. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 15360/60000][Iteration 765][Wall Clock 69.073753645s] Trained 256 records in 0.068160537 seconds. Throughput is 3755.839 records/second. Loss is 0.6169488. Sequentialfabab260's hyper parameters: Current learning rate is 0.008674531575294934. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 15616/60000][Iteration 766][Wall Clock 69.141778099s] Trained 256 records in 0.068024454 seconds. Throughput is 3763.3523 records/second. Loss is 0.6697118. Sequentialfabab260's hyper parameters: Current learning rate is 0.008673026886383347. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:08 INFO  DistriOptimizer$:408 - [Epoch 4 15872/60000][Iteration 767][Wall Clock 69.216953203s] Trained 256 records in 0.075175104 seconds. Throughput is 3405.3826 records/second. Loss is 0.6178082. Sequentialfabab260's hyper parameters: Current learning rate is 0.008671522719389525. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 16128/60000][Iteration 768][Wall Clock 69.307843185s] Trained 256 records in 0.090889982 seconds. Throughput is 2816.592 records/second. Loss is 0.72197044. Sequentialfabab260's hyper parameters: Current learning rate is 0.008670019074041963. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 16384/60000][Iteration 769][Wall Clock 69.395090359s] Trained 256 records in 0.087247174 seconds. Throughput is 2934.1926 records/second. Loss is 0.76395607. Sequentialfabab260's hyper parameters: Current learning rate is 0.00866851595006935. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 16640/60000][Iteration 770][Wall Clock 69.474531736s] Trained 256 records in 0.079441377 seconds. Throughput is 3222.5022 records/second. Loss is 0.6677853. Sequentialfabab260's hyper parameters: Current learning rate is 0.008667013347200556. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 16896/60000][Iteration 771][Wall Clock 69.563341202s] Trained 256 records in 0.088809466 seconds. Throughput is 2882.5754 records/second. Loss is 0.70521957. Sequentialfabab260's hyper parameters: Current learning rate is 0.008665511265164646. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 17152/60000][Iteration 772][Wall Clock 69.636400476s] Trained 256 records in 0.073059274 seconds. Throughput is 3504.0042 records/second. Loss is 0.68879944. Sequentialfabab260's hyper parameters: Current learning rate is 0.00866400970369087. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 17408/60000][Iteration 773][Wall Clock 69.710487585s] Trained 256 records in 0.074087109 seconds. Throughput is 3455.392 records/second. Loss is 0.6518372. Sequentialfabab260's hyper parameters: Current learning rate is 0.008662508662508662. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 17664/60000][Iteration 774][Wall Clock 69.786811685s] Trained 256 records in 0.0763241 seconds. Throughput is 3354.1177 records/second. Loss is 0.7870053. Sequentialfabab260's hyper parameters: Current learning rate is 0.008661008141347652. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 17920/60000][Iteration 775][Wall Clock 69.856646811s] Trained 256 records in 0.069835126 seconds. Throughput is 3665.777 records/second. Loss is 0.8193028. Sequentialfabab260's hyper parameters: Current learning rate is 0.008659508139937652. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 18176/60000][Iteration 776][Wall Clock 69.928431757s] Trained 256 records in 0.071784946 seconds. Throughput is 3566.2075 records/second. Loss is 0.77693266. Sequentialfabab260's hyper parameters: Current learning rate is 0.008658008658008658. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 18432/60000][Iteration 777][Wall Clock 70.00255528s] Trained 256 records in 0.074123523 seconds. Throughput is 3453.6943 records/second. Loss is 0.7087562. Sequentialfabab260's hyper parameters: Current learning rate is 0.00865650969529086. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 18688/60000][Iteration 778][Wall Clock 70.068694231s] Trained 256 records in 0.066138951 seconds. Throughput is 3870.639 records/second. Loss is 0.6855017. Sequentialfabab260's hyper parameters: Current learning rate is 0.008655011251514627. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 18944/60000][Iteration 779][Wall Clock 70.142799639s] Trained 256 records in 0.074105408 seconds. Throughput is 3454.5388 records/second. Loss is 0.74974364. Sequentialfabab260's hyper parameters: Current learning rate is 0.008653513326410523. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:09 INFO  DistriOptimizer$:408 - [Epoch 4 19200/60000][Iteration 780][Wall Clock 70.210209427s] Trained 256 records in 0.067409788 seconds. Throughput is 3797.668 records/second. Loss is 0.74092233. Sequentialfabab260's hyper parameters: Current learning rate is 0.008652015919709292. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 19456/60000][Iteration 781][Wall Clock 70.288093725s] Trained 256 records in 0.077884298 seconds. Throughput is 3286.9268 records/second. Loss is 0.59434867. Sequentialfabab260's hyper parameters: Current learning rate is 0.00865051903114187. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 19712/60000][Iteration 782][Wall Clock 70.360001601s] Trained 256 records in 0.071907876 seconds. Throughput is 3560.1106 records/second. Loss is 0.72564614. Sequentialfabab260's hyper parameters: Current learning rate is 0.008649022660439369. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 19968/60000][Iteration 783][Wall Clock 70.431351187s] Trained 256 records in 0.071349586 seconds. Throughput is 3587.9678 records/second. Loss is 0.71430504. Sequentialfabab260's hyper parameters: Current learning rate is 0.008647526807333102. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 20224/60000][Iteration 784][Wall Clock 70.507302479s] Trained 256 records in 0.075951292 seconds. Throughput is 3370.5813 records/second. Loss is 0.6542326. Sequentialfabab260's hyper parameters: Current learning rate is 0.008646031471554556. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 20480/60000][Iteration 785][Wall Clock 70.578572477s] Trained 256 records in 0.071269998 seconds. Throughput is 3591.9744 records/second. Loss is 0.64542955. Sequentialfabab260's hyper parameters: Current learning rate is 0.008644536652835409. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 20736/60000][Iteration 786][Wall Clock 70.657553415s] Trained 256 records in 0.078980938 seconds. Throughput is 3241.2883 records/second. Loss is 0.60273457. Sequentialfabab260's hyper parameters: Current learning rate is 0.00864304235090752. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 20992/60000][Iteration 787][Wall Clock 70.733324951s] Trained 256 records in 0.075771536 seconds. Throughput is 3378.5776 records/second. Loss is 0.65062237. Sequentialfabab260's hyper parameters: Current learning rate is 0.008641548565502939. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 21248/60000][Iteration 788][Wall Clock 70.804443162s] Trained 256 records in 0.071118211 seconds. Throughput is 3599.6404 records/second. Loss is 0.6415521. Sequentialfabab260's hyper parameters: Current learning rate is 0.008640055296353897. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 21504/60000][Iteration 789][Wall Clock 70.882544057s] Trained 256 records in 0.078100895 seconds. Throughput is 3277.8113 records/second. Loss is 0.64902383. Sequentialfabab260's hyper parameters: Current learning rate is 0.008638562543192813. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 21760/60000][Iteration 790][Wall Clock 70.949492413s] Trained 256 records in 0.066948356 seconds. Throughput is 3823.843 records/second. Loss is 0.6837159. Sequentialfabab260's hyper parameters: Current learning rate is 0.00863707030575229. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 22016/60000][Iteration 791][Wall Clock 71.031174966s] Trained 256 records in 0.081682553 seconds. Throughput is 3134.0842 records/second. Loss is 0.66699255. Sequentialfabab260's hyper parameters: Current learning rate is 0.008635578583765112. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 22272/60000][Iteration 792][Wall Clock 71.096229434s] Trained 256 records in 0.065054468 seconds. Throughput is 3935.164 records/second. Loss is 0.6883058. Sequentialfabab260's hyper parameters: Current learning rate is 0.008634087376964255. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:10 INFO  DistriOptimizer$:408 - [Epoch 4 22528/60000][Iteration 793][Wall Clock 71.179904138s] Trained 256 records in 0.083674704 seconds. Throughput is 3059.467 records/second. Loss is 0.6320924. Sequentialfabab260's hyper parameters: Current learning rate is 0.008632596685082872. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 22784/60000][Iteration 794][Wall Clock 71.260442928s] Trained 256 records in 0.08053879 seconds. Throughput is 3178.5928 records/second. Loss is 0.6890341. Sequentialfabab260's hyper parameters: Current learning rate is 0.008631106507854307. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 23040/60000][Iteration 795][Wall Clock 71.345233551s] Trained 256 records in 0.084790623 seconds. Throughput is 3019.2017 records/second. Loss is 0.7559686. Sequentialfabab260's hyper parameters: Current learning rate is 0.00862961684501208. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 23296/60000][Iteration 796][Wall Clock 71.428794413s] Trained 256 records in 0.083560862 seconds. Throughput is 3063.6353 records/second. Loss is 0.5955985. Sequentialfabab260's hyper parameters: Current learning rate is 0.008628127696289905. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 23552/60000][Iteration 797][Wall Clock 71.509706111s] Trained 256 records in 0.080911698 seconds. Throughput is 3163.943 records/second. Loss is 0.6914247. Sequentialfabab260's hyper parameters: Current learning rate is 0.00862663906142167. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 23808/60000][Iteration 798][Wall Clock 71.584234742s] Trained 256 records in 0.074528631 seconds. Throughput is 3434.9214 records/second. Loss is 0.6886224. Sequentialfabab260's hyper parameters: Current learning rate is 0.008625150940141452. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 24064/60000][Iteration 799][Wall Clock 71.659638843s] Trained 256 records in 0.075404101 seconds. Throughput is 3395.0408 records/second. Loss is 0.63646495. Sequentialfabab260's hyper parameters: Current learning rate is 0.008623663332183512. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 24320/60000][Iteration 800][Wall Clock 71.739273751s] Trained 256 records in 0.079634908 seconds. Throughput is 3214.6707 records/second. Loss is 0.68799925. Sequentialfabab260's hyper parameters: Current learning rate is 0.00862217623728229. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 24576/60000][Iteration 801][Wall Clock 71.815886112s] Trained 256 records in 0.076612361 seconds. Throughput is 3341.4973 records/second. Loss is 0.6493554. Sequentialfabab260's hyper parameters: Current learning rate is 0.008620689655172415. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 24832/60000][Iteration 802][Wall Clock 71.885511861s] Trained 256 records in 0.069625749 seconds. Throughput is 3676.8005 records/second. Loss is 0.59320045. Sequentialfabab260's hyper parameters: Current learning rate is 0.008619203585588691. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 25088/60000][Iteration 803][Wall Clock 71.952392944s] Trained 256 records in 0.066881083 seconds. Throughput is 3827.6892 records/second. Loss is 0.7051149. Sequentialfabab260's hyper parameters: Current learning rate is 0.008617718028266115. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 25344/60000][Iteration 804][Wall Clock 72.023551462s] Trained 256 records in 0.071158518 seconds. Throughput is 3597.6016 records/second. Loss is 0.6405958. Sequentialfabab260's hyper parameters: Current learning rate is 0.008616232982939858. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 25600/60000][Iteration 805][Wall Clock 72.101514594s] Trained 256 records in 0.077963132 seconds. Throughput is 3283.6035 records/second. Loss is 0.6783229. Sequentialfabab260's hyper parameters: Current learning rate is 0.008614748449345278. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 25856/60000][Iteration 806][Wall Clock 72.170667822s] Trained 256 records in 0.069153228 seconds. Throughput is 3701.924 records/second. Loss is 0.7864931. Sequentialfabab260's hyper parameters: Current learning rate is 0.008613264427217916. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:11 INFO  DistriOptimizer$:408 - [Epoch 4 26112/60000][Iteration 807][Wall Clock 72.238279862s] Trained 256 records in 0.06761204 seconds. Throughput is 3786.308 records/second. Loss is 0.5994097. Sequentialfabab260's hyper parameters: Current learning rate is 0.00861178091629349. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 26368/60000][Iteration 808][Wall Clock 72.303884705s] Trained 256 records in 0.065604843 seconds. Throughput is 3902.151 records/second. Loss is 0.5907645. Sequentialfabab260's hyper parameters: Current learning rate is 0.008610297916307904. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 26624/60000][Iteration 809][Wall Clock 72.36948367s] Trained 256 records in 0.065598965 seconds. Throughput is 3902.5007 records/second. Loss is 0.7434899. Sequentialfabab260's hyper parameters: Current learning rate is 0.008608815426997245. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 26880/60000][Iteration 810][Wall Clock 72.448524525s] Trained 256 records in 0.079040855 seconds. Throughput is 3238.8313 records/second. Loss is 0.6703532. Sequentialfabab260's hyper parameters: Current learning rate is 0.00860733344809778. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 27136/60000][Iteration 811][Wall Clock 72.523373673s] Trained 256 records in 0.074849148 seconds. Throughput is 3420.2124 records/second. Loss is 0.67187965. Sequentialfabab260's hyper parameters: Current learning rate is 0.008605851979345956. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 27392/60000][Iteration 812][Wall Clock 72.605298778s] Trained 256 records in 0.081925105 seconds. Throughput is 3124.8054 records/second. Loss is 0.6605914. Sequentialfabab260's hyper parameters: Current learning rate is 0.008604371020478403. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 27648/60000][Iteration 813][Wall Clock 72.680133282s] Trained 256 records in 0.074834504 seconds. Throughput is 3420.8818 records/second. Loss is 0.6580396. Sequentialfabab260's hyper parameters: Current learning rate is 0.008602890571231933. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 27904/60000][Iteration 814][Wall Clock 72.748561525s] Trained 256 records in 0.068428243 seconds. Throughput is 3741.1455 records/second. Loss is 0.6264699. Sequentialfabab260's hyper parameters: Current learning rate is 0.00860141063134354. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 28160/60000][Iteration 815][Wall Clock 72.82415143s] Trained 256 records in 0.075589905 seconds. Throughput is 3386.6958 records/second. Loss is 0.6406743. Sequentialfabab260's hyper parameters: Current learning rate is 0.008599931200550396. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 28416/60000][Iteration 816][Wall Clock 72.905040993s] Trained 256 records in 0.080889563 seconds. Throughput is 3164.8088 records/second. Loss is 0.6113858. Sequentialfabab260's hyper parameters: Current learning rate is 0.008598452278589854. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 28672/60000][Iteration 817][Wall Clock 72.978058739s] Trained 256 records in 0.073017746 seconds. Throughput is 3505.997 records/second. Loss is 0.65384793. Sequentialfabab260's hyper parameters: Current learning rate is 0.00859697386519945. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 28928/60000][Iteration 818][Wall Clock 73.049800406s] Trained 256 records in 0.071741667 seconds. Throughput is 3568.3584 records/second. Loss is 0.65620553. Sequentialfabab260's hyper parameters: Current learning rate is 0.008595495960116899. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 29184/60000][Iteration 819][Wall Clock 73.130689335s] Trained 256 records in 0.080888929 seconds. Throughput is 3164.8337 records/second. Loss is 0.6162466. Sequentialfabab260's hyper parameters: Current learning rate is 0.008594018563080097. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:12 INFO  DistriOptimizer$:408 - [Epoch 4 29440/60000][Iteration 820][Wall Clock 73.210233525s] Trained 256 records in 0.07954419 seconds. Throughput is 3218.337 records/second. Loss is 0.5678865. Sequentialfabab260's hyper parameters: Current learning rate is 0.008592541673827118. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 29696/60000][Iteration 821][Wall Clock 73.29293793s] Trained 256 records in 0.082704405 seconds. Throughput is 3095.3613 records/second. Loss is 0.62708175. Sequentialfabab260's hyper parameters: Current learning rate is 0.00859106529209622. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 29952/60000][Iteration 822][Wall Clock 73.362160071s] Trained 256 records in 0.069222141 seconds. Throughput is 3698.2388 records/second. Loss is 0.60308725. Sequentialfabab260's hyper parameters: Current learning rate is 0.008589589417625837. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 30208/60000][Iteration 823][Wall Clock 73.423977815s] Trained 256 records in 0.061817744 seconds. Throughput is 4141.206 records/second. Loss is 0.62651026. Sequentialfabab260's hyper parameters: Current learning rate is 0.008588114050154586. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 30464/60000][Iteration 824][Wall Clock 73.504888334s] Trained 256 records in 0.080910519 seconds. Throughput is 3163.989 records/second. Loss is 0.6220417. Sequentialfabab260's hyper parameters: Current learning rate is 0.00858663918942126. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 30720/60000][Iteration 825][Wall Clock 73.573795307s] Trained 256 records in 0.068906973 seconds. Throughput is 3715.1538 records/second. Loss is 0.6080395. Sequentialfabab260's hyper parameters: Current learning rate is 0.008585164835164834. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 30976/60000][Iteration 826][Wall Clock 73.64775713s] Trained 256 records in 0.073961823 seconds. Throughput is 3461.245 records/second. Loss is 0.659838. Sequentialfabab260's hyper parameters: Current learning rate is 0.008583690987124463. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 31232/60000][Iteration 827][Wall Clock 73.721534464s] Trained 256 records in 0.073777334 seconds. Throughput is 3469.9004 records/second. Loss is 0.6366571. Sequentialfabab260's hyper parameters: Current learning rate is 0.008582217645039478. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 31488/60000][Iteration 828][Wall Clock 73.788765338s] Trained 256 records in 0.067230874 seconds. Throughput is 3807.7744 records/second. Loss is 0.6735165. Sequentialfabab260's hyper parameters: Current learning rate is 0.008580744808649392. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 31744/60000][Iteration 829][Wall Clock 73.874246681s] Trained 256 records in 0.085481343 seconds. Throughput is 2994.8054 records/second. Loss is 0.69666755. Sequentialfabab260's hyper parameters: Current learning rate is 0.008579272477693892. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 32000/60000][Iteration 830][Wall Clock 73.940830157s] Trained 256 records in 0.066583476 seconds. Throughput is 3844.7976 records/second. Loss is 0.7053405. Sequentialfabab260's hyper parameters: Current learning rate is 0.00857780065191285. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 32256/60000][Iteration 831][Wall Clock 74.008663854s] Trained 256 records in 0.067833697 seconds. Throughput is 3773.9353 records/second. Loss is 0.6871141. Sequentialfabab260's hyper parameters: Current learning rate is 0.008576329331046312. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 32512/60000][Iteration 832][Wall Clock 74.077578949s] Trained 256 records in 0.068915095 seconds. Throughput is 3714.716 records/second. Loss is 0.65178484. Sequentialfabab260's hyper parameters: Current learning rate is 0.008574858514834506. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 32768/60000][Iteration 833][Wall Clock 74.143944425s] Trained 256 records in 0.066365476 seconds. Throughput is 3857.4275 records/second. Loss is 0.6212337. Sequentialfabab260's hyper parameters: Current learning rate is 0.008573388203017831. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:13 INFO  DistriOptimizer$:408 - [Epoch 4 33024/60000][Iteration 834][Wall Clock 74.214876998s] Trained 256 records in 0.070932573 seconds. Throughput is 3609.0613 records/second. Loss is 0.6795301. Sequentialfabab260's hyper parameters: Current learning rate is 0.008571918395336876. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 33280/60000][Iteration 835][Wall Clock 74.284663491s] Trained 256 records in 0.069786493 seconds. Throughput is 3668.3315 records/second. Loss is 0.611837. Sequentialfabab260's hyper parameters: Current learning rate is 0.008570449091532395. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 33536/60000][Iteration 836][Wall Clock 74.359434175s] Trained 256 records in 0.074770684 seconds. Throughput is 3423.8018 records/second. Loss is 0.644957. Sequentialfabab260's hyper parameters: Current learning rate is 0.00856898029134533. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 33792/60000][Iteration 837][Wall Clock 74.433285292s] Trained 256 records in 0.073851117 seconds. Throughput is 3466.4338 records/second. Loss is 0.69850814. Sequentialfabab260's hyper parameters: Current learning rate is 0.008567511994516792. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 34048/60000][Iteration 838][Wall Clock 74.508220927s] Trained 256 records in 0.074935635 seconds. Throughput is 3416.2651 records/second. Loss is 0.60995156. Sequentialfabab260's hyper parameters: Current learning rate is 0.008566044200788075. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 34304/60000][Iteration 839][Wall Clock 74.585938994s] Trained 256 records in 0.077718067 seconds. Throughput is 3293.9575 records/second. Loss is 0.66903734. Sequentialfabab260's hyper parameters: Current learning rate is 0.00856457690990065. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 34560/60000][Iteration 840][Wall Clock 74.657530166s] Trained 256 records in 0.071591172 seconds. Throughput is 3575.86 records/second. Loss is 0.6036124. Sequentialfabab260's hyper parameters: Current learning rate is 0.008563110121596164. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 34816/60000][Iteration 841][Wall Clock 74.728261445s] Trained 256 records in 0.070731279 seconds. Throughput is 3619.332 records/second. Loss is 0.6469407. Sequentialfabab260's hyper parameters: Current learning rate is 0.00856164383561644. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 35072/60000][Iteration 842][Wall Clock 74.805303978s] Trained 256 records in 0.077042533 seconds. Throughput is 3322.8398 records/second. Loss is 0.6707567. Sequentialfabab260's hyper parameters: Current learning rate is 0.008560178051703475. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 35328/60000][Iteration 843][Wall Clock 74.887688772s] Trained 256 records in 0.082384794 seconds. Throughput is 3107.3696 records/second. Loss is 0.5948226. Sequentialfabab260's hyper parameters: Current learning rate is 0.008558712769599451. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 35584/60000][Iteration 844][Wall Clock 74.958245168s] Trained 256 records in 0.070556396 seconds. Throughput is 3628.3032 records/second. Loss is 0.6155733. Sequentialfabab260's hyper parameters: Current learning rate is 0.008557247989046722. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 35840/60000][Iteration 845][Wall Clock 75.035226672s] Trained 256 records in 0.076981504 seconds. Throughput is 3325.4739 records/second. Loss is 0.7315669. Sequentialfabab260's hyper parameters: Current learning rate is 0.008555783709787816. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 36096/60000][Iteration 846][Wall Clock 75.106771425s] Trained 256 records in 0.071544753 seconds. Throughput is 3578.18 records/second. Loss is 0.64407414. Sequentialfabab260's hyper parameters: Current learning rate is 0.00855431993156544. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:14 INFO  DistriOptimizer$:408 - [Epoch 4 36352/60000][Iteration 847][Wall Clock 75.182874154s] Trained 256 records in 0.076102729 seconds. Throughput is 3363.8743 records/second. Loss is 0.6849462. Sequentialfabab260's hyper parameters: Current learning rate is 0.008552856654122478. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 36608/60000][Iteration 848][Wall Clock 75.262027996s] Trained 256 records in 0.079153842 seconds. Throughput is 3234.208 records/second. Loss is 0.5604604. Sequentialfabab260's hyper parameters: Current learning rate is 0.008551393877201984. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 36864/60000][Iteration 849][Wall Clock 75.331932297s] Trained 256 records in 0.069904301 seconds. Throughput is 3662.1497 records/second. Loss is 0.5894861. Sequentialfabab260's hyper parameters: Current learning rate is 0.008549931600547196. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 37120/60000][Iteration 850][Wall Clock 75.396988849s] Trained 256 records in 0.065056552 seconds. Throughput is 3935.0378 records/second. Loss is 0.6222585. Sequentialfabab260's hyper parameters: Current learning rate is 0.008548469823901523. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 37376/60000][Iteration 851][Wall Clock 75.464907434s] Trained 256 records in 0.067918585 seconds. Throughput is 3769.2188 records/second. Loss is 0.64071596. Sequentialfabab260's hyper parameters: Current learning rate is 0.008547008547008548. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 37632/60000][Iteration 852][Wall Clock 75.536071918s] Trained 256 records in 0.071164484 seconds. Throughput is 3597.3 records/second. Loss is 0.6219628. Sequentialfabab260's hyper parameters: Current learning rate is 0.008545547769612033. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 37888/60000][Iteration 853][Wall Clock 75.612400077s] Trained 256 records in 0.076328159 seconds. Throughput is 3353.9392 records/second. Loss is 0.6051179. Sequentialfabab260's hyper parameters: Current learning rate is 0.008544087491455913. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 38144/60000][Iteration 854][Wall Clock 75.684964091s] Trained 256 records in 0.072564014 seconds. Throughput is 3527.9194 records/second. Loss is 0.6845742. Sequentialfabab260's hyper parameters: Current learning rate is 0.008542627712284298. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 38400/60000][Iteration 855][Wall Clock 75.759015256s] Trained 256 records in 0.074051165 seconds. Throughput is 3457.0693 records/second. Loss is 0.6261835. Sequentialfabab260's hyper parameters: Current learning rate is 0.008541168431841476. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 38656/60000][Iteration 856][Wall Clock 75.83292881s] Trained 256 records in 0.073913554 seconds. Throughput is 3463.5056 records/second. Loss is 0.6128215. Sequentialfabab260's hyper parameters: Current learning rate is 0.008539709649871904. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 38912/60000][Iteration 857][Wall Clock 75.899943037s] Trained 256 records in 0.067014227 seconds. Throughput is 3820.0845 records/second. Loss is 0.65999997. Sequentialfabab260's hyper parameters: Current learning rate is 0.00853825136612022. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 39168/60000][Iteration 858][Wall Clock 75.968167589s] Trained 256 records in 0.068224552 seconds. Throughput is 3752.315 records/second. Loss is 0.5773177. Sequentialfabab260's hyper parameters: Current learning rate is 0.008536793580331228. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 39424/60000][Iteration 859][Wall Clock 76.036664606s] Trained 256 records in 0.068497017 seconds. Throughput is 3737.389 records/second. Loss is 0.5731711. Sequentialfabab260's hyper parameters: Current learning rate is 0.008535336292249915. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 39680/60000][Iteration 860][Wall Clock 76.100641025s] Trained 256 records in 0.063976419 seconds. Throughput is 4001.474 records/second. Loss is 0.6667533. Sequentialfabab260's hyper parameters: Current learning rate is 0.008533879501621438. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 39936/60000][Iteration 861][Wall Clock 76.165909845s] Trained 256 records in 0.06526882 seconds. Throughput is 3922.2402 records/second. Loss is 0.6504384. Sequentialfabab260's hyper parameters: Current learning rate is 0.008532423208191127. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:15 INFO  DistriOptimizer$:408 - [Epoch 4 40192/60000][Iteration 862][Wall Clock 76.235436846s] Trained 256 records in 0.069527001 seconds. Throughput is 3682.0227 records/second. Loss is 0.5995801. Sequentialfabab260's hyper parameters: Current learning rate is 0.008530967411704487. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:16 INFO  DistriOptimizer$:408 - [Epoch 4 40448/60000][Iteration 863][Wall Clock 76.310973619s] Trained 256 records in 0.075536773 seconds. Throughput is 3389.078 records/second. Loss is 0.60548204. Sequentialfabab260's hyper parameters: Current learning rate is 0.008529512111907198. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:16 INFO  DistriOptimizer$:408 - [Epoch 4 40704/60000][Iteration 864][Wall Clock 76.378651529s] Trained 256 records in 0.06767791 seconds. Throughput is 3782.6228 records/second. Loss is 0.53643477. Sequentialfabab260's hyper parameters: Current learning rate is 0.008528057308545113. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:16 INFO  DistriOptimizer$:408 - [Epoch 4 40960/60000][Iteration 865][Wall Clock 76.446747913s] Trained 256 records in 0.068096384 seconds. Throughput is 3759.3772 records/second. Loss is 0.6687893. Sequentialfabab260's hyper parameters: Current learning rate is 0.008526603001364257. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:16 INFO  DistriOptimizer$:408 - [Epoch 4 41216/60000][Iteration 866][Wall Clock 76.521365235s] Trained 256 records in 0.074617322 seconds. Throughput is 3430.8389 records/second. Loss is 0.6369244. Sequentialfabab260's hyper parameters: Current learning rate is 0.008525149190110827. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:16 INFO  DistriOptimizer$:408 - [Epoch 4 41472/60000][Iteration 867][Wall Clock 76.599343575s] Trained 256 records in 0.07797834 seconds. Throughput is 3282.963 records/second. Loss is 0.58209884. Sequentialfabab260's hyper parameters: Current learning rate is 0.008523695874531197. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:16 INFO  DistriOptimizer$:408 - [Epoch 4 41728/60000][Iteration 868][Wall Clock 76.701582216s] Trained 256 records in 0.102238641 seconds. Throughput is 2503.9456 records/second. Loss is 0.57058054. Sequentialfabab260's hyper parameters: Current learning rate is 0.00852224305437191. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:16 INFO  DistriOptimizer$:408 - [Epoch 4 41984/60000][Iteration 869][Wall Clock 76.78769241s] Trained 256 records in 0.086110194 seconds. Throughput is 2972.9348 records/second. Loss is 0.56290644. Sequentialfabab260's hyper parameters: Current learning rate is 0.008520790729379687. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:16 INFO  DistriOptimizer$:408 - [Epoch 4 42240/60000][Iteration 870][Wall Clock 76.877667862s] Trained 256 records in 0.089975452 seconds. Throughput is 2845.2205 records/second. Loss is 0.62027895. Sequentialfabab260's hyper parameters: Current learning rate is 0.008519338899301414. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:16 INFO  DistriOptimizer$:408 - [Epoch 4 42496/60000][Iteration 871][Wall Clock 76.956583331s] Trained 256 records in 0.078915469 seconds. Throughput is 3243.9773 records/second. Loss is 0.68160236. Sequentialfabab260's hyper parameters: Current learning rate is 0.008517887563884158. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:16 INFO  DistriOptimizer$:408 - [Epoch 4 42752/60000][Iteration 872][Wall Clock 77.050716928s] Trained 256 records in 0.094133597 seconds. Throughput is 2719.539 records/second. Loss is 0.6953275. Sequentialfabab260's hyper parameters: Current learning rate is 0.008516436722875149. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:16 INFO  DistriOptimizer$:408 - [Epoch 4 43008/60000][Iteration 873][Wall Clock 77.129710637s] Trained 256 records in 0.078993709 seconds. Throughput is 3240.7644 records/second. Loss is 0.7053907. Sequentialfabab260's hyper parameters: Current learning rate is 0.0085149863760218. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:16 INFO  DistriOptimizer$:408 - [Epoch 4 43264/60000][Iteration 874][Wall Clock 77.213023348s] Trained 256 records in 0.083312711 seconds. Throughput is 3072.7603 records/second. Loss is 0.6479418. Sequentialfabab260's hyper parameters: Current learning rate is 0.008513536523071684. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 43520/60000][Iteration 875][Wall Clock 77.28577103s] Trained 256 records in 0.072747682 seconds. Throughput is 3519.0122 records/second. Loss is 0.598908. Sequentialfabab260's hyper parameters: Current learning rate is 0.008512087163772556. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 43776/60000][Iteration 876][Wall Clock 77.353897764s] Trained 256 records in 0.068126734 seconds. Throughput is 3757.7026 records/second. Loss is 0.6458461. Sequentialfabab260's hyper parameters: Current learning rate is 0.00851063829787234. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 44032/60000][Iteration 877][Wall Clock 77.42448432s] Trained 256 records in 0.070586556 seconds. Throughput is 3626.753 records/second. Loss is 0.6895447. Sequentialfabab260's hyper parameters: Current learning rate is 0.00850918992511913. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 44288/60000][Iteration 878][Wall Clock 77.493672296s] Trained 256 records in 0.069187976 seconds. Throughput is 3700.065 records/second. Loss is 0.61695886. Sequentialfabab260's hyper parameters: Current learning rate is 0.008507742045261188. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 44544/60000][Iteration 879][Wall Clock 77.564132191s] Trained 256 records in 0.070459895 seconds. Throughput is 3633.2725 records/second. Loss is 0.57887924. Sequentialfabab260's hyper parameters: Current learning rate is 0.008506294658046955. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 44800/60000][Iteration 880][Wall Clock 77.644131088s] Trained 256 records in 0.079998897 seconds. Throughput is 3200.0442 records/second. Loss is 0.6104847. Sequentialfabab260's hyper parameters: Current learning rate is 0.008504847763225038. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 45056/60000][Iteration 881][Wall Clock 77.716062653s] Trained 256 records in 0.071931565 seconds. Throughput is 3558.9385 records/second. Loss is 0.6100091. Sequentialfabab260's hyper parameters: Current learning rate is 0.008503401360544218. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 45312/60000][Iteration 882][Wall Clock 77.799300225s] Trained 256 records in 0.083237572 seconds. Throughput is 3075.5342 records/second. Loss is 0.6242687. Sequentialfabab260's hyper parameters: Current learning rate is 0.008501955449753445. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 45568/60000][Iteration 883][Wall Clock 77.876567375s] Trained 256 records in 0.07726715 seconds. Throughput is 3313.1804 records/second. Loss is 0.5753123. Sequentialfabab260's hyper parameters: Current learning rate is 0.008500510030601835. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 45824/60000][Iteration 884][Wall Clock 77.94745076s] Trained 256 records in 0.070883385 seconds. Throughput is 3611.5657 records/second. Loss is 0.625018. Sequentialfabab260's hyper parameters: Current learning rate is 0.008499065102838687. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 46080/60000][Iteration 885][Wall Clock 78.020110775s] Trained 256 records in 0.072660015 seconds. Throughput is 3523.2583 records/second. Loss is 0.64945894. Sequentialfabab260's hyper parameters: Current learning rate is 0.00849762066621346. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 46336/60000][Iteration 886][Wall Clock 78.097214272s] Trained 256 records in 0.077103497 seconds. Throughput is 3320.2126 records/second. Loss is 0.6676401. Sequentialfabab260's hyper parameters: Current learning rate is 0.008496176720475786. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:17 INFO  DistriOptimizer$:408 - [Epoch 4 46592/60000][Iteration 887][Wall Clock 78.183250503s] Trained 256 records in 0.086036231 seconds. Throughput is 2975.4907 records/second. Loss is 0.59783477. Sequentialfabab260's hyper parameters: Current learning rate is 0.008494733265375467. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:18 INFO  DistriOptimizer$:408 - [Epoch 4 46848/60000][Iteration 888][Wall Clock 78.261583991s] Trained 256 records in 0.078333488 seconds. Throughput is 3268.0786 records/second. Loss is 0.59305406. Sequentialfabab260's hyper parameters: Current learning rate is 0.008493290300662476. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:18 INFO  DistriOptimizer$:408 - [Epoch 4 47104/60000][Iteration 889][Wall Clock 78.339338882s] Trained 256 records in 0.077754891 seconds. Throughput is 3292.3972 records/second. Loss is 0.6554851. Sequentialfabab260's hyper parameters: Current learning rate is 0.008491847826086956. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:18 INFO  DistriOptimizer$:408 - [Epoch 4 47360/60000][Iteration 890][Wall Clock 78.418695901s] Trained 256 records in 0.079357019 seconds. Throughput is 3225.9275 records/second. Loss is 0.5941425. Sequentialfabab260's hyper parameters: Current learning rate is 0.008490405841399219. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:18 INFO  DistriOptimizer$:408 - [Epoch 4 47616/60000][Iteration 891][Wall Clock 78.492594583s] Trained 256 records in 0.073898682 seconds. Throughput is 3464.2026 records/second. Loss is 0.5293347. Sequentialfabab260's hyper parameters: Current learning rate is 0.008488964346349746. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:18 INFO  DistriOptimizer$:408 - [Epoch 4 47872/60000][Iteration 892][Wall Clock 78.578274983s] Trained 256 records in 0.0856804 seconds. Throughput is 2987.8477 records/second. Loss is 0.5869738. Sequentialfabab260's hyper parameters: Current learning rate is 0.008487523340689187. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:18 INFO  DistriOptimizer$:408 - [Epoch 4 48128/60000][Iteration 893][Wall Clock 78.662762948s] Trained 256 records in 0.084487965 seconds. Throughput is 3030.0173 records/second. Loss is 0.6067457. Sequentialfabab260's hyper parameters: Current learning rate is 0.008486082824168364. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:18 INFO  DistriOptimizer$:408 - [Epoch 4 48384/60000][Iteration 894][Wall Clock 78.763414244s] Trained 256 records in 0.100651296 seconds. Throughput is 2543.4348 records/second. Loss is 0.54496074. Sequentialfabab260's hyper parameters: Current learning rate is 0.008484642796538265. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:18 INFO  DistriOptimizer$:408 - [Epoch 4 48640/60000][Iteration 895][Wall Clock 78.849614011s] Trained 256 records in 0.086199767 seconds. Throughput is 2969.8457 records/second. Loss is 0.68723184. Sequentialfabab260's hyper parameters: Current learning rate is 0.00848320325755005. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:18 INFO  DistriOptimizer$:408 - [Epoch 4 48896/60000][Iteration 896][Wall Clock 78.948700396s] Trained 256 records in 0.099086385 seconds. Throughput is 2583.6042 records/second. Loss is 0.5204736. Sequentialfabab260's hyper parameters: Current learning rate is 0.008481764206955046. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:18 INFO  DistriOptimizer$:408 - [Epoch 4 49152/60000][Iteration 897][Wall Clock 79.025072967s] Trained 256 records in 0.076372571 seconds. Throughput is 3351.9888 records/second. Loss is 0.5710864. Sequentialfabab260's hyper parameters: Current learning rate is 0.008480325644504749. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:18 INFO  DistriOptimizer$:408 - [Epoch 4 49408/60000][Iteration 898][Wall Clock 79.106129816s] Trained 256 records in 0.081056849 seconds. Throughput is 3158.2773 records/second. Loss is 0.59271085. Sequentialfabab260's hyper parameters: Current learning rate is 0.008478887569950822. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:18 INFO  DistriOptimizer$:408 - [Epoch 4 49664/60000][Iteration 899][Wall Clock 79.175883757s] Trained 256 records in 0.069753941 seconds. Throughput is 3670.0437 records/second. Loss is 0.62351054. Sequentialfabab260's hyper parameters: Current learning rate is 0.0084774499830451. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 49920/60000][Iteration 900][Wall Clock 79.240420415s] Trained 256 records in 0.064536658 seconds. Throughput is 3966.7375 records/second. Loss is 0.5245865. Sequentialfabab260's hyper parameters: Current learning rate is 0.008476012883539583. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 50176/60000][Iteration 901][Wall Clock 79.320981849s] Trained 256 records in 0.080561434 seconds. Throughput is 3177.699 records/second. Loss is 0.5517651. Sequentialfabab260's hyper parameters: Current learning rate is 0.00847457627118644. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 50432/60000][Iteration 902][Wall Clock 79.397306003s] Trained 256 records in 0.076324154 seconds. Throughput is 3354.115 records/second. Loss is 0.66265565. Sequentialfabab260's hyper parameters: Current learning rate is 0.00847314014573801. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 50688/60000][Iteration 903][Wall Clock 79.472679042s] Trained 256 records in 0.075373039 seconds. Throughput is 3396.44 records/second. Loss is 0.56954956. Sequentialfabab260's hyper parameters: Current learning rate is 0.008471704506946797. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 50944/60000][Iteration 904][Wall Clock 79.55231571s] Trained 256 records in 0.079636668 seconds. Throughput is 3214.5994 records/second. Loss is 0.54055434. Sequentialfabab260's hyper parameters: Current learning rate is 0.008470269354565475. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 51200/60000][Iteration 905][Wall Clock 79.627822833s] Trained 256 records in 0.075507123 seconds. Throughput is 3390.4087 records/second. Loss is 0.6115357. Sequentialfabab260's hyper parameters: Current learning rate is 0.008468834688346883. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 51456/60000][Iteration 906][Wall Clock 79.704787947s] Trained 256 records in 0.076965114 seconds. Throughput is 3326.1821 records/second. Loss is 0.6152614. Sequentialfabab260's hyper parameters: Current learning rate is 0.00846740050804403. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 51712/60000][Iteration 907][Wall Clock 79.779142929s] Trained 256 records in 0.074354982 seconds. Throughput is 3442.9434 records/second. Loss is 0.66163486. Sequentialfabab260's hyper parameters: Current learning rate is 0.008465966813410091. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 51968/60000][Iteration 908][Wall Clock 79.855420948s] Trained 256 records in 0.076278019 seconds. Throughput is 3356.144 records/second. Loss is 0.623657. Sequentialfabab260's hyper parameters: Current learning rate is 0.008464533604198408. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 52224/60000][Iteration 909][Wall Clock 79.925743206s] Trained 256 records in 0.070322258 seconds. Throughput is 3640.3835 records/second. Loss is 0.56129223. Sequentialfabab260's hyper parameters: Current learning rate is 0.008463100880162493. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 52480/60000][Iteration 910][Wall Clock 80.003870342s] Trained 256 records in 0.078127136 seconds. Throughput is 3276.7102 records/second. Loss is 0.6223431. Sequentialfabab260's hyper parameters: Current learning rate is 0.008461668641056016. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 52736/60000][Iteration 911][Wall Clock 80.089267557s] Trained 256 records in 0.085397215 seconds. Throughput is 2997.7559 records/second. Loss is 0.61676514. Sequentialfabab260's hyper parameters: Current learning rate is 0.008460236886632826. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:19 INFO  DistriOptimizer$:408 - [Epoch 4 52992/60000][Iteration 912][Wall Clock 80.189915491s] Trained 256 records in 0.100647934 seconds. Throughput is 2543.5198 records/second. Loss is 0.5736126. Sequentialfabab260's hyper parameters: Current learning rate is 0.00845880561664693. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:20 INFO  DistriOptimizer$:408 - [Epoch 4 53248/60000][Iteration 913][Wall Clock 80.266351632s] Trained 256 records in 0.076436141 seconds. Throughput is 3349.2012 records/second. Loss is 0.5469345. Sequentialfabab260's hyper parameters: Current learning rate is 0.008457374830852505. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:20 INFO  DistriOptimizer$:408 - [Epoch 4 53504/60000][Iteration 914][Wall Clock 80.338021254s] Trained 256 records in 0.071669622 seconds. Throughput is 3571.9456 records/second. Loss is 0.6415052. Sequentialfabab260's hyper parameters: Current learning rate is 0.00845594452900389. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:20 INFO  DistriOptimizer$:408 - [Epoch 4 53760/60000][Iteration 915][Wall Clock 80.420660625s] Trained 256 records in 0.082639371 seconds. Throughput is 3097.7969 records/second. Loss is 0.55579156. Sequentialfabab260's hyper parameters: Current learning rate is 0.008454514710855596. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:20 INFO  DistriOptimizer$:408 - [Epoch 4 54016/60000][Iteration 916][Wall Clock 80.50500318s] Trained 256 records in 0.084342555 seconds. Throughput is 3035.2412 records/second. Loss is 0.5305965. Sequentialfabab260's hyper parameters: Current learning rate is 0.008453085376162298. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:20 INFO  DistriOptimizer$:408 - [Epoch 4 54272/60000][Iteration 917][Wall Clock 80.578256974s] Trained 256 records in 0.073253794 seconds. Throughput is 3494.6995 records/second. Loss is 0.67219853. Sequentialfabab260's hyper parameters: Current learning rate is 0.008451656524678837. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:20 INFO  DistriOptimizer$:408 - [Epoch 4 54528/60000][Iteration 918][Wall Clock 80.663597044s] Trained 256 records in 0.08534007 seconds. Throughput is 2999.7632 records/second. Loss is 0.58477676. Sequentialfabab260's hyper parameters: Current learning rate is 0.008450228156160217. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:20 INFO  DistriOptimizer$:408 - [Epoch 4 54784/60000][Iteration 919][Wall Clock 80.753657233s] Trained 256 records in 0.090060189 seconds. Throughput is 2842.5435 records/second. Loss is 0.64056104. Sequentialfabab260's hyper parameters: Current learning rate is 0.00844880027036161. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:20 INFO  DistriOptimizer$:408 - [Epoch 4 55040/60000][Iteration 920][Wall Clock 80.843808249s] Trained 256 records in 0.090151016 seconds. Throughput is 2839.6794 records/second. Loss is 0.70688707. Sequentialfabab260's hyper parameters: Current learning rate is 0.00844737286703835. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:20 INFO  DistriOptimizer$:408 - [Epoch 4 55296/60000][Iteration 921][Wall Clock 80.971272339s] Trained 256 records in 0.12746409 seconds. Throughput is 2008.4088 records/second. Loss is 0.5574989. Sequentialfabab260's hyper parameters: Current learning rate is 0.008445945945945946. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:20 INFO  DistriOptimizer$:408 - [Epoch 4 55552/60000][Iteration 922][Wall Clock 81.052693993s] Trained 256 records in 0.081421654 seconds. Throughput is 3144.127 records/second. Loss is 0.56532913. Sequentialfabab260's hyper parameters: Current learning rate is 0.008444519506840062. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:20 INFO  DistriOptimizer$:408 - [Epoch 4 55808/60000][Iteration 923][Wall Clock 81.14906148s] Trained 256 records in 0.096367487 seconds. Throughput is 2656.4976 records/second. Loss is 0.6383661. Sequentialfabab260's hyper parameters: Current learning rate is 0.008443093549476527. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 56064/60000][Iteration 924][Wall Clock 81.22315501s] Trained 256 records in 0.07409353 seconds. Throughput is 3455.0925 records/second. Loss is 0.59255385. Sequentialfabab260's hyper parameters: Current learning rate is 0.008441668073611346. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 56320/60000][Iteration 925][Wall Clock 81.301960271s] Trained 256 records in 0.078805261 seconds. Throughput is 3248.5142 records/second. Loss is 0.56000096. Sequentialfabab260's hyper parameters: Current learning rate is 0.008440243079000674. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 56576/60000][Iteration 926][Wall Clock 81.382867233s] Trained 256 records in 0.080906962 seconds. Throughput is 3164.1282 records/second. Loss is 0.5682125. Sequentialfabab260's hyper parameters: Current learning rate is 0.008438818565400843. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 56832/60000][Iteration 927][Wall Clock 81.469072338s] Trained 256 records in 0.086205105 seconds. Throughput is 2969.6619 records/second. Loss is 0.64417994. Sequentialfabab260's hyper parameters: Current learning rate is 0.008437394532568343. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 57088/60000][Iteration 928][Wall Clock 81.550359122s] Trained 256 records in 0.081286784 seconds. Throughput is 3149.3435 records/second. Loss is 0.557535. Sequentialfabab260's hyper parameters: Current learning rate is 0.008435970980259827. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 57344/60000][Iteration 929][Wall Clock 81.622622987s] Trained 256 records in 0.072263865 seconds. Throughput is 3542.5728 records/second. Loss is 0.6551981. Sequentialfabab260's hyper parameters: Current learning rate is 0.008434547908232119. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 57600/60000][Iteration 930][Wall Clock 81.710210567s] Trained 256 records in 0.08758758 seconds. Throughput is 2922.7888 records/second. Loss is 0.6040267. Sequentialfabab260's hyper parameters: Current learning rate is 0.0084331253162422. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 57856/60000][Iteration 931][Wall Clock 81.788026164s] Trained 256 records in 0.077815597 seconds. Throughput is 3289.8289 records/second. Loss is 0.5640026. Sequentialfabab260's hyper parameters: Current learning rate is 0.008431703204047219. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 58112/60000][Iteration 932][Wall Clock 81.861998504s] Trained 256 records in 0.07397234 seconds. Throughput is 3460.7532 records/second. Loss is 0.5912697. Sequentialfabab260's hyper parameters: Current learning rate is 0.008430281571404486. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 58368/60000][Iteration 933][Wall Clock 81.932046915s] Trained 256 records in 0.070048411 seconds. Throughput is 3654.6152 records/second. Loss is 0.50386643. Sequentialfabab260's hyper parameters: Current learning rate is 0.008428860418071478. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 58624/60000][Iteration 934][Wall Clock 82.00683096s] Trained 256 records in 0.074784045 seconds. Throughput is 3423.19 records/second. Loss is 0.66345423. Sequentialfabab260's hyper parameters: Current learning rate is 0.008427439743805831. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 58880/60000][Iteration 935][Wall Clock 82.083116775s] Trained 256 records in 0.076285815 seconds. Throughput is 3355.8008 records/second. Loss is 0.61141634. Sequentialfabab260's hyper parameters: Current learning rate is 0.008426019548365351. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:21 INFO  DistriOptimizer$:408 - [Epoch 4 59136/60000][Iteration 936][Wall Clock 82.154535485s] Trained 256 records in 0.07141871 seconds. Throughput is 3584.4949 records/second. Loss is 0.54420364. Sequentialfabab260's hyper parameters: Current learning rate is 0.008424599831508003. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:22 INFO  DistriOptimizer$:408 - [Epoch 4 59392/60000][Iteration 937][Wall Clock 82.227745115s] Trained 256 records in 0.07320963 seconds. Throughput is 3496.8079 records/second. Loss is 0.5771694. Sequentialfabab260's hyper parameters: Current learning rate is 0.008423180592991913. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:22 INFO  DistriOptimizer$:408 - [Epoch 4 59648/60000][Iteration 938][Wall Clock 82.295754352s] Trained 256 records in 0.068009237 seconds. Throughput is 3764.1946 records/second. Loss is 0.5213772. Sequentialfabab260's hyper parameters: Current learning rate is 0.008421761832575375. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:22 INFO  DistriOptimizer$:408 - [Epoch 4 59904/60000][Iteration 939][Wall Clock 82.370604679s] Trained 256 records in 0.074850327 seconds. Throughput is 3420.1587 records/second. Loss is 0.52360934. Sequentialfabab260's hyper parameters: Current learning rate is 0.008420343550016841. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:22 INFO  DistriOptimizer$:408 - [Epoch 4 60160/60000][Iteration 940][Wall Clock 82.441523096s] Trained 256 records in 0.070918417 seconds. Throughput is 3609.7815 records/second. Loss is 0.514819. Sequentialfabab260's hyper parameters: Current learning rate is 0.00841892574507493. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:22 INFO  DistriOptimizer$:452 - [Epoch 4 60160/60000][Iteration 940][Wall Clock 82.441523096s] Epoch finished. Wall clock time is 82845.363928 ms
2019-10-17 12:22:22 INFO  DistriOptimizer$:111 - [Epoch 4 60160/60000][Iteration 940][Wall Clock 82.441523096s] Validate model...
2019-10-17 12:22:22 INFO  DistriOptimizer$:178 - [Epoch 4 60160/60000][Iteration 940][Wall Clock 82.441523096s] validate model throughput is 32471.555 records/second
2019-10-17 12:22:22 INFO  DistriOptimizer$:181 - [Epoch 4 60160/60000][Iteration 940][Wall Clock 82.441523096s] Top1Accuracy is Accuracy(correct: 8605, count: 10000, accuracy: 0.8605)
2019-10-17 12:22:22 INFO  DistriOptimizer$:221 - [Wall Clock 82.845363928s] Save model to /tmp/lenet5/20191017_122059
2019-10-17 12:22:22 INFO  DistriOptimizer$:226 - [Wall Clock 82.845363928s] Save optimMethod com.intel.analytics.bigdl.optim.SGD@9429a82 to /tmp/lenet5/20191017_122059
2019-10-17 12:22:22 INFO  DistriOptimizer$:408 - [Epoch 5 256/60000][Iteration 941][Wall Clock 82.932306042s] Trained 256 records in 0.086942114 seconds. Throughput is 2944.4878 records/second. Loss is 0.567868. Sequentialfabab260's hyper parameters: Current learning rate is 0.008417508417508417. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:22 INFO  DistriOptimizer$:408 - [Epoch 5 512/60000][Iteration 942][Wall Clock 83.028072215s] Trained 256 records in 0.095766173 seconds. Throughput is 2673.1777 records/second. Loss is 0.68729997. Sequentialfabab260's hyper parameters: Current learning rate is 0.008416091567076251. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:22 INFO  DistriOptimizer$:408 - [Epoch 5 768/60000][Iteration 943][Wall Clock 83.108540957s] Trained 256 records in 0.080468742 seconds. Throughput is 3181.3594 records/second. Loss is 0.5816306. Sequentialfabab260's hyper parameters: Current learning rate is 0.008414675193537528. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:22 INFO  DistriOptimizer$:408 - [Epoch 5 1024/60000][Iteration 944][Wall Clock 83.204615606s] Trained 256 records in 0.096074649 seconds. Throughput is 2664.5947 records/second. Loss is 0.6046307. Sequentialfabab260's hyper parameters: Current learning rate is 0.008413259296651522. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 1280/60000][Iteration 945][Wall Clock 83.27513785s] Trained 256 records in 0.070522244 seconds. Throughput is 3630.0605 records/second. Loss is 0.5947668. Sequentialfabab260's hyper parameters: Current learning rate is 0.008411843876177657. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 1536/60000][Iteration 946][Wall Clock 83.345599855s] Trained 256 records in 0.070462005 seconds. Throughput is 3633.1638 records/second. Loss is 0.55600846. Sequentialfabab260's hyper parameters: Current learning rate is 0.008410428931875526. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 1792/60000][Iteration 947][Wall Clock 83.423161147s] Trained 256 records in 0.077561292 seconds. Throughput is 3300.6155 records/second. Loss is 0.593367. Sequentialfabab260's hyper parameters: Current learning rate is 0.008409014463504878. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 2048/60000][Iteration 948][Wall Clock 83.498312543s] Trained 256 records in 0.075151396 seconds. Throughput is 3406.4568 records/second. Loss is 0.54776. Sequentialfabab260's hyper parameters: Current learning rate is 0.008407600470825626. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 2304/60000][Iteration 949][Wall Clock 83.57428514s] Trained 256 records in 0.075972597 seconds. Throughput is 3369.6362 records/second. Loss is 0.48984444. Sequentialfabab260's hyper parameters: Current learning rate is 0.008406186953597848. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 2560/60000][Iteration 950][Wall Clock 83.647031366s] Trained 256 records in 0.072746226 seconds. Throughput is 3519.083 records/second. Loss is 0.60894763. Sequentialfabab260's hyper parameters: Current learning rate is 0.00840477391158178. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 2816/60000][Iteration 951][Wall Clock 83.716948653s] Trained 256 records in 0.069917287 seconds. Throughput is 3661.4695 records/second. Loss is 0.5407807. Sequentialfabab260's hyper parameters: Current learning rate is 0.008403361344537816. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 3072/60000][Iteration 952][Wall Clock 83.810284024s] Trained 256 records in 0.093335371 seconds. Throughput is 2742.7974 records/second. Loss is 0.60128146. Sequentialfabab260's hyper parameters: Current learning rate is 0.008401949252226518. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 3328/60000][Iteration 953][Wall Clock 83.883555779s] Trained 256 records in 0.073271755 seconds. Throughput is 3493.843 records/second. Loss is 0.6161232. Sequentialfabab260's hyper parameters: Current learning rate is 0.008400537634408603. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 3584/60000][Iteration 954][Wall Clock 83.951582951s] Trained 256 records in 0.068027172 seconds. Throughput is 3763.2024 records/second. Loss is 0.5027038. Sequentialfabab260's hyper parameters: Current learning rate is 0.008399126490844951. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 3840/60000][Iteration 955][Wall Clock 84.018499181s] Trained 256 records in 0.06691623 seconds. Throughput is 3825.679 records/second. Loss is 0.56696105. Sequentialfabab260's hyper parameters: Current learning rate is 0.008397715821296607. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 4096/60000][Iteration 956][Wall Clock 84.087837332s] Trained 256 records in 0.069338151 seconds. Throughput is 3692.0513 records/second. Loss is 0.5416127. Sequentialfabab260's hyper parameters: Current learning rate is 0.008396305625524769. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:23 INFO  DistriOptimizer$:408 - [Epoch 5 4352/60000][Iteration 957][Wall Clock 84.157884945s] Trained 256 records in 0.070047613 seconds. Throughput is 3654.6572 records/second. Loss is 0.52992696. Sequentialfabab260's hyper parameters: Current learning rate is 0.0083948959032908. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 4608/60000][Iteration 958][Wall Clock 84.231677329s] Trained 256 records in 0.073792384 seconds. Throughput is 3469.1926 records/second. Loss is 0.5486564. Sequentialfabab260's hyper parameters: Current learning rate is 0.008393486654356219. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 4864/60000][Iteration 959][Wall Clock 84.299974589s] Trained 256 records in 0.06829726 seconds. Throughput is 3748.3203 records/second. Loss is 0.5448201. Sequentialfabab260's hyper parameters: Current learning rate is 0.008392077878482713. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 5120/60000][Iteration 960][Wall Clock 84.364467613s] Trained 256 records in 0.064493024 seconds. Throughput is 3969.4216 records/second. Loss is 0.6117091. Sequentialfabab260's hyper parameters: Current learning rate is 0.00839066957543212. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 5376/60000][Iteration 961][Wall Clock 84.432813861s] Trained 256 records in 0.068346248 seconds. Throughput is 3745.6335 records/second. Loss is 0.6307674. Sequentialfabab260's hyper parameters: Current learning rate is 0.008389261744966443. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 5632/60000][Iteration 962][Wall Clock 84.509673195s] Trained 256 records in 0.076859334 seconds. Throughput is 3330.76 records/second. Loss is 0.55571955. Sequentialfabab260's hyper parameters: Current learning rate is 0.008387854386847846. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 5888/60000][Iteration 963][Wall Clock 84.575162301s] Trained 256 records in 0.065489106 seconds. Throughput is 3909.047 records/second. Loss is 0.62586665. Sequentialfabab260's hyper parameters: Current learning rate is 0.008386447500838645. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 6144/60000][Iteration 964][Wall Clock 84.645215148s] Trained 256 records in 0.070052847 seconds. Throughput is 3654.384 records/second. Loss is 0.5550452. Sequentialfabab260's hyper parameters: Current learning rate is 0.008385041086701324. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 6400/60000][Iteration 965][Wall Clock 84.710972772s] Trained 256 records in 0.065757624 seconds. Throughput is 3893.0847 records/second. Loss is 0.56698626. Sequentialfabab260's hyper parameters: Current learning rate is 0.008383635144198523. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 6656/60000][Iteration 966][Wall Clock 84.785601571s] Trained 256 records in 0.074628799 seconds. Throughput is 3430.311 records/second. Loss is 0.5085046. Sequentialfabab260's hyper parameters: Current learning rate is 0.008382229673093043. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 6912/60000][Iteration 967][Wall Clock 84.86004801s] Trained 256 records in 0.074446439 seconds. Throughput is 3438.7139 records/second. Loss is 0.5681009. Sequentialfabab260's hyper parameters: Current learning rate is 0.008380824673147838. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 7168/60000][Iteration 968][Wall Clock 84.945263296s] Trained 256 records in 0.085215286 seconds. Throughput is 3004.1558 records/second. Loss is 0.57596743. Sequentialfabab260's hyper parameters: Current learning rate is 0.008379420144126027. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 7424/60000][Iteration 969][Wall Clock 85.032238025s] Trained 256 records in 0.086974729 seconds. Throughput is 2943.3835 records/second. Loss is 0.627308. Sequentialfabab260's hyper parameters: Current learning rate is 0.008378016085790885. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 7680/60000][Iteration 970][Wall Clock 85.106804057s] Trained 256 records in 0.074566032 seconds. Throughput is 3433.1987 records/second. Loss is 0.52050364. Sequentialfabab260's hyper parameters: Current learning rate is 0.008376612497905847. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:24 INFO  DistriOptimizer$:408 - [Epoch 5 7936/60000][Iteration 971][Wall Clock 85.180790555s] Trained 256 records in 0.073986498 seconds. Throughput is 3460.0906 records/second. Loss is 0.61838895. Sequentialfabab260's hyper parameters: Current learning rate is 0.008375209380234507. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 8192/60000][Iteration 972][Wall Clock 85.254203296s] Trained 256 records in 0.073412741 seconds. Throughput is 3487.1333 records/second. Loss is 0.5869456. Sequentialfabab260's hyper parameters: Current learning rate is 0.008373806732540614. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 8448/60000][Iteration 973][Wall Clock 85.319711176s] Trained 256 records in 0.06550788 seconds. Throughput is 3907.9268 records/second. Loss is 0.5717058. Sequentialfabab260's hyper parameters: Current learning rate is 0.008372404554588079. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 8704/60000][Iteration 974][Wall Clock 85.39852733s] Trained 256 records in 0.078816154 seconds. Throughput is 3248.0652 records/second. Loss is 0.5354742. Sequentialfabab260's hyper parameters: Current learning rate is 0.008371002846140967. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 8960/60000][Iteration 975][Wall Clock 85.463432405s] Trained 256 records in 0.064905075 seconds. Throughput is 3944.2214 records/second. Loss is 0.5265232. Sequentialfabab260's hyper parameters: Current learning rate is 0.008369601606963508. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 9216/60000][Iteration 976][Wall Clock 85.536128229s] Trained 256 records in 0.072695824 seconds. Throughput is 3521.523 records/second. Loss is 0.5651526. Sequentialfabab260's hyper parameters: Current learning rate is 0.008368200836820083. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 9472/60000][Iteration 977][Wall Clock 85.607003923s] Trained 256 records in 0.070875694 seconds. Throughput is 3611.9575 records/second. Loss is 0.6321025. Sequentialfabab260's hyper parameters: Current learning rate is 0.008366800535475234. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 9728/60000][Iteration 978][Wall Clock 85.673968573s] Trained 256 records in 0.06696465 seconds. Throughput is 3822.9126 records/second. Loss is 0.6028863. Sequentialfabab260's hyper parameters: Current learning rate is 0.008365400702693659. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 9984/60000][Iteration 979][Wall Clock 85.743229693s] Trained 256 records in 0.06926112 seconds. Throughput is 3696.1575 records/second. Loss is 0.62072957. Sequentialfabab260's hyper parameters: Current learning rate is 0.008364001338240215. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 10240/60000][Iteration 980][Wall Clock 85.814785388s] Trained 256 records in 0.071555695 seconds. Throughput is 3577.6328 records/second. Loss is 0.5731899. Sequentialfabab260's hyper parameters: Current learning rate is 0.008362602441879913. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 10496/60000][Iteration 981][Wall Clock 85.888356479s] Trained 256 records in 0.073571091 seconds. Throughput is 3479.6274 records/second. Loss is 0.5251658. Sequentialfabab260's hyper parameters: Current learning rate is 0.008361204013377928. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 10752/60000][Iteration 982][Wall Clock 85.954936368s] Trained 256 records in 0.066579889 seconds. Throughput is 3845.0051 records/second. Loss is 0.5952555. Sequentialfabab260's hyper parameters: Current learning rate is 0.008359806052499582. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 11008/60000][Iteration 983][Wall Clock 86.029055828s] Trained 256 records in 0.07411946 seconds. Throughput is 3453.8835 records/second. Loss is 0.52429426. Sequentialfabab260's hyper parameters: Current learning rate is 0.008358408559010364. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 11264/60000][Iteration 984][Wall Clock 86.099501639s] Trained 256 records in 0.070445811 seconds. Throughput is 3633.9988 records/second. Loss is 0.52810156. Sequentialfabab260's hyper parameters: Current learning rate is 0.008357011532675915. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:25 INFO  DistriOptimizer$:408 - [Epoch 5 11520/60000][Iteration 985][Wall Clock 86.165967423s] Trained 256 records in 0.066465784 seconds. Throughput is 3851.6057 records/second. Loss is 0.551477. Sequentialfabab260's hyper parameters: Current learning rate is 0.008355614973262033. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 11776/60000][Iteration 986][Wall Clock 86.240607889s] Trained 256 records in 0.074640466 seconds. Throughput is 3429.775 records/second. Loss is 0.586061. Sequentialfabab260's hyper parameters: Current learning rate is 0.00835421888053467. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 12032/60000][Iteration 987][Wall Clock 86.305376595s] Trained 256 records in 0.064768706 seconds. Throughput is 3952.526 records/second. Loss is 0.56397396. Sequentialfabab260's hyper parameters: Current learning rate is 0.00835282325425994. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 12288/60000][Iteration 988][Wall Clock 86.371671287s] Trained 256 records in 0.066294692 seconds. Throughput is 3861.546 records/second. Loss is 0.51524943. Sequentialfabab260's hyper parameters: Current learning rate is 0.008351428094204109. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 12544/60000][Iteration 989][Wall Clock 86.446912981s] Trained 256 records in 0.075241694 seconds. Throughput is 3402.369 records/second. Loss is 0.54455805. Sequentialfabab260's hyper parameters: Current learning rate is 0.008350033400133601. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 12800/60000][Iteration 990][Wall Clock 86.520108201s] Trained 256 records in 0.07319522 seconds. Throughput is 3497.496 records/second. Loss is 0.5127877. Sequentialfabab260's hyper parameters: Current learning rate is 0.008348639171814994. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 13056/60000][Iteration 991][Wall Clock 86.604484886s] Trained 256 records in 0.084376685 seconds. Throughput is 3034.0134 records/second. Loss is 0.5556177. Sequentialfabab260's hyper parameters: Current learning rate is 0.008347245409015026. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 13312/60000][Iteration 992][Wall Clock 86.683471953s] Trained 256 records in 0.078987067 seconds. Throughput is 3241.0369 records/second. Loss is 0.56218505. Sequentialfabab260's hyper parameters: Current learning rate is 0.008345852111500586. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 13568/60000][Iteration 993][Wall Clock 86.762619531s] Trained 256 records in 0.079147578 seconds. Throughput is 3234.464 records/second. Loss is 0.5104109. Sequentialfabab260's hyper parameters: Current learning rate is 0.00834445927903872. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 13824/60000][Iteration 994][Wall Clock 86.846806944s] Trained 256 records in 0.084187413 seconds. Throughput is 3040.8347 records/second. Loss is 0.54916567. Sequentialfabab260's hyper parameters: Current learning rate is 0.00834306691139663. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 14080/60000][Iteration 995][Wall Clock 86.92306219s] Trained 256 records in 0.076255246 seconds. Throughput is 3357.146 records/second. Loss is 0.58407116. Sequentialfabab260's hyper parameters: Current learning rate is 0.008341675008341674. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 14336/60000][Iteration 996][Wall Clock 87.010373722s] Trained 256 records in 0.087311532 seconds. Throughput is 2932.0298 records/second. Loss is 0.5387542. Sequentialfabab260's hyper parameters: Current learning rate is 0.008340283569641367. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 14592/60000][Iteration 997][Wall Clock 87.079854016s] Trained 256 records in 0.069480294 seconds. Throughput is 3684.498 records/second. Loss is 0.5623868. Sequentialfabab260's hyper parameters: Current learning rate is 0.008338892595063376. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:26 INFO  DistriOptimizer$:408 - [Epoch 5 14848/60000][Iteration 998][Wall Clock 87.144814172s] Trained 256 records in 0.064960156 seconds. Throughput is 3940.877 records/second. Loss is 0.643581. Sequentialfabab260's hyper parameters: Current learning rate is 0.00833750208437552. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 15104/60000][Iteration 999][Wall Clock 87.216836179s] Trained 256 records in 0.072022007 seconds. Throughput is 3554.4692 records/second. Loss is 0.6085425. Sequentialfabab260's hyper parameters: Current learning rate is 0.008336112037345782. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 15360/60000][Iteration 1000][Wall Clock 87.296359715s] Trained 256 records in 0.079523536 seconds. Throughput is 3219.1729 records/second. Loss is 0.5515283. Sequentialfabab260's hyper parameters: Current learning rate is 0.008334722453742291. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 15616/60000][Iteration 1001][Wall Clock 87.366128843s] Trained 256 records in 0.069769128 seconds. Throughput is 3669.2446 records/second. Loss is 0.55959237. Sequentialfabab260's hyper parameters: Current learning rate is 0.008333333333333333. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 15872/60000][Iteration 1002][Wall Clock 87.439815584s] Trained 256 records in 0.073686741 seconds. Throughput is 3474.1665 records/second. Loss is 0.60015416. Sequentialfabab260's hyper parameters: Current learning rate is 0.008331944675887352. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 16128/60000][Iteration 1003][Wall Clock 87.510421607s] Trained 256 records in 0.070606023 seconds. Throughput is 3625.753 records/second. Loss is 0.6322559. Sequentialfabab260's hyper parameters: Current learning rate is 0.008330556481172941. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 16384/60000][Iteration 1004][Wall Clock 87.580298958s] Trained 256 records in 0.069877351 seconds. Throughput is 3663.562 records/second. Loss is 0.5580479. Sequentialfabab260's hyper parameters: Current learning rate is 0.008329168748958853. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 16640/60000][Iteration 1005][Wall Clock 87.65772317s] Trained 256 records in 0.077424212 seconds. Throughput is 3306.4592 records/second. Loss is 0.524629. Sequentialfabab260's hyper parameters: Current learning rate is 0.008327781479013991. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 16896/60000][Iteration 1006][Wall Clock 87.729356955s] Trained 256 records in 0.071633785 seconds. Throughput is 3573.7327 records/second. Loss is 0.5834689. Sequentialfabab260's hyper parameters: Current learning rate is 0.00832639467110741. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 17152/60000][Iteration 1007][Wall Clock 87.80073408s] Trained 256 records in 0.071377125 seconds. Throughput is 3586.583 records/second. Loss is 0.573022. Sequentialfabab260's hyper parameters: Current learning rate is 0.008325008325008324. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 17408/60000][Iteration 1008][Wall Clock 87.865645039s] Trained 256 records in 0.064910959 seconds. Throughput is 3943.8643 records/second. Loss is 0.5344943. Sequentialfabab260's hyper parameters: Current learning rate is 0.0083236224404861. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 17664/60000][Iteration 1009][Wall Clock 87.943554542s] Trained 256 records in 0.077909503 seconds. Throughput is 3285.8638 records/second. Loss is 0.57434124. Sequentialfabab260's hyper parameters: Current learning rate is 0.008322237017310254. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 17920/60000][Iteration 1010][Wall Clock 88.013181874s] Trained 256 records in 0.069627332 seconds. Throughput is 3676.7173 records/second. Loss is 0.5123504. Sequentialfabab260's hyper parameters: Current learning rate is 0.008320852055250457. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 18176/60000][Iteration 1011][Wall Clock 88.089179376s] Trained 256 records in 0.075997502 seconds. Throughput is 3368.5317 records/second. Loss is 0.67453986. Sequentialfabab260's hyper parameters: Current learning rate is 0.00831946755407654. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:27 INFO  DistriOptimizer$:408 - [Epoch 5 18432/60000][Iteration 1012][Wall Clock 88.15435023s] Trained 256 records in 0.065170854 seconds. Throughput is 3928.1362 records/second. Loss is 0.55630153. Sequentialfabab260's hyper parameters: Current learning rate is 0.008318083513558477. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 18688/60000][Iteration 1013][Wall Clock 88.227630078s] Trained 256 records in 0.073279848 seconds. Throughput is 3493.457 records/second. Loss is 0.540835. Sequentialfabab260's hyper parameters: Current learning rate is 0.008316699933466402. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 18944/60000][Iteration 1014][Wall Clock 88.305530637s] Trained 256 records in 0.077900559 seconds. Throughput is 3286.241 records/second. Loss is 0.62361383. Sequentialfabab260's hyper parameters: Current learning rate is 0.008315316813570598. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 19200/60000][Iteration 1015][Wall Clock 88.386933913s] Trained 256 records in 0.081403276 seconds. Throughput is 3144.8364 records/second. Loss is 0.48529038. Sequentialfabab260's hyper parameters: Current learning rate is 0.008313934153641503. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 19456/60000][Iteration 1016][Wall Clock 88.466979969s] Trained 256 records in 0.080046056 seconds. Throughput is 3198.1587 records/second. Loss is 0.5246526. Sequentialfabab260's hyper parameters: Current learning rate is 0.00831255195344971. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 19712/60000][Iteration 1017][Wall Clock 88.548179371s] Trained 256 records in 0.081199402 seconds. Throughput is 3152.7327 records/second. Loss is 0.5003562. Sequentialfabab260's hyper parameters: Current learning rate is 0.008311170212765957. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 19968/60000][Iteration 1018][Wall Clock 88.636833415s] Trained 256 records in 0.088654044 seconds. Throughput is 2887.6292 records/second. Loss is 0.57486606. Sequentialfabab260's hyper parameters: Current learning rate is 0.008309788931361143. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 20224/60000][Iteration 1019][Wall Clock 88.728489309s] Trained 256 records in 0.091655894 seconds. Throughput is 2793.0554 records/second. Loss is 0.56365883. Sequentialfabab260's hyper parameters: Current learning rate is 0.008308408109006314. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 20480/60000][Iteration 1020][Wall Clock 88.807849775s] Trained 256 records in 0.079360466 seconds. Throughput is 3225.7876 records/second. Loss is 0.58225733. Sequentialfabab260's hyper parameters: Current learning rate is 0.00830702774547267. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 20736/60000][Iteration 1021][Wall Clock 88.876630346s] Trained 256 records in 0.068780571 seconds. Throughput is 3721.9812 records/second. Loss is 0.4939195. Sequentialfabab260's hyper parameters: Current learning rate is 0.008305647840531562. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 20992/60000][Iteration 1022][Wall Clock 88.94236554s] Trained 256 records in 0.065735194 seconds. Throughput is 3894.4133 records/second. Loss is 0.6012132. Sequentialfabab260's hyper parameters: Current learning rate is 0.008304268393954492. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 21248/60000][Iteration 1023][Wall Clock 89.012200579s] Trained 256 records in 0.069835039 seconds. Throughput is 3665.7817 records/second. Loss is 0.5078258. Sequentialfabab260's hyper parameters: Current learning rate is 0.008302889405513119. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 21504/60000][Iteration 1024][Wall Clock 89.090122456s] Trained 256 records in 0.077921877 seconds. Throughput is 3285.3418 records/second. Loss is 0.5254908. Sequentialfabab260's hyper parameters: Current learning rate is 0.008301510874979245. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:28 INFO  DistriOptimizer$:408 - [Epoch 5 21760/60000][Iteration 1025][Wall Clock 89.160112628s] Trained 256 records in 0.069990172 seconds. Throughput is 3657.6562 records/second. Loss is 0.60681885. Sequentialfabab260's hyper parameters: Current learning rate is 0.008300132802124834. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 22016/60000][Iteration 1026][Wall Clock 89.228859396s] Trained 256 records in 0.068746768 seconds. Throughput is 3723.8115 records/second. Loss is 0.59304905. Sequentialfabab260's hyper parameters: Current learning rate is 0.008298755186721992. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 22272/60000][Iteration 1027][Wall Clock 89.293970067s] Trained 256 records in 0.065110671 seconds. Throughput is 3931.7673 records/second. Loss is 0.56917924. Sequentialfabab260's hyper parameters: Current learning rate is 0.00829737802854298. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 22528/60000][Iteration 1028][Wall Clock 89.358812013s] Trained 256 records in 0.064841946 seconds. Throughput is 3948.0615 records/second. Loss is 0.51644343. Sequentialfabab260's hyper parameters: Current learning rate is 0.008296001327360213. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 22784/60000][Iteration 1029][Wall Clock 89.431647684s] Trained 256 records in 0.072835671 seconds. Throughput is 3514.7615 records/second. Loss is 0.5443224. Sequentialfabab260's hyper parameters: Current learning rate is 0.00829462508294625. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 23040/60000][Iteration 1030][Wall Clock 89.502738953s] Trained 256 records in 0.071091269 seconds. Throughput is 3601.0046 records/second. Loss is 0.5141961. Sequentialfabab260's hyper parameters: Current learning rate is 0.00829324929507381. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 23296/60000][Iteration 1031][Wall Clock 89.565546836s] Trained 256 records in 0.062807883 seconds. Throughput is 4075.9216 records/second. Loss is 0.589393. Sequentialfabab260's hyper parameters: Current learning rate is 0.008291873963515755. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 23552/60000][Iteration 1032][Wall Clock 89.633110382s] Trained 256 records in 0.067563546 seconds. Throughput is 3789.0254 records/second. Loss is 0.5405199. Sequentialfabab260's hyper parameters: Current learning rate is 0.0082904990880451. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 23808/60000][Iteration 1033][Wall Clock 89.70629812s] Trained 256 records in 0.073187738 seconds. Throughput is 3497.8538 records/second. Loss is 0.5648535. Sequentialfabab260's hyper parameters: Current learning rate is 0.008289124668435014. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 24064/60000][Iteration 1034][Wall Clock 89.777484902s] Trained 256 records in 0.071186782 seconds. Throughput is 3596.1733 records/second. Loss is 0.5910603. Sequentialfabab260's hyper parameters: Current learning rate is 0.00828775070445881. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 24320/60000][Iteration 1035][Wall Clock 89.847997796s] Trained 256 records in 0.070512894 seconds. Throughput is 3630.5417 records/second. Loss is 0.5364482. Sequentialfabab260's hyper parameters: Current learning rate is 0.008286377195889956. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 24576/60000][Iteration 1036][Wall Clock 89.914743386s] Trained 256 records in 0.06674559 seconds. Throughput is 3835.4595 records/second. Loss is 0.5470415. Sequentialfabab260's hyper parameters: Current learning rate is 0.008285004142502071. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 24832/60000][Iteration 1037][Wall Clock 89.979943225s] Trained 256 records in 0.065199839 seconds. Throughput is 3926.3901 records/second. Loss is 0.6406828. Sequentialfabab260's hyper parameters: Current learning rate is 0.00828363154406892. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 25088/60000][Iteration 1038][Wall Clock 90.051941826s] Trained 256 records in 0.071998601 seconds. Throughput is 3555.6245 records/second. Loss is 0.51564515. Sequentialfabab260's hyper parameters: Current learning rate is 0.008282259400364419. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:29 INFO  DistriOptimizer$:408 - [Epoch 5 25344/60000][Iteration 1039][Wall Clock 90.13039057s] Trained 256 records in 0.078448744 seconds. Throughput is 3263.2773 records/second. Loss is 0.49188778. Sequentialfabab260's hyper parameters: Current learning rate is 0.008280887711162636. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:30 INFO  DistriOptimizer$:408 - [Epoch 5 25600/60000][Iteration 1040][Wall Clock 90.204646684s] Trained 256 records in 0.074256114 seconds. Throughput is 3447.5276 records/second. Loss is 0.5355872. Sequentialfabab260's hyper parameters: Current learning rate is 0.008279516476237788. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:30 INFO  DistriOptimizer$:408 - [Epoch 5 25856/60000][Iteration 1041][Wall Clock 90.284662929s] Trained 256 records in 0.080016245 seconds. Throughput is 3199.35 records/second. Loss is 0.5820691. Sequentialfabab260's hyper parameters: Current learning rate is 0.008278145695364239. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:30 INFO  DistriOptimizer$:408 - [Epoch 5 26112/60000][Iteration 1042][Wall Clock 90.367425001s] Trained 256 records in 0.082762072 seconds. Throughput is 3093.2043 records/second. Loss is 0.5636471. Sequentialfabab260's hyper parameters: Current learning rate is 0.008276775368316504. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:30 INFO  DistriOptimizer$:408 - [Epoch 5 26368/60000][Iteration 1043][Wall Clock 90.449899365s] Trained 256 records in 0.082474364 seconds. Throughput is 3103.9949 records/second. Loss is 0.5264227. Sequentialfabab260's hyper parameters: Current learning rate is 0.00827540549486925. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:30 INFO  DistriOptimizer$:408 - [Epoch 5 26624/60000][Iteration 1044][Wall Clock 90.561925498s] Trained 256 records in 0.112026133 seconds. Throughput is 2285.1812 records/second. Loss is 0.54220265. Sequentialfabab260's hyper parameters: Current learning rate is 0.008274036074797285. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:30 INFO  DistriOptimizer$:408 - [Epoch 5 26880/60000][Iteration 1045][Wall Clock 90.658786238s] Trained 256 records in 0.09686074 seconds. Throughput is 2642.9697 records/second. Loss is 0.594883. Sequentialfabab260's hyper parameters: Current learning rate is 0.00827266710787558. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:30 INFO  DistriOptimizer$:408 - [Epoch 5 27136/60000][Iteration 1046][Wall Clock 90.734779167s] Trained 256 records in 0.075992929 seconds. Throughput is 3368.7346 records/second. Loss is 0.5459657. Sequentialfabab260's hyper parameters: Current learning rate is 0.008271298593879239. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:30 INFO  DistriOptimizer$:408 - [Epoch 5 27392/60000][Iteration 1047][Wall Clock 90.830943822s] Trained 256 records in 0.096164655 seconds. Throughput is 2662.1006 records/second. Loss is 0.51927054. Sequentialfabab260's hyper parameters: Current learning rate is 0.008269930532583526. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:30 INFO  DistriOptimizer$:408 - [Epoch 5 27648/60000][Iteration 1048][Wall Clock 90.916559618s] Trained 256 records in 0.085615796 seconds. Throughput is 2990.1023 records/second. Loss is 0.6075982. Sequentialfabab260's hyper parameters: Current learning rate is 0.00826856292376385. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:30 INFO  DistriOptimizer$:408 - [Epoch 5 27904/60000][Iteration 1049][Wall Clock 90.98972262s] Trained 256 records in 0.073163002 seconds. Throughput is 3499.0364 records/second. Loss is 0.51082945. Sequentialfabab260's hyper parameters: Current learning rate is 0.008267195767195767. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:30 INFO  DistriOptimizer$:408 - [Epoch 5 28160/60000][Iteration 1050][Wall Clock 91.054723427s] Trained 256 records in 0.065000807 seconds. Throughput is 3938.4126 records/second. Loss is 0.5436475. Sequentialfabab260's hyper parameters: Current learning rate is 0.008265829062654984. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:30 INFO  DistriOptimizer$:408 - [Epoch 5 28416/60000][Iteration 1051][Wall Clock 91.13180729s] Trained 256 records in 0.077083863 seconds. Throughput is 3321.0583 records/second. Loss is 0.58000463. Sequentialfabab260's hyper parameters: Current learning rate is 0.008264462809917356. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 28672/60000][Iteration 1052][Wall Clock 91.208771701s] Trained 256 records in 0.076964411 seconds. Throughput is 3326.213 records/second. Loss is 0.6173866. Sequentialfabab260's hyper parameters: Current learning rate is 0.008263097008758883. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 28928/60000][Iteration 1053][Wall Clock 91.298684812s] Trained 256 records in 0.089913111 seconds. Throughput is 2847.1934 records/second. Loss is 0.4719554. Sequentialfabab260's hyper parameters: Current learning rate is 0.008261731658955718. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 29184/60000][Iteration 1054][Wall Clock 91.369178554s] Trained 256 records in 0.070493742 seconds. Throughput is 3631.528 records/second. Loss is 0.59729236. Sequentialfabab260's hyper parameters: Current learning rate is 0.008260366760284157. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 29440/60000][Iteration 1055][Wall Clock 91.444866646s] Trained 256 records in 0.075688092 seconds. Throughput is 3382.302 records/second. Loss is 0.552593. Sequentialfabab260's hyper parameters: Current learning rate is 0.008259002312520646. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 29696/60000][Iteration 1056][Wall Clock 91.520314749s] Trained 256 records in 0.075448103 seconds. Throughput is 3393.0608 records/second. Loss is 0.62028444. Sequentialfabab260's hyper parameters: Current learning rate is 0.008257638315441783. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 29952/60000][Iteration 1057][Wall Clock 91.597573791s] Trained 256 records in 0.077259042 seconds. Throughput is 3313.528 records/second. Loss is 0.5641038. Sequentialfabab260's hyper parameters: Current learning rate is 0.008256274768824306. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 30208/60000][Iteration 1058][Wall Clock 91.675948672s] Trained 256 records in 0.078374881 seconds. Throughput is 3266.3528 records/second. Loss is 0.5187004. Sequentialfabab260's hyper parameters: Current learning rate is 0.008254911672445105. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 30464/60000][Iteration 1059][Wall Clock 91.741330298s] Trained 256 records in 0.065381626 seconds. Throughput is 3915.4731 records/second. Loss is 0.5128367. Sequentialfabab260's hyper parameters: Current learning rate is 0.008253549026081214. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 30720/60000][Iteration 1060][Wall Clock 91.806836945s] Trained 256 records in 0.065506647 seconds. Throughput is 3908.0005 records/second. Loss is 0.5652666. Sequentialfabab260's hyper parameters: Current learning rate is 0.00825218682950982. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 30976/60000][Iteration 1061][Wall Clock 91.874213714s] Trained 256 records in 0.067376769 seconds. Throughput is 3799.529 records/second. Loss is 0.540782. Sequentialfabab260's hyper parameters: Current learning rate is 0.008250825082508252. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 31232/60000][Iteration 1062][Wall Clock 91.947975413s] Trained 256 records in 0.073761699 seconds. Throughput is 3470.6357 records/second. Loss is 0.51140296. Sequentialfabab260's hyper parameters: Current learning rate is 0.008249463784853986. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 31488/60000][Iteration 1063][Wall Clock 92.022480279s] Trained 256 records in 0.074504866 seconds. Throughput is 3436.017 records/second. Loss is 0.54318035. Sequentialfabab260's hyper parameters: Current learning rate is 0.008248102936324647. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 31744/60000][Iteration 1064][Wall Clock 92.094208279s] Trained 256 records in 0.071728 seconds. Throughput is 3569.0386 records/second. Loss is 0.5452984. Sequentialfabab260's hyper parameters: Current learning rate is 0.008246742536698003. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:31 INFO  DistriOptimizer$:408 - [Epoch 5 32000/60000][Iteration 1065][Wall Clock 92.165484263s] Trained 256 records in 0.071275984 seconds. Throughput is 3591.6724 records/second. Loss is 0.536811. Sequentialfabab260's hyper parameters: Current learning rate is 0.008245382585751979. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 32256/60000][Iteration 1066][Wall Clock 92.252535603s] Trained 256 records in 0.08705134 seconds. Throughput is 2940.7935 records/second. Loss is 0.5164999. Sequentialfabab260's hyper parameters: Current learning rate is 0.008244023083264633. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 32512/60000][Iteration 1067][Wall Clock 92.331443188s] Trained 256 records in 0.078907585 seconds. Throughput is 3244.3015 records/second. Loss is 0.551469. Sequentialfabab260's hyper parameters: Current learning rate is 0.008242664029014177. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 32768/60000][Iteration 1068][Wall Clock 92.414842987s] Trained 256 records in 0.083399799 seconds. Throughput is 3069.5515 records/second. Loss is 0.48887533. Sequentialfabab260's hyper parameters: Current learning rate is 0.008241305422778969. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 33024/60000][Iteration 1069][Wall Clock 92.49540551s] Trained 256 records in 0.080562523 seconds. Throughput is 3177.6562 records/second. Loss is 0.57732344. Sequentialfabab260's hyper parameters: Current learning rate is 0.008239947264337508. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 33280/60000][Iteration 1070][Wall Clock 92.57896865s] Trained 256 records in 0.08356314 seconds. Throughput is 3063.5518 records/second. Loss is 0.5694237. Sequentialfabab260's hyper parameters: Current learning rate is 0.008238589553468446. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 33536/60000][Iteration 1071][Wall Clock 92.659766588s] Trained 256 records in 0.080797938 seconds. Throughput is 3168.3977 records/second. Loss is 0.4898606. Sequentialfabab260's hyper parameters: Current learning rate is 0.008237232289950576. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 33792/60000][Iteration 1072][Wall Clock 92.737304415s] Trained 256 records in 0.077537827 seconds. Throughput is 3301.6143 records/second. Loss is 0.54715526. Sequentialfabab260's hyper parameters: Current learning rate is 0.00823587547356284. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 34048/60000][Iteration 1073][Wall Clock 92.810187837s] Trained 256 records in 0.072883422 seconds. Throughput is 3512.4587 records/second. Loss is 0.5142963. Sequentialfabab260's hyper parameters: Current learning rate is 0.008234519104084322. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 34304/60000][Iteration 1074][Wall Clock 92.877104453s] Trained 256 records in 0.066916616 seconds. Throughput is 3825.6567 records/second. Loss is 0.54680663. Sequentialfabab260's hyper parameters: Current learning rate is 0.008233163181294254. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 34560/60000][Iteration 1075][Wall Clock 92.951517387s] Trained 256 records in 0.074412934 seconds. Throughput is 3440.2622 records/second. Loss is 0.5648893. Sequentialfabab260's hyper parameters: Current learning rate is 0.008231807704972012. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 34816/60000][Iteration 1076][Wall Clock 93.021219256s] Trained 256 records in 0.069701869 seconds. Throughput is 3672.7854 records/second. Loss is 0.5632169. Sequentialfabab260's hyper parameters: Current learning rate is 0.008230452674897118. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 35072/60000][Iteration 1077][Wall Clock 93.097758769s] Trained 256 records in 0.076539513 seconds. Throughput is 3344.6775 records/second. Loss is 0.5310245. Sequentialfabab260's hyper parameters: Current learning rate is 0.008229098090849242. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:32 INFO  DistriOptimizer$:408 - [Epoch 5 35328/60000][Iteration 1078][Wall Clock 93.171045036s] Trained 256 records in 0.073286267 seconds. Throughput is 3493.1511 records/second. Loss is 0.46909976. Sequentialfabab260's hyper parameters: Current learning rate is 0.008227743952608195. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 35584/60000][Iteration 1079][Wall Clock 93.236757746s] Trained 256 records in 0.06571271 seconds. Throughput is 3895.7454 records/second. Loss is 0.6504361. Sequentialfabab260's hyper parameters: Current learning rate is 0.008226390259953932. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 35840/60000][Iteration 1080][Wall Clock 93.298822638s] Trained 256 records in 0.062064892 seconds. Throughput is 4124.715 records/second. Loss is 0.56052375. Sequentialfabab260's hyper parameters: Current learning rate is 0.008225037012666558. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 36096/60000][Iteration 1081][Wall Clock 93.364684613s] Trained 256 records in 0.065861975 seconds. Throughput is 3886.9165 records/second. Loss is 0.53040147. Sequentialfabab260's hyper parameters: Current learning rate is 0.008223684210526315. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 36352/60000][Iteration 1082][Wall Clock 93.437957361s] Trained 256 records in 0.073272748 seconds. Throughput is 3493.7954 records/second. Loss is 0.51431227. Sequentialfabab260's hyper parameters: Current learning rate is 0.0082223318533136. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 36608/60000][Iteration 1083][Wall Clock 93.50795924s] Trained 256 records in 0.070001879 seconds. Throughput is 3657.0447 records/second. Loss is 0.51044893. Sequentialfabab260's hyper parameters: Current learning rate is 0.008220979940808944. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 36864/60000][Iteration 1084][Wall Clock 93.585045334s] Trained 256 records in 0.077086094 seconds. Throughput is 3320.9622 records/second. Loss is 0.508773. Sequentialfabab260's hyper parameters: Current learning rate is 0.008219628472793028. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 37120/60000][Iteration 1085][Wall Clock 93.66005336s] Trained 256 records in 0.075008026 seconds. Throughput is 3412.968 records/second. Loss is 0.47220743. Sequentialfabab260's hyper parameters: Current learning rate is 0.00821827744904668. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 37376/60000][Iteration 1086][Wall Clock 93.727771573s] Trained 256 records in 0.067718213 seconds. Throughput is 3780.3713 records/second. Loss is 0.55056804. Sequentialfabab260's hyper parameters: Current learning rate is 0.008216926869350863. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 37632/60000][Iteration 1087][Wall Clock 93.801910144s] Trained 256 records in 0.074138571 seconds. Throughput is 3452.9934 records/second. Loss is 0.5869056. Sequentialfabab260's hyper parameters: Current learning rate is 0.00821557673348669. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 37888/60000][Iteration 1088][Wall Clock 93.866493612s] Trained 256 records in 0.064583468 seconds. Throughput is 3963.8628 records/second. Loss is 0.5138619. Sequentialfabab260's hyper parameters: Current learning rate is 0.008214227041235419. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 38144/60000][Iteration 1089][Wall Clock 93.938243606s] Trained 256 records in 0.071749994 seconds. Throughput is 3567.9446 records/second. Loss is 0.5469253. Sequentialfabab260's hyper parameters: Current learning rate is 0.00821287779237845. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 38400/60000][Iteration 1090][Wall Clock 94.001891319s] Trained 256 records in 0.063647713 seconds. Throughput is 4022.14 records/second. Loss is 0.53736377. Sequentialfabab260's hyper parameters: Current learning rate is 0.008211528986697324. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 38656/60000][Iteration 1091][Wall Clock 94.074665266s] Trained 256 records in 0.072773947 seconds. Throughput is 3517.7424 records/second. Loss is 0.5311483. Sequentialfabab260's hyper parameters: Current learning rate is 0.008210180623973728. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:33 INFO  DistriOptimizer$:408 - [Epoch 5 38912/60000][Iteration 1092][Wall Clock 94.155805994s] Trained 256 records in 0.081140728 seconds. Throughput is 3155.0125 records/second. Loss is 0.54337555. Sequentialfabab260's hyper parameters: Current learning rate is 0.008208832703989493. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 39168/60000][Iteration 1093][Wall Clock 94.237477489s] Trained 256 records in 0.081671495 seconds. Throughput is 3134.5083 records/second. Loss is 0.45738173. Sequentialfabab260's hyper parameters: Current learning rate is 0.008207485226526593. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 39424/60000][Iteration 1094][Wall Clock 94.313742423s] Trained 256 records in 0.076264934 seconds. Throughput is 3356.7197 records/second. Loss is 0.47021013. Sequentialfabab260's hyper parameters: Current learning rate is 0.008206138191367143. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 39680/60000][Iteration 1095][Wall Clock 94.391879835s] Trained 256 records in 0.078137412 seconds. Throughput is 3276.2795 records/second. Loss is 0.47802374. Sequentialfabab260's hyper parameters: Current learning rate is 0.008204791598293402. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 39936/60000][Iteration 1096][Wall Clock 94.467624334s] Trained 256 records in 0.075744499 seconds. Throughput is 3379.7832 records/second. Loss is 0.5262156. Sequentialfabab260's hyper parameters: Current learning rate is 0.008203445447087777. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 40192/60000][Iteration 1097][Wall Clock 94.540333464s] Trained 256 records in 0.07270913 seconds. Throughput is 3520.8784 records/second. Loss is 0.54470295. Sequentialfabab260's hyper parameters: Current learning rate is 0.008202099737532808. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 40448/60000][Iteration 1098][Wall Clock 94.607193472s] Trained 256 records in 0.066860008 seconds. Throughput is 3828.896 records/second. Loss is 0.48946282. Sequentialfabab260's hyper parameters: Current learning rate is 0.008200754469411186. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 40704/60000][Iteration 1099][Wall Clock 94.672290217s] Trained 256 records in 0.065096745 seconds. Throughput is 3932.6084 records/second. Loss is 0.5501576. Sequentialfabab260's hyper parameters: Current learning rate is 0.008199409642505739. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 40960/60000][Iteration 1100][Wall Clock 94.741798801s] Trained 256 records in 0.069508584 seconds. Throughput is 3682.9985 records/second. Loss is 0.6360853. Sequentialfabab260's hyper parameters: Current learning rate is 0.008198065256599442. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 41216/60000][Iteration 1101][Wall Clock 94.812486333s] Trained 256 records in 0.070687532 seconds. Throughput is 3621.5723 records/second. Loss is 0.55730474. Sequentialfabab260's hyper parameters: Current learning rate is 0.00819672131147541. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 41472/60000][Iteration 1102][Wall Clock 94.875427985s] Trained 256 records in 0.062941652 seconds. Throughput is 4067.2588 records/second. Loss is 0.5731162. Sequentialfabab260's hyper parameters: Current learning rate is 0.0081953778069169. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 41728/60000][Iteration 1103][Wall Clock 94.945022642s] Trained 256 records in 0.069594657 seconds. Throughput is 3678.443 records/second. Loss is 0.5347195. Sequentialfabab260's hyper parameters: Current learning rate is 0.00819403474270731. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 41984/60000][Iteration 1104][Wall Clock 95.021746438s] Trained 256 records in 0.076723796 seconds. Throughput is 3336.644 records/second. Loss is 0.5419371. Sequentialfabab260's hyper parameters: Current learning rate is 0.008192692118630182. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:34 INFO  DistriOptimizer$:408 - [Epoch 5 42240/60000][Iteration 1105][Wall Clock 95.092306411s] Trained 256 records in 0.070559973 seconds. Throughput is 3628.1194 records/second. Loss is 0.5299546. Sequentialfabab260's hyper parameters: Current learning rate is 0.0081913499344692. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 42496/60000][Iteration 1106][Wall Clock 95.178069157s] Trained 256 records in 0.085762746 seconds. Throughput is 2984.979 records/second. Loss is 0.5403046. Sequentialfabab260's hyper parameters: Current learning rate is 0.00819000819000819. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 42752/60000][Iteration 1107][Wall Clock 95.258411078s] Trained 256 records in 0.080341921 seconds. Throughput is 3186.3813 records/second. Loss is 0.49051663. Sequentialfabab260's hyper parameters: Current learning rate is 0.008188666885031117. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 43008/60000][Iteration 1108][Wall Clock 95.328294406s] Trained 256 records in 0.069883328 seconds. Throughput is 3663.2483 records/second. Loss is 0.5317428. Sequentialfabab260's hyper parameters: Current learning rate is 0.008187326019322089. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 43264/60000][Iteration 1109][Wall Clock 95.401024373s] Trained 256 records in 0.072729967 seconds. Throughput is 3519.8696 records/second. Loss is 0.49624848. Sequentialfabab260's hyper parameters: Current learning rate is 0.008185985592665358. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 43520/60000][Iteration 1110][Wall Clock 95.471036109s] Trained 256 records in 0.070011736 seconds. Throughput is 3656.5298 records/second. Loss is 0.47961718. Sequentialfabab260's hyper parameters: Current learning rate is 0.00818464560484531. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 43776/60000][Iteration 1111][Wall Clock 95.5487969s] Trained 256 records in 0.077760791 seconds. Throughput is 3292.1475 records/second. Loss is 0.49340373. Sequentialfabab260's hyper parameters: Current learning rate is 0.008183306055646482. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 44032/60000][Iteration 1112][Wall Clock 95.619414582s] Trained 256 records in 0.070617682 seconds. Throughput is 3625.1543 records/second. Loss is 0.5522831. Sequentialfabab260's hyper parameters: Current learning rate is 0.008181966944853543. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 44288/60000][Iteration 1113][Wall Clock 95.687057119s] Trained 256 records in 0.067642537 seconds. Throughput is 3784.6006 records/second. Loss is 0.5402973. Sequentialfabab260's hyper parameters: Current learning rate is 0.008180628272251309. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 44544/60000][Iteration 1114][Wall Clock 95.75621144s] Trained 256 records in 0.069154321 seconds. Throughput is 3701.8655 records/second. Loss is 0.5170688. Sequentialfabab260's hyper parameters: Current learning rate is 0.008179290037624735. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 44800/60000][Iteration 1115][Wall Clock 95.832841723s] Trained 256 records in 0.076630283 seconds. Throughput is 3340.716 records/second. Loss is 0.49303764. Sequentialfabab260's hyper parameters: Current learning rate is 0.008177952240758915. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 45056/60000][Iteration 1116][Wall Clock 95.915276419s] Trained 256 records in 0.082434696 seconds. Throughput is 3105.4883 records/second. Loss is 0.5189996. Sequentialfabab260's hyper parameters: Current learning rate is 0.008176614881439084. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 45312/60000][Iteration 1117][Wall Clock 95.994924575s] Trained 256 records in 0.079648156 seconds. Throughput is 3214.1357 records/second. Loss is 0.4675153. Sequentialfabab260's hyper parameters: Current learning rate is 0.008175277959450621. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 45568/60000][Iteration 1118][Wall Clock 96.083930506s] Trained 256 records in 0.089005931 seconds. Throughput is 2876.213 records/second. Loss is 0.5402333. Sequentialfabab260's hyper parameters: Current learning rate is 0.008173941474579042. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:35 INFO  DistriOptimizer$:408 - [Epoch 5 45824/60000][Iteration 1119][Wall Clock 96.165117507s] Trained 256 records in 0.081187001 seconds. Throughput is 3153.214 records/second. Loss is 0.60460454. Sequentialfabab260's hyper parameters: Current learning rate is 0.008172605426610004. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 46080/60000][Iteration 1120][Wall Clock 96.242303987s] Trained 256 records in 0.07718648 seconds. Throughput is 3316.643 records/second. Loss is 0.5451033. Sequentialfabab260's hyper parameters: Current learning rate is 0.008171269815329302. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 46336/60000][Iteration 1121][Wall Clock 96.313930628s] Trained 256 records in 0.071626641 seconds. Throughput is 3574.089 records/second. Loss is 0.5103208. Sequentialfabab260's hyper parameters: Current learning rate is 0.008169934640522876. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 46592/60000][Iteration 1122][Wall Clock 96.383625648s] Trained 256 records in 0.06969502 seconds. Throughput is 3673.1462 records/second. Loss is 0.46921957. Sequentialfabab260's hyper parameters: Current learning rate is 0.008168599901976801. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 46848/60000][Iteration 1123][Wall Clock 96.456021858s] Trained 256 records in 0.07239621 seconds. Throughput is 3536.0967 records/second. Loss is 0.48215532. Sequentialfabab260's hyper parameters: Current learning rate is 0.008167265599477296. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 47104/60000][Iteration 1124][Wall Clock 96.524450897s] Trained 256 records in 0.068429039 seconds. Throughput is 3741.1018 records/second. Loss is 0.5829668. Sequentialfabab260's hyper parameters: Current learning rate is 0.008165931732810713. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 47360/60000][Iteration 1125][Wall Clock 96.599070096s] Trained 256 records in 0.074619199 seconds. Throughput is 3430.7527 records/second. Loss is 0.4985589. Sequentialfabab260's hyper parameters: Current learning rate is 0.008164598301763552. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 47616/60000][Iteration 1126][Wall Clock 96.669062966s] Trained 256 records in 0.06999287 seconds. Throughput is 3657.5154 records/second. Loss is 0.54294497. Sequentialfabab260's hyper parameters: Current learning rate is 0.008163265306122448. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 47872/60000][Iteration 1127][Wall Clock 96.734139686s] Trained 256 records in 0.06507672 seconds. Throughput is 3933.818 records/second. Loss is 0.5093547. Sequentialfabab260's hyper parameters: Current learning rate is 0.008161932745674175. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 48128/60000][Iteration 1128][Wall Clock 96.803506283s] Trained 256 records in 0.069366597 seconds. Throughput is 3690.537 records/second. Loss is 0.46445048. Sequentialfabab260's hyper parameters: Current learning rate is 0.008160600620205648. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 48384/60000][Iteration 1129][Wall Clock 96.867836746s] Trained 256 records in 0.064330463 seconds. Throughput is 3979.452 records/second. Loss is 0.55567586. Sequentialfabab260's hyper parameters: Current learning rate is 0.008159268929503917. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 48640/60000][Iteration 1130][Wall Clock 96.941540843s] Trained 256 records in 0.073704097 seconds. Throughput is 3473.3484 records/second. Loss is 0.48243254. Sequentialfabab260's hyper parameters: Current learning rate is 0.008157937673356175. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 48896/60000][Iteration 1131][Wall Clock 97.007616155s] Trained 256 records in 0.066075312 seconds. Throughput is 3874.367 records/second. Loss is 0.62713397. Sequentialfabab260's hyper parameters: Current learning rate is 0.008156606851549756. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 49152/60000][Iteration 1132][Wall Clock 97.081959511s] Trained 256 records in 0.074343356 seconds. Throughput is 3443.482 records/second. Loss is 0.61879027. Sequentialfabab260's hyper parameters: Current learning rate is 0.008155276463872126. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:36 INFO  DistriOptimizer$:408 - [Epoch 5 49408/60000][Iteration 1133][Wall Clock 97.146848719s] Trained 256 records in 0.064889208 seconds. Throughput is 3945.186 records/second. Loss is 0.44410616. Sequentialfabab260's hyper parameters: Current learning rate is 0.008153946510110895. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:37 INFO  DistriOptimizer$:408 - [Epoch 5 49664/60000][Iteration 1134][Wall Clock 97.208963064s] Trained 256 records in 0.062114345 seconds. Throughput is 4121.431 records/second. Loss is 0.5676392. Sequentialfabab260's hyper parameters: Current learning rate is 0.008152616990053808. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:37 INFO  DistriOptimizer$:408 - [Epoch 5 49920/60000][Iteration 1135][Wall Clock 97.284394504s] Trained 256 records in 0.07543144 seconds. Throughput is 3393.8105 records/second. Loss is 0.4783253. Sequentialfabab260's hyper parameters: Current learning rate is 0.008151287903488753. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:37 INFO  DistriOptimizer$:408 - [Epoch 5 50176/60000][Iteration 1136][Wall Clock 97.3595544s] Trained 256 records in 0.075159896 seconds. Throughput is 3406.072 records/second. Loss is 0.39896053. Sequentialfabab260's hyper parameters: Current learning rate is 0.008149959250203748. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:37 INFO  DistriOptimizer$:408 - [Epoch 5 50432/60000][Iteration 1137][Wall Clock 97.433353477s] Trained 256 records in 0.073799077 seconds. Throughput is 3468.8782 records/second. Loss is 0.5483422. Sequentialfabab260's hyper parameters: Current learning rate is 0.008148631029986962. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:37 INFO  DistriOptimizer$:408 - [Epoch 5 50688/60000][Iteration 1138][Wall Clock 97.516836107s] Trained 256 records in 0.08348263 seconds. Throughput is 3066.506 records/second. Loss is 0.5702599. Sequentialfabab260's hyper parameters: Current learning rate is 0.008147303242626691. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:37 INFO  DistriOptimizer$:408 - [Epoch 5 50944/60000][Iteration 1139][Wall Clock 97.591573688s] Trained 256 records in 0.074737581 seconds. Throughput is 3425.3184 records/second. Loss is 0.49219322. Sequentialfabab260's hyper parameters: Current learning rate is 0.008145975887911373. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:37 INFO  DistriOptimizer$:408 - [Epoch 5 51200/60000][Iteration 1140][Wall Clock 97.670656205s] Trained 256 records in 0.079082517 seconds. Throughput is 3237.125 records/second. Loss is 0.5112293. Sequentialfabab260's hyper parameters: Current learning rate is 0.008144648965629582. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:37 INFO  DistriOptimizer$:408 - [Epoch 5 51456/60000][Iteration 1141][Wall Clock 97.752656434s] Trained 256 records in 0.082000229 seconds. Throughput is 3121.9426 records/second. Loss is 0.47768486. Sequentialfabab260's hyper parameters: Current learning rate is 0.008143322475570033. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:37 INFO  DistriOptimizer$:408 - [Epoch 5 51712/60000][Iteration 1142][Wall Clock 97.849646583s] Trained 256 records in 0.096990149 seconds. Throughput is 2639.4434 records/second. Loss is 0.5265783. Sequentialfabab260's hyper parameters: Current learning rate is 0.008141996417521577. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:37 INFO  DistriOptimizer$:408 - [Epoch 5 51968/60000][Iteration 1143][Wall Clock 97.937407095s] Trained 256 records in 0.087760512 seconds. Throughput is 2917.0293 records/second. Loss is 0.521165. Sequentialfabab260's hyper parameters: Current learning rate is 0.008140670791273202. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:37 INFO  DistriOptimizer$:408 - [Epoch 5 52224/60000][Iteration 1144][Wall Clock 98.020078374s] Trained 256 records in 0.082671279 seconds. Throughput is 3096.6016 records/second. Loss is 0.57910204. Sequentialfabab260's hyper parameters: Current learning rate is 0.008139345596614033. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:37 INFO  DistriOptimizer$:408 - [Epoch 5 52480/60000][Iteration 1145][Wall Clock 98.10306951s] Trained 256 records in 0.082991136 seconds. Throughput is 3084.6667 records/second. Loss is 0.457622. Sequentialfabab260's hyper parameters: Current learning rate is 0.008138020833333332. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 52736/60000][Iteration 1146][Wall Clock 98.190250752s] Trained 256 records in 0.087181242 seconds. Throughput is 2936.4116 records/second. Loss is 0.5427864. Sequentialfabab260's hyper parameters: Current learning rate is 0.008136696501220505. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 52992/60000][Iteration 1147][Wall Clock 98.27267408s] Trained 256 records in 0.082423328 seconds. Throughput is 3105.9167 records/second. Loss is 0.53857243. Sequentialfabab260's hyper parameters: Current learning rate is 0.008135372600065083. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 53248/60000][Iteration 1148][Wall Clock 98.34485914s] Trained 256 records in 0.07218506 seconds. Throughput is 3546.4402 records/second. Loss is 0.5069189. Sequentialfabab260's hyper parameters: Current learning rate is 0.008134049129656743. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 53504/60000][Iteration 1149][Wall Clock 98.418659765s] Trained 256 records in 0.073800625 seconds. Throughput is 3468.8054 records/second. Loss is 0.5623107. Sequentialfabab260's hyper parameters: Current learning rate is 0.008132726089785295. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 53760/60000][Iteration 1150][Wall Clock 98.488999288s] Trained 256 records in 0.070339523 seconds. Throughput is 3639.4902 records/second. Loss is 0.49660325. Sequentialfabab260's hyper parameters: Current learning rate is 0.00813140348024069. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 54016/60000][Iteration 1151][Wall Clock 98.554369152s] Trained 256 records in 0.065369864 seconds. Throughput is 3916.1775 records/second. Loss is 0.497617. Sequentialfabab260's hyper parameters: Current learning rate is 0.008130081300813009. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 54272/60000][Iteration 1152][Wall Clock 98.623475733s] Trained 256 records in 0.069106581 seconds. Throughput is 3704.423 records/second. Loss is 0.5174494. Sequentialfabab260's hyper parameters: Current learning rate is 0.008128759551292473. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 54528/60000][Iteration 1153][Wall Clock 98.691768584s] Trained 256 records in 0.068292851 seconds. Throughput is 3748.5623 records/second. Loss is 0.53697133. Sequentialfabab260's hyper parameters: Current learning rate is 0.008127438231469442. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 54784/60000][Iteration 1154][Wall Clock 98.762868524s] Trained 256 records in 0.07109994 seconds. Throughput is 3600.5657 records/second. Loss is 0.48811144. Sequentialfabab260's hyper parameters: Current learning rate is 0.008126117341134406. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 55040/60000][Iteration 1155][Wall Clock 98.832135191s] Trained 256 records in 0.069266667 seconds. Throughput is 3695.8613 records/second. Loss is 0.5596541. Sequentialfabab260's hyper parameters: Current learning rate is 0.008124796880077998. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 55296/60000][Iteration 1156][Wall Clock 98.911965966s] Trained 256 records in 0.079830775 seconds. Throughput is 3206.7834 records/second. Loss is 0.60549057. Sequentialfabab260's hyper parameters: Current learning rate is 0.008123476848090982. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 55552/60000][Iteration 1157][Wall Clock 98.98667412s] Trained 256 records in 0.074708154 seconds. Throughput is 3426.6672 records/second. Loss is 0.43101367. Sequentialfabab260's hyper parameters: Current learning rate is 0.008122157244964262. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 55808/60000][Iteration 1158][Wall Clock 99.065537075s] Trained 256 records in 0.078862955 seconds. Throughput is 3246.1375 records/second. Loss is 0.59651697. Sequentialfabab260's hyper parameters: Current learning rate is 0.008120838070488873. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:38 INFO  DistriOptimizer$:408 - [Epoch 5 56064/60000][Iteration 1159][Wall Clock 99.143833512s] Trained 256 records in 0.078296437 seconds. Throughput is 3269.625 records/second. Loss is 0.44541815. Sequentialfabab260's hyper parameters: Current learning rate is 0.008119519324455992. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 56320/60000][Iteration 1160][Wall Clock 99.213819142s] Trained 256 records in 0.06998563 seconds. Throughput is 3657.8938 records/second. Loss is 0.5061398. Sequentialfabab260's hyper parameters: Current learning rate is 0.008118201006656925. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 56576/60000][Iteration 1161][Wall Clock 99.284926018s] Trained 256 records in 0.071106876 seconds. Throughput is 3600.2146 records/second. Loss is 0.45043042. Sequentialfabab260's hyper parameters: Current learning rate is 0.008116883116883118. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 56832/60000][Iteration 1162][Wall Clock 99.357414679s] Trained 256 records in 0.072488661 seconds. Throughput is 3531.587 records/second. Loss is 0.47339043. Sequentialfabab260's hyper parameters: Current learning rate is 0.008115565654926148. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 57088/60000][Iteration 1163][Wall Clock 99.432055304s] Trained 256 records in 0.074640625 seconds. Throughput is 3429.7676 records/second. Loss is 0.48287013. Sequentialfabab260's hyper parameters: Current learning rate is 0.008114248620577734. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 57344/60000][Iteration 1164][Wall Clock 99.506378119s] Trained 256 records in 0.074322815 seconds. Throughput is 3444.4336 records/second. Loss is 0.5346003. Sequentialfabab260's hyper parameters: Current learning rate is 0.008112932013629727. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 57600/60000][Iteration 1165][Wall Clock 99.576756396s] Trained 256 records in 0.070378277 seconds. Throughput is 3637.4863 records/second. Loss is 0.665407. Sequentialfabab260's hyper parameters: Current learning rate is 0.008111615833874108. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 57856/60000][Iteration 1166][Wall Clock 99.6542588s] Trained 256 records in 0.077502404 seconds. Throughput is 3303.1233 records/second. Loss is 0.5026415. Sequentialfabab260's hyper parameters: Current learning rate is 0.008110300081103. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 58112/60000][Iteration 1167][Wall Clock 99.739763475s] Trained 256 records in 0.085504675 seconds. Throughput is 2993.9883 records/second. Loss is 0.45368552. Sequentialfabab260's hyper parameters: Current learning rate is 0.00810898475510866. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 58368/60000][Iteration 1168][Wall Clock 99.829175819s] Trained 256 records in 0.089412344 seconds. Throughput is 2863.1392 records/second. Loss is 0.4779893. Sequentialfabab260's hyper parameters: Current learning rate is 0.008107669855683477. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 58624/60000][Iteration 1169][Wall Clock 99.928743652s] Trained 256 records in 0.099567833 seconds. Throughput is 2571.1116 records/second. Loss is 0.47522908. Sequentialfabab260's hyper parameters: Current learning rate is 0.008106355382619975. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 58880/60000][Iteration 1170][Wall Clock 100.011890855s] Trained 256 records in 0.083147203 seconds. Throughput is 3078.8767 records/second. Loss is 0.54421455. Sequentialfabab260's hyper parameters: Current learning rate is 0.008105041335710812. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 59136/60000][Iteration 1171][Wall Clock 100.083303692s] Trained 256 records in 0.071412837 seconds. Throughput is 3584.7896 records/second. Loss is 0.4702879. Sequentialfabab260's hyper parameters: Current learning rate is 0.008103727714748784. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:39 INFO  DistriOptimizer$:408 - [Epoch 5 59392/60000][Iteration 1172][Wall Clock 100.147714236s] Trained 256 records in 0.064410544 seconds. Throughput is 3974.5044 records/second. Loss is 0.45942262. Sequentialfabab260's hyper parameters: Current learning rate is 0.00810241451952682. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:40 INFO  DistriOptimizer$:408 - [Epoch 5 59648/60000][Iteration 1173][Wall Clock 100.219152362s] Trained 256 records in 0.071438126 seconds. Throughput is 3583.5208 records/second. Loss is 0.5486467. Sequentialfabab260's hyper parameters: Current learning rate is 0.00810110174983798. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:40 INFO  DistriOptimizer$:408 - [Epoch 5 59904/60000][Iteration 1174][Wall Clock 100.285806789s] Trained 256 records in 0.066654427 seconds. Throughput is 3840.705 records/second. Loss is 0.4993879. Sequentialfabab260's hyper parameters: Current learning rate is 0.008099789405475458. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:40 INFO  DistriOptimizer$:408 - [Epoch 5 60160/60000][Iteration 1175][Wall Clock 100.357209976s] Trained 256 records in 0.071403187 seconds. Throughput is 3585.274 records/second. Loss is 0.5428723. Sequentialfabab260's hyper parameters: Current learning rate is 0.008098477486232589. Current dampening is 1.7976931348623157E308.  
2019-10-17 12:22:40 INFO  DistriOptimizer$:452 - [Epoch 5 60160/60000][Iteration 1175][Wall Clock 100.357209976s] Epoch finished. Wall clock time is 100822.102717 ms
2019-10-17 12:22:40 INFO  DistriOptimizer$:111 - [Epoch 5 60160/60000][Iteration 1175][Wall Clock 100.357209976s] Validate model...
2019-10-17 12:22:40 INFO  DistriOptimizer$:178 - [Epoch 5 60160/60000][Iteration 1175][Wall Clock 100.357209976s] validate model throughput is 32411.086 records/second
2019-10-17 12:22:40 INFO  DistriOptimizer$:181 - [Epoch 5 60160/60000][Iteration 1175][Wall Clock 100.357209976s] Top1Accuracy is Accuracy(correct: 8776, count: 10000, accuracy: 0.8776)
2019-10-17 12:22:40 INFO  DistriOptimizer$:221 - [Wall Clock 100.822102717s] Save model to /tmp/lenet5/20191017_122059
2019-10-17 12:22:40 INFO  DistriOptimizer$:226 - [Wall Clock 100.822102717s] Save optimMethod com.intel.analytics.bigdl.optim.SGD@9429a82 to /tmp/lenet5/20191017_122059
